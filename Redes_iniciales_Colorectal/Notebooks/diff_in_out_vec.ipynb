{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520642ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle as pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from Stats import WeightQuantization, IntroduceFaultsInWeights, GenerateFaultsList\n",
    "from NetsVecino import GetNeuralNetworkModel\n",
    "from Training import GetDatasets\n",
    "from Simulation import get_all_outputs\n",
    "from Simulation import buffer_simulation, save_obj, load_obj\n",
    "from funciones import compilNet, same_elements\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pandas import ExcelWriter\n",
    "from openpyxl import Workbook\n",
    "from openpyxl import load_workbook\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40f5e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainBatchSize = testBatchSize = 1\n",
    "_,_,test_dataset = GetDatasets('colorectal_histology',(80,5,15),(227,227), 8, trainBatchSize, testBatchSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cc23ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_size  = 16\n",
    "afrac_size = 11  \n",
    "aint_size  = 4\n",
    "wfrac_size = 11\n",
    "wint_size  = 4\n",
    "\n",
    "\n",
    "cwd = os.getcwd()\n",
    "wgt_dir = os.path.join(cwd, 'Data')\n",
    "wgt_dir = os.path.join(wgt_dir, 'Trained Weights')\n",
    "wgt_dir = os.path.join(wgt_dir, 'AlexNet')\n",
    "wgt_dir = os.path.join(wgt_dir, 'Colorectal Dataset')\n",
    "wgt_dir = os.path.join(wgt_dir,'Weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2894e2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cargar_errores = True\n",
    "\n",
    "\n",
    "\n",
    "if Cargar_errores:\n",
    "    locs  = load_obj('Data/Fault Characterization/error_mask y locs_buffer_act_vc-707/locs_054')\n",
    "    error_mask = load_obj('Data/Fault Characterization/error_mask y locs_buffer_act_vc-707/error_mask_054')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f530d8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "NetVecinos = GetNeuralNetworkModel('AlexNet', (227,227,3), 8, faulty_addresses=locs, masked_faults=error_mask,\n",
    "                             aging_active=True, word_size=word_size, frac_size=afrac_size, batch_size = testBatchSize)\n",
    "\n",
    "#Cuantizacion de los pesos\n",
    "\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "NetVecinos.compile(optimizer=optimizer, loss=loss, metrics='accuracy')\n",
    "NetVecinos.load_weights(wgt_dir).expect_partial()\n",
    "WeightQuantization(model = NetVecinos, frac_bits = 11, int_bits = 4)\n",
    "#loss,acc =NetVecinos.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76021e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_layer=[2,8,10,16,18,24,30,36,38,44,49,53]\n",
    "\n",
    "diff_inp_output=[]\n",
    "razon=[]\n",
    "capa=[]\n",
    "numero=[]\n",
    "with pd.ExcelWriter('Analizando_fichero_detalle/diferencias_entre_resultados/Alexnet_Vecino.xlsx') as writer:\n",
    "    \n",
    "    for i,j in enumerate(write_layer):\n",
    "        \n",
    "        \n",
    "        print('Capa',j,NetVecinos.layers[j].__class__.__name__)\n",
    "        capa.append(NetVecinos.layers[j].__class__.__name__)\n",
    "        numero.append(j)\n",
    "        X = [x for x,y in test_dataset]\n",
    "        #salidas del modelo sin fallas para la primer imagen del dataset de prueba\n",
    "        out= get_all_outputs(NetVecinos,X[0])\n",
    "        #print ('out',out[1])\n",
    "        #out=get_all_outputs(Net2,iterator)\n",
    "        #salidas del modelo con fallas para la primer imagen del dataset de prueba\n",
    "        out_quantizacion = out[j-1]\n",
    "        out_error_and_correction = out[j]\n",
    "        out_input= tf.math.reduce_sum(tf.math.abs(tf.math.subtract(out_quantizacion,out_error_and_correction )))\n",
    "        diff_inp_output.append(out_input.numpy())\n",
    "        \n",
    "        out_output=tf.math.divide(tf.math.reduce_sum(tf.math.abs(out_quantizacion)),tf.math.reduce_sum(tf.math.abs(out_error_and_correction)))\n",
    "        razon.append(out_output.numpy())\n",
    "        \n",
    "        print(' diferencia absoluta input y outputs: ', diff_inp_output)\n",
    "        #print('razón absoluta entre input y outputs: ', tf.math.reduce_sum(tf.math.abs(tf.math.divide (out_quantizacion,out_error_and_correction ))))\n",
    "        print('razón',razon)\n",
    "    df_numero=pd.DataFrame(numero)\n",
    "    df_capa = pd.DataFrame(capa)   \n",
    "    df_inp_output=pd.DataFrame(diff_inp_output)\n",
    "    df_razon=pd.DataFrame(razon)\n",
    "        \n",
    "        \n",
    "    buf_diff_inp_out = pd.concat([df_numero,df_capa,df_inp_output,df_razon], axis=1, join='outer')\n",
    "    buf_diff_inp_out.columns = ['numero','Capa','df_inp_output','df_razon']\n",
    "    buf_diff_inp_out.to_excel(writer, sheet_name='buf_diff_inp_out', index=False)\n",
    "      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_first",
   "language": "python",
   "name": "env_first"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
