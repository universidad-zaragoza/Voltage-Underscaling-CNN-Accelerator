{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#from Heatmap_plot import Heatmap\n",
    "from Training import GetIMBDDataset\n",
    "from Nets_original  import GetNeuralNetworkModel\n",
    "from Stats import WeightQuantization, ActivationStats, CheckAccuracyAndLoss, QuantizationEffect, GetReadAndWrites\n",
    "from Simulation import buffer_simulation, save_obj, load_obj\n",
    "\n",
    "tf.random.set_seed(1234)\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; a) Get Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<__array_function__ internals>:180: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "C:\\Users\\usuario\\anaconda3\\envs\\env_first\\lib\\site-packages\\tensorflow\\python\\keras\\datasets\\imdb.py:155: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "C:\\Users\\usuario\\anaconda3\\envs\\env_first\\lib\\site-packages\\tensorflow\\python\\keras\\datasets\\imdb.py:156: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "train_batch_size = test_batch_size = 32\n",
    "train_set,valid_set,test_set  = GetIMBDDataset(train_batch_size, test_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; b) Get Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'bool' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m SentimentalNet \u001b[38;5;241m=\u001b[39m \u001b[43mGetNeuralNetworkModel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSentimentalNet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mquantization\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43maging_active\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#metrics = ['accuracy']\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Compile Model\u001b[39;00m\n",
      "File \u001b[1;32m~\\Desktop\\CNN_Gating\\Nets_original.py:518\u001b[0m, in \u001b[0;36mGetNeuralNetworkModel\u001b[1;34m(architecture, input_shape, output_shape, faulty_addresses, masked_faults, quantization, aging_active, word_size, frac_size, batch_size)\u001b[0m\n\u001b[0;32m    516\u001b[0m top_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5000\u001b[39m\n\u001b[0;32m    517\u001b[0m max_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m\t\n\u001b[1;32m--> 518\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maging_active\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[43maging_active\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m    519\u001b[0m input_layer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mInput(input_shape)\n\u001b[0;32m    520\u001b[0m x \u001b[38;5;241m=\u001b[39m Embedding(top_words, \u001b[38;5;241m32\u001b[39m, input_length\u001b[38;5;241m=\u001b[39mmax_words)(input_layer)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'bool' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "SentimentalNet = GetNeuralNetworkModel('SentimentalNet',(500),1,quantization = False,aging_active = False)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "#metrics = ['accuracy']\n",
    "# Compile Model\n",
    "SentimentalNet.compile(optimizer=optimizer, loss='binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; d) Save/Load Weigths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "wgt_dir = os.path.join(cwd, 'Data')\n",
    "wgt_dir = os.path.join(wgt_dir, 'Trained Weights')\n",
    "wgt_dir = os.path.join(wgt_dir, 'SentimentalNet')\n",
    "wgt_dir = os.path.join(wgt_dir, 'IMBD Reviews Dataset')\n",
    "wgt_dir = os.path.join(wgt_dir,'Weights')\n",
    "#SentimentalNet.load_weights(wgtDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  e) Activation Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean value (MMU): 0.0029325176\n",
      "mean value (Buffer): 0.00909117\n",
      "maximum (MMU): 0.16042535\n",
      "minimum (MMU): -0.15042056\n",
      "maximum (Buffer): 0.52474165\n",
      "minimum (Buffer): -0.049999762\n",
      "saturation ratio (MMU): 0.0\n",
      "saturation ratio (Buffer): 0.0\n"
     ]
    }
   ],
   "source": [
    "    ActivationStats(SentimentalNet,test_set,14,1,469)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  f) Write/Read Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "procesados:  25\n",
      "procesados:  50\n",
      "procesados:  75\n",
      "procesados:  100\n",
      "procesados:  125\n",
      "procesados:  150\n"
     ]
    }
   ],
   "source": [
    "Indices  = [1,4,8,12,16]\n",
    "#Data     = GetReadAndWrites(SentimentalNet,Indices,16000,150,CNN_gating=False)\n",
    "#stats    = {'Lecturas': Data['Reads'],'Escrituras': Data['Writes']}\n",
    "#Baseline_Acceses   = pd.DataFrame(stats).reset_index(drop=False)\n",
    "Data     = GetReadAndWrites(SentimentalNet,Indices,1024*1024,150,CNN_gating=True)\n",
    "stats    = {'Lecturas': Data['Reads'],'Escrituras': Data['Writes']}\n",
    "CNN_gating_Acceses = pd.DataFrame(stats).reset_index(drop=False)\n",
    "#save_obj(Baseline_Acceses,'Data/Acceses/SentimentalNet/Baseline')\n",
    "#save_obj(CNN_gating_Acceses,'Data/Acceses/SentimentalNet/CNN_gating_Fix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x239ed881370>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.stackplot(CNN_gating_Acceses.to_dict()['index'].values(),\n",
    "              CNN_gating_Acceses.to_dict()['Lecturas'].values(),\n",
    "              CNN_gating_Acceses.to_dict()['Escrituras'].values(),\n",
    "              colors=['blue', 'orange'])\n",
    "plt.legend(['Reads','Writes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x15dce44ca08>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.stackplot(Baseline_Acceses.to_dict()['index'].values(),\n",
    "              Baseline_Acceses.to_dict()['Lecturas'].values(),\n",
    "              Baseline_Acceses.to_dict()['Escrituras'].values(),\n",
    "              colors=['blue', 'orange'])\n",
    "plt.legend(['Reads','Writes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Quantization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; a) Starting Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shapes (16000, 250) and (8000, 250) are incompatible",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mCheckAccuracyAndLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSentimentalNet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwgt_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mact_frac_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mact_int_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwgt_frac_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwgt_int_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                    \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_shape\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtest_batch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\CNN_Gating\\Stats.py:115\u001b[0m, in \u001b[0;36mCheckAccuracyAndLoss\u001b[1;34m(architecture, test_dataset, wgt_dir, act_frac_size, act_int_size, wgt_frac_size, wgt_int_size, input_shape, output_shape, faulty_addresses, masked_faults, aging_active, batch_size, verbose, weights_faults)\u001b[0m\n\u001b[0;32m    109\u001b[0m qNet \u001b[38;5;241m=\u001b[39m GetNeuralNetworkModel(architecture,input_shape,output_shape,faulty_addresses,masked_faults,aging_active\u001b[38;5;241m=\u001b[39maging_active,\n\u001b[0;32m    110\u001b[0m \t\t\t\t\t\t\t word_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m+\u001b[39mact_frac_size\u001b[38;5;241m+\u001b[39mact_int_size), frac_size\u001b[38;5;241m=\u001b[39mact_frac_size, batch_size \u001b[38;5;241m=\u001b[39m batch_size)\n\u001b[0;32m    111\u001b[0m \u001b[38;5;66;03m#print('valor de aging_active', aging_active)\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \n\u001b[0;32m    113\u001b[0m \n\u001b[0;32m    114\u001b[0m \u001b[38;5;66;03m#Load Weights\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m \u001b[43mqNet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwgt_dir\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mexpect_partial()\n\u001b[0;32m    116\u001b[0m pesos_antes \u001b[38;5;241m=\u001b[39m qNet\u001b[38;5;241m.\u001b[39mget_weights()\n\u001b[0;32m    117\u001b[0m \u001b[38;5;66;03m#print(len(pesos_antes))\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;66;03m#print('pesos antes',pesos_antes)\u001b[39;00m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;66;03m#print('qnet.load',qNet.load_weights(wgt_dir).expect_partial())\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;66;03m#Quantize Weights\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\env_first\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2297\u001b[0m, in \u001b[0;36mModel.load_weights\u001b[1;34m(self, filepath, by_name, skip_mismatch, options)\u001b[0m\n\u001b[0;32m   2295\u001b[0m filepath, save_format \u001b[38;5;241m=\u001b[39m _detect_save_format(filepath)\n\u001b[0;32m   2296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m save_format \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m-> 2297\u001b[0m   status \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_trackable_saver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrestore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2298\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m by_name:\n\u001b[0;32m   2299\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m   2300\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWeights may only be loaded based on topology into Models when \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   2301\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloading TensorFlow-formatted weights (got by_name=True to \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   2302\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mload_weights).\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\env_first\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\util.py:1338\u001b[0m, in \u001b[0;36mTrackableSaver.restore\u001b[1;34m(self, save_path, options)\u001b[0m\n\u001b[0;32m   1330\u001b[0m object_graph_proto\u001b[38;5;241m.\u001b[39mParseFromString(object_graph_string)\n\u001b[0;32m   1331\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m _CheckpointRestoreCoordinator(\n\u001b[0;32m   1332\u001b[0m     object_graph_proto\u001b[38;5;241m=\u001b[39mobject_graph_proto,\n\u001b[0;32m   1333\u001b[0m     save_path\u001b[38;5;241m=\u001b[39msave_path,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1336\u001b[0m     graph_view\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph_view,\n\u001b[0;32m   1337\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions)\n\u001b[1;32m-> 1338\u001b[0m \u001b[43mbase\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCheckpointPosition\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproto_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrestore\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph_view\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1341\u001b[0m \u001b[38;5;66;03m# Attached dependencies are not attached to the root, so should be restored\u001b[39;00m\n\u001b[0;32m   1342\u001b[0m \u001b[38;5;66;03m# separately.\u001b[39;00m\n\u001b[0;32m   1343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph_view\u001b[38;5;241m.\u001b[39mattached_dependencies:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\env_first\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py:258\u001b[0m, in \u001b[0;36mCheckpointPosition.restore\u001b[1;34m(self, trackable)\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39minit_scope():\n\u001b[0;32m    255\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbind_object(trackable):\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;66;03m# This object's correspondence with a checkpointed object is new, so\u001b[39;00m\n\u001b[0;32m    257\u001b[0m     \u001b[38;5;66;03m# process deferred restorations for it and its dependencies.\u001b[39;00m\n\u001b[1;32m--> 258\u001b[0m     restore_ops \u001b[38;5;241m=\u001b[39m \u001b[43mtrackable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_restore_from_checkpoint_position\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m restore_ops:\n\u001b[0;32m    260\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint\u001b[38;5;241m.\u001b[39mnew_restore_ops(restore_ops)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\env_first\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py:977\u001b[0m, in \u001b[0;36mTrackable._restore_from_checkpoint_position\u001b[1;34m(self, checkpoint_position)\u001b[0m\n\u001b[0;32m    974\u001b[0m   tensor_saveables\u001b[38;5;241m.\u001b[39mupdate(new_tensor_saveables)\n\u001b[0;32m    975\u001b[0m   python_saveables\u001b[38;5;241m.\u001b[39mextend(new_python_saveables)\n\u001b[0;32m    976\u001b[0m restore_ops\u001b[38;5;241m.\u001b[39mextend(\n\u001b[1;32m--> 977\u001b[0m     \u001b[43mcurrent_position\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrestore_saveables\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtensor_saveables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpython_saveables\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    979\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m restore_ops\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\env_first\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\util.py:308\u001b[0m, in \u001b[0;36m_CheckpointRestoreCoordinator.restore_saveables\u001b[1;34m(self, tensor_saveables, python_saveables)\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mset\u001b[39m(tensor_saveables\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;241m!=\u001b[39m validated_names:\n\u001b[0;32m    305\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    306\u001b[0m       (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaveable keys changed when validating. Got back \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, was \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    307\u001b[0m        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpecting \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m%\u001b[39m (tensor_saveables\u001b[38;5;241m.\u001b[39mkeys(), validated_names))\n\u001b[1;32m--> 308\u001b[0m new_restore_ops \u001b[38;5;241m=\u001b[39m \u001b[43mfunctional_saver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMultiDeviceSaver\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    309\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidated_saveables\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrestore\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_path_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m    311\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m name, restore_op \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(new_restore_ops\u001b[38;5;241m.\u001b[39mitems()):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\env_first\\lib\\site-packages\\tensorflow\\python\\training\\saving\\functional_saver.py:339\u001b[0m, in \u001b[0;36mMultiDeviceSaver.restore\u001b[1;34m(self, file_prefix, options)\u001b[0m\n\u001b[0;32m    337\u001b[0m   restore_ops \u001b[38;5;241m=\u001b[39m tf_function_restore()\n\u001b[0;32m    338\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 339\u001b[0m   restore_ops \u001b[38;5;241m=\u001b[39m \u001b[43mrestore_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_restore_callbacks:\n\u001b[0;32m    342\u001b[0m   callback()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\env_first\\lib\\site-packages\\tensorflow\\python\\training\\saving\\functional_saver.py:323\u001b[0m, in \u001b[0;36mMultiDeviceSaver.restore.<locals>.restore_fn\u001b[1;34m()\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m device, saver \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_single_device_savers\u001b[38;5;241m.\u001b[39mitems()):\n\u001b[0;32m    322\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mdevice(device):\n\u001b[1;32m--> 323\u001b[0m     restore_ops\u001b[38;5;241m.\u001b[39mupdate(\u001b[43msaver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrestore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_prefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m restore_ops\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\env_first\\lib\\site-packages\\tensorflow\\python\\training\\saving\\functional_saver.py:115\u001b[0m, in \u001b[0;36m_SingleDeviceSaver.restore\u001b[1;34m(self, file_prefix, options)\u001b[0m\n\u001b[0;32m    112\u001b[0m restore_ops \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m saveable, restored_tensors \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_saveable_objects,\n\u001b[0;32m    114\u001b[0m                                       structured_restored_tensors):\n\u001b[1;32m--> 115\u001b[0m   restore_ops[saveable\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m \u001b[43msaveable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrestore\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[43m      \u001b[49m\u001b[43mrestored_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrestored_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m restore_ops\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\env_first\\lib\\site-packages\\tensorflow\\python\\training\\saving\\saveable_object_util.py:131\u001b[0m, in \u001b[0;36mResourceVariableSaveable.restore\u001b[1;34m(self, restored_tensors, restored_shapes)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_var_device):\n\u001b[0;32m    130\u001b[0m   restored_tensor \u001b[38;5;241m=\u001b[39m array_ops\u001b[38;5;241m.\u001b[39midentity(restored_tensor)\n\u001b[1;32m--> 131\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mresource_variable_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape_safe_assign_variable_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_op\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_var_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrestored_tensor\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\env_first\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:308\u001b[0m, in \u001b[0;36mshape_safe_assign_variable_handle\u001b[1;34m(handle, shape, value, name)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _handle_graph(handle):\n\u001b[0;32m    307\u001b[0m   value_tensor \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(value)\n\u001b[1;32m--> 308\u001b[0m \u001b[43mshape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_is_compatible_with\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m gen_resource_variable_ops\u001b[38;5;241m.\u001b[39massign_variable_op(\n\u001b[0;32m    310\u001b[0m     handle, value_tensor, name\u001b[38;5;241m=\u001b[39mname)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\env_first\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py:1161\u001b[0m, in \u001b[0;36mTensorShape.assert_is_compatible_with\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m   1149\u001b[0m \u001b[38;5;124;03m\"\"\"Raises exception if `self` and `other` do not represent the same shape.\u001b[39;00m\n\u001b[0;32m   1150\u001b[0m \n\u001b[0;32m   1151\u001b[0m \u001b[38;5;124;03mThis method can be used to assert that there exists a shape that both\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1158\u001b[0m \u001b[38;5;124;03m  ValueError: If `self` and `other` do not represent the same shape.\u001b[39;00m\n\u001b[0;32m   1159\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_compatible_with(other):\n\u001b[1;32m-> 1161\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShapes \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m are incompatible\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m, other))\n",
      "\u001b[1;31mValueError\u001b[0m: Shapes (16000, 250) and (8000, 250) are incompatible"
     ]
    }
   ],
   "source": [
    "CheckAccuracyAndLoss('SentimentalNet', test_set, wgt_dir, act_frac_size = 16, act_int_size = 16, wgt_frac_size = 16, wgt_int_size = 16, \n",
    "                    input_shape = (500), output_shape = 1, batch_size = test_batch_size);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; b) Number of bits analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estoy aquí en el efecto d ecuantización\n",
      "Activation fraction part\n",
      "aging_active en AddCustomLayers False\n",
      "dentro de includin Aging False\n",
      "dentro de GenerateAddressList \n",
      "aging_active dentro de aging_argiments False\n",
      "aging_active en AddCustomLayers False\n",
      "dentro de includin Aging False\n",
      "dentro de GenerateAddressList \n",
      "aging_active dentro de aging_argiments False\n",
      "aging_active en AddCustomLayers False\n",
      "dentro de includin Aging False\n",
      "dentro de GenerateAddressList \n",
      "aging_active dentro de aging_argiments False\n",
      "aging_active en AddCustomLayers False\n",
      "dentro de includin Aging False\n",
      "dentro de GenerateAddressList \n",
      "aging_active dentro de aging_argiments False\n",
      "aging_active en AddCustomLayers False\n",
      "aging_active en AddCustomLayers False\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shapes (16000, 250) and (8000, 250) are incompatible",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [40]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mQuantizationEffect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSentimentalNet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mtest_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43mwgt_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mtest_batch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\CNN_Gating\\Stats.py:341\u001b[0m, in \u001b[0;36mQuantizationEffect\u001b[1;34m(architecture, dataset, wgt_dir, input_shape, output_shape, batch_size, verbose)\u001b[0m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mActivation fraction part\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m bit \u001b[38;5;129;01min\u001b[39;00m bits:\n\u001b[1;32m--> 341\u001b[0m \tloss, acc \u001b[38;5;241m=\u001b[39m \u001b[43mCheckAccuracyAndLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43marchitecture\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwgt_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mact_frac_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mact_int_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mNQBE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwgt_frac_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mNQBE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwgt_int_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mNQBE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[43m\t\t\t\t\t\t\t\t\t\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_shape\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moutput_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    343\u001b[0m \t\u001b[38;5;28mprint\u001b[39m(bit,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m bits results: \u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macc: \u001b[39m\u001b[38;5;124m'\u001b[39m,acc, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss: \u001b[39m\u001b[38;5;124m'\u001b[39m,loss)\n\u001b[0;32m    344\u001b[0m \tdf \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mappend(pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExperiment\u001b[39m\u001b[38;5;124m'\u001b[39m:[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mActivation fraction part\u001b[39m\u001b[38;5;124m'\u001b[39m],\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbits\u001b[39m\u001b[38;5;124m'\u001b[39m:[bit],\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macc\u001b[39m\u001b[38;5;124m'\u001b[39m:[acc],\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m:[loss]}))\n",
      "File \u001b[1;32m~\\Desktop\\CNN_Gating\\Stats.py:115\u001b[0m, in \u001b[0;36mCheckAccuracyAndLoss\u001b[1;34m(architecture, test_dataset, wgt_dir, act_frac_size, act_int_size, wgt_frac_size, wgt_int_size, input_shape, output_shape, faulty_addresses, masked_faults, aging_active, batch_size, verbose, weights_faults)\u001b[0m\n\u001b[0;32m    109\u001b[0m qNet \u001b[38;5;241m=\u001b[39m GetNeuralNetworkModel(architecture,input_shape,output_shape,faulty_addresses,masked_faults,aging_active\u001b[38;5;241m=\u001b[39maging_active,\n\u001b[0;32m    110\u001b[0m \t\t\t\t\t\t\t word_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m+\u001b[39mact_frac_size\u001b[38;5;241m+\u001b[39mact_int_size), frac_size\u001b[38;5;241m=\u001b[39mact_frac_size, batch_size \u001b[38;5;241m=\u001b[39m batch_size)\n\u001b[0;32m    111\u001b[0m \u001b[38;5;66;03m#print('valor de aging_active', aging_active)\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \n\u001b[0;32m    113\u001b[0m \n\u001b[0;32m    114\u001b[0m \u001b[38;5;66;03m#Load Weights\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m \u001b[43mqNet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwgt_dir\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mexpect_partial()\n\u001b[0;32m    116\u001b[0m pesos_antes \u001b[38;5;241m=\u001b[39m qNet\u001b[38;5;241m.\u001b[39mget_weights()\n\u001b[0;32m    117\u001b[0m \u001b[38;5;66;03m#print(len(pesos_antes))\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;66;03m#print('pesos antes',pesos_antes)\u001b[39;00m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;66;03m#print('qnet.load',qNet.load_weights(wgt_dir).expect_partial())\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;66;03m#Quantize Weights\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\env_first\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2297\u001b[0m, in \u001b[0;36mModel.load_weights\u001b[1;34m(self, filepath, by_name, skip_mismatch, options)\u001b[0m\n\u001b[0;32m   2295\u001b[0m filepath, save_format \u001b[38;5;241m=\u001b[39m _detect_save_format(filepath)\n\u001b[0;32m   2296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m save_format \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m-> 2297\u001b[0m   status \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_trackable_saver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrestore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2298\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m by_name:\n\u001b[0;32m   2299\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m   2300\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWeights may only be loaded based on topology into Models when \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   2301\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloading TensorFlow-formatted weights (got by_name=True to \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   2302\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mload_weights).\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\env_first\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\util.py:1338\u001b[0m, in \u001b[0;36mTrackableSaver.restore\u001b[1;34m(self, save_path, options)\u001b[0m\n\u001b[0;32m   1330\u001b[0m object_graph_proto\u001b[38;5;241m.\u001b[39mParseFromString(object_graph_string)\n\u001b[0;32m   1331\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m _CheckpointRestoreCoordinator(\n\u001b[0;32m   1332\u001b[0m     object_graph_proto\u001b[38;5;241m=\u001b[39mobject_graph_proto,\n\u001b[0;32m   1333\u001b[0m     save_path\u001b[38;5;241m=\u001b[39msave_path,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1336\u001b[0m     graph_view\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph_view,\n\u001b[0;32m   1337\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions)\n\u001b[1;32m-> 1338\u001b[0m \u001b[43mbase\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCheckpointPosition\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproto_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrestore\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph_view\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1341\u001b[0m \u001b[38;5;66;03m# Attached dependencies are not attached to the root, so should be restored\u001b[39;00m\n\u001b[0;32m   1342\u001b[0m \u001b[38;5;66;03m# separately.\u001b[39;00m\n\u001b[0;32m   1343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph_view\u001b[38;5;241m.\u001b[39mattached_dependencies:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\env_first\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py:258\u001b[0m, in \u001b[0;36mCheckpointPosition.restore\u001b[1;34m(self, trackable)\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39minit_scope():\n\u001b[0;32m    255\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbind_object(trackable):\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;66;03m# This object's correspondence with a checkpointed object is new, so\u001b[39;00m\n\u001b[0;32m    257\u001b[0m     \u001b[38;5;66;03m# process deferred restorations for it and its dependencies.\u001b[39;00m\n\u001b[1;32m--> 258\u001b[0m     restore_ops \u001b[38;5;241m=\u001b[39m \u001b[43mtrackable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_restore_from_checkpoint_position\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m restore_ops:\n\u001b[0;32m    260\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint\u001b[38;5;241m.\u001b[39mnew_restore_ops(restore_ops)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\env_first\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py:977\u001b[0m, in \u001b[0;36mTrackable._restore_from_checkpoint_position\u001b[1;34m(self, checkpoint_position)\u001b[0m\n\u001b[0;32m    974\u001b[0m   tensor_saveables\u001b[38;5;241m.\u001b[39mupdate(new_tensor_saveables)\n\u001b[0;32m    975\u001b[0m   python_saveables\u001b[38;5;241m.\u001b[39mextend(new_python_saveables)\n\u001b[0;32m    976\u001b[0m restore_ops\u001b[38;5;241m.\u001b[39mextend(\n\u001b[1;32m--> 977\u001b[0m     \u001b[43mcurrent_position\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrestore_saveables\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtensor_saveables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpython_saveables\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    979\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m restore_ops\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\env_first\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\util.py:308\u001b[0m, in \u001b[0;36m_CheckpointRestoreCoordinator.restore_saveables\u001b[1;34m(self, tensor_saveables, python_saveables)\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mset\u001b[39m(tensor_saveables\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;241m!=\u001b[39m validated_names:\n\u001b[0;32m    305\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    306\u001b[0m       (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaveable keys changed when validating. Got back \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, was \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    307\u001b[0m        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpecting \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m%\u001b[39m (tensor_saveables\u001b[38;5;241m.\u001b[39mkeys(), validated_names))\n\u001b[1;32m--> 308\u001b[0m new_restore_ops \u001b[38;5;241m=\u001b[39m \u001b[43mfunctional_saver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMultiDeviceSaver\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    309\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidated_saveables\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrestore\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_path_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m    311\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m name, restore_op \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(new_restore_ops\u001b[38;5;241m.\u001b[39mitems()):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\env_first\\lib\\site-packages\\tensorflow\\python\\training\\saving\\functional_saver.py:339\u001b[0m, in \u001b[0;36mMultiDeviceSaver.restore\u001b[1;34m(self, file_prefix, options)\u001b[0m\n\u001b[0;32m    337\u001b[0m   restore_ops \u001b[38;5;241m=\u001b[39m tf_function_restore()\n\u001b[0;32m    338\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 339\u001b[0m   restore_ops \u001b[38;5;241m=\u001b[39m \u001b[43mrestore_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_restore_callbacks:\n\u001b[0;32m    342\u001b[0m   callback()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\env_first\\lib\\site-packages\\tensorflow\\python\\training\\saving\\functional_saver.py:323\u001b[0m, in \u001b[0;36mMultiDeviceSaver.restore.<locals>.restore_fn\u001b[1;34m()\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m device, saver \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_single_device_savers\u001b[38;5;241m.\u001b[39mitems()):\n\u001b[0;32m    322\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mdevice(device):\n\u001b[1;32m--> 323\u001b[0m     restore_ops\u001b[38;5;241m.\u001b[39mupdate(\u001b[43msaver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrestore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_prefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m restore_ops\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\env_first\\lib\\site-packages\\tensorflow\\python\\training\\saving\\functional_saver.py:115\u001b[0m, in \u001b[0;36m_SingleDeviceSaver.restore\u001b[1;34m(self, file_prefix, options)\u001b[0m\n\u001b[0;32m    112\u001b[0m restore_ops \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m saveable, restored_tensors \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_saveable_objects,\n\u001b[0;32m    114\u001b[0m                                       structured_restored_tensors):\n\u001b[1;32m--> 115\u001b[0m   restore_ops[saveable\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m \u001b[43msaveable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrestore\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[43m      \u001b[49m\u001b[43mrestored_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrestored_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m restore_ops\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\env_first\\lib\\site-packages\\tensorflow\\python\\training\\saving\\saveable_object_util.py:131\u001b[0m, in \u001b[0;36mResourceVariableSaveable.restore\u001b[1;34m(self, restored_tensors, restored_shapes)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_var_device):\n\u001b[0;32m    130\u001b[0m   restored_tensor \u001b[38;5;241m=\u001b[39m array_ops\u001b[38;5;241m.\u001b[39midentity(restored_tensor)\n\u001b[1;32m--> 131\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mresource_variable_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape_safe_assign_variable_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_op\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_var_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrestored_tensor\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\env_first\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:308\u001b[0m, in \u001b[0;36mshape_safe_assign_variable_handle\u001b[1;34m(handle, shape, value, name)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _handle_graph(handle):\n\u001b[0;32m    307\u001b[0m   value_tensor \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(value)\n\u001b[1;32m--> 308\u001b[0m \u001b[43mshape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_is_compatible_with\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m gen_resource_variable_ops\u001b[38;5;241m.\u001b[39massign_variable_op(\n\u001b[0;32m    310\u001b[0m     handle, value_tensor, name\u001b[38;5;241m=\u001b[39mname)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\env_first\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py:1161\u001b[0m, in \u001b[0;36mTensorShape.assert_is_compatible_with\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m   1149\u001b[0m \u001b[38;5;124;03m\"\"\"Raises exception if `self` and `other` do not represent the same shape.\u001b[39;00m\n\u001b[0;32m   1150\u001b[0m \n\u001b[0;32m   1151\u001b[0m \u001b[38;5;124;03mThis method can be used to assert that there exists a shape that both\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1158\u001b[0m \u001b[38;5;124;03m  ValueError: If `self` and `other` do not represent the same shape.\u001b[39;00m\n\u001b[0;32m   1159\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_compatible_with(other):\n\u001b[1;32m-> 1161\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShapes \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m are incompatible\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m, other))\n",
      "\u001b[1;31mValueError\u001b[0m: Shapes (16000, 250) and (8000, 250) are incompatible"
     ]
    }
   ],
   "source": [
    "df = QuantizationEffect('SentimentalNet',test_set,wgt_dir,(500),1,test_batch_size)\n",
    "#save_obj(df,'Data/Quantization/SentimentalNet/IMDB Reviews Dataset/Quantization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; f) Used Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aging_active en AddCustomLayers False\n",
      "dentro de includin Aging False\n",
      "dentro de GenerateAddressList \n",
      "aging_active dentro de aging_argiments False\n",
      "aging_active en AddCustomLayers False\n",
      "dentro de includin Aging False\n",
      "dentro de GenerateAddressList \n",
      "aging_active dentro de aging_argiments False\n",
      "aging_active en AddCustomLayers False\n",
      "dentro de includin Aging False\n",
      "dentro de GenerateAddressList \n",
      "aging_active dentro de aging_argiments False\n",
      "aging_active en AddCustomLayers False\n",
      "dentro de includin Aging False\n",
      "dentro de GenerateAddressList \n",
      "aging_active dentro de aging_argiments False\n",
      "aging_active en AddCustomLayers False\n",
      "aging_active en AddCustomLayers False\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shapes (16000, 250) and (8000, 250) are incompatible",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [43]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mCheckAccuracyAndLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSentimentalNet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwgt_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mact_frac_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m14\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mact_int_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwgt_frac_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwgt_int_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                    \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_shape\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtest_batch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\CNN_Gating\\Stats.py:115\u001b[0m, in \u001b[0;36mCheckAccuracyAndLoss\u001b[1;34m(architecture, test_dataset, wgt_dir, act_frac_size, act_int_size, wgt_frac_size, wgt_int_size, input_shape, output_shape, faulty_addresses, masked_faults, aging_active, batch_size, verbose, weights_faults)\u001b[0m\n\u001b[0;32m    109\u001b[0m qNet \u001b[38;5;241m=\u001b[39m GetNeuralNetworkModel(architecture,input_shape,output_shape,faulty_addresses,masked_faults,aging_active\u001b[38;5;241m=\u001b[39maging_active,\n\u001b[0;32m    110\u001b[0m \t\t\t\t\t\t\t word_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m+\u001b[39mact_frac_size\u001b[38;5;241m+\u001b[39mact_int_size), frac_size\u001b[38;5;241m=\u001b[39mact_frac_size, batch_size \u001b[38;5;241m=\u001b[39m batch_size)\n\u001b[0;32m    111\u001b[0m \u001b[38;5;66;03m#print('valor de aging_active', aging_active)\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \n\u001b[0;32m    113\u001b[0m \n\u001b[0;32m    114\u001b[0m \u001b[38;5;66;03m#Load Weights\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m \u001b[43mqNet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwgt_dir\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mexpect_partial()\n\u001b[0;32m    116\u001b[0m pesos_antes \u001b[38;5;241m=\u001b[39m qNet\u001b[38;5;241m.\u001b[39mget_weights()\n\u001b[0;32m    117\u001b[0m \u001b[38;5;66;03m#print(len(pesos_antes))\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;66;03m#print('pesos antes',pesos_antes)\u001b[39;00m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;66;03m#print('qnet.load',qNet.load_weights(wgt_dir).expect_partial())\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;66;03m#Quantize Weights\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\env_first\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2297\u001b[0m, in \u001b[0;36mModel.load_weights\u001b[1;34m(self, filepath, by_name, skip_mismatch, options)\u001b[0m\n\u001b[0;32m   2295\u001b[0m filepath, save_format \u001b[38;5;241m=\u001b[39m _detect_save_format(filepath)\n\u001b[0;32m   2296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m save_format \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m-> 2297\u001b[0m   status \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_trackable_saver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrestore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2298\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m by_name:\n\u001b[0;32m   2299\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m   2300\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWeights may only be loaded based on topology into Models when \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   2301\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloading TensorFlow-formatted weights (got by_name=True to \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   2302\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mload_weights).\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\env_first\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\util.py:1338\u001b[0m, in \u001b[0;36mTrackableSaver.restore\u001b[1;34m(self, save_path, options)\u001b[0m\n\u001b[0;32m   1330\u001b[0m object_graph_proto\u001b[38;5;241m.\u001b[39mParseFromString(object_graph_string)\n\u001b[0;32m   1331\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m _CheckpointRestoreCoordinator(\n\u001b[0;32m   1332\u001b[0m     object_graph_proto\u001b[38;5;241m=\u001b[39mobject_graph_proto,\n\u001b[0;32m   1333\u001b[0m     save_path\u001b[38;5;241m=\u001b[39msave_path,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1336\u001b[0m     graph_view\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph_view,\n\u001b[0;32m   1337\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions)\n\u001b[1;32m-> 1338\u001b[0m \u001b[43mbase\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCheckpointPosition\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproto_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrestore\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph_view\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1341\u001b[0m \u001b[38;5;66;03m# Attached dependencies are not attached to the root, so should be restored\u001b[39;00m\n\u001b[0;32m   1342\u001b[0m \u001b[38;5;66;03m# separately.\u001b[39;00m\n\u001b[0;32m   1343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph_view\u001b[38;5;241m.\u001b[39mattached_dependencies:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\env_first\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py:258\u001b[0m, in \u001b[0;36mCheckpointPosition.restore\u001b[1;34m(self, trackable)\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39minit_scope():\n\u001b[0;32m    255\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbind_object(trackable):\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;66;03m# This object's correspondence with a checkpointed object is new, so\u001b[39;00m\n\u001b[0;32m    257\u001b[0m     \u001b[38;5;66;03m# process deferred restorations for it and its dependencies.\u001b[39;00m\n\u001b[1;32m--> 258\u001b[0m     restore_ops \u001b[38;5;241m=\u001b[39m \u001b[43mtrackable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_restore_from_checkpoint_position\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m restore_ops:\n\u001b[0;32m    260\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint\u001b[38;5;241m.\u001b[39mnew_restore_ops(restore_ops)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\env_first\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py:977\u001b[0m, in \u001b[0;36mTrackable._restore_from_checkpoint_position\u001b[1;34m(self, checkpoint_position)\u001b[0m\n\u001b[0;32m    974\u001b[0m   tensor_saveables\u001b[38;5;241m.\u001b[39mupdate(new_tensor_saveables)\n\u001b[0;32m    975\u001b[0m   python_saveables\u001b[38;5;241m.\u001b[39mextend(new_python_saveables)\n\u001b[0;32m    976\u001b[0m restore_ops\u001b[38;5;241m.\u001b[39mextend(\n\u001b[1;32m--> 977\u001b[0m     \u001b[43mcurrent_position\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrestore_saveables\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtensor_saveables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpython_saveables\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    979\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m restore_ops\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\env_first\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\util.py:308\u001b[0m, in \u001b[0;36m_CheckpointRestoreCoordinator.restore_saveables\u001b[1;34m(self, tensor_saveables, python_saveables)\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mset\u001b[39m(tensor_saveables\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;241m!=\u001b[39m validated_names:\n\u001b[0;32m    305\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    306\u001b[0m       (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaveable keys changed when validating. Got back \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, was \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    307\u001b[0m        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpecting \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m%\u001b[39m (tensor_saveables\u001b[38;5;241m.\u001b[39mkeys(), validated_names))\n\u001b[1;32m--> 308\u001b[0m new_restore_ops \u001b[38;5;241m=\u001b[39m \u001b[43mfunctional_saver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMultiDeviceSaver\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    309\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidated_saveables\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrestore\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_path_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m    311\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m name, restore_op \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(new_restore_ops\u001b[38;5;241m.\u001b[39mitems()):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\env_first\\lib\\site-packages\\tensorflow\\python\\training\\saving\\functional_saver.py:339\u001b[0m, in \u001b[0;36mMultiDeviceSaver.restore\u001b[1;34m(self, file_prefix, options)\u001b[0m\n\u001b[0;32m    337\u001b[0m   restore_ops \u001b[38;5;241m=\u001b[39m tf_function_restore()\n\u001b[0;32m    338\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 339\u001b[0m   restore_ops \u001b[38;5;241m=\u001b[39m \u001b[43mrestore_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_restore_callbacks:\n\u001b[0;32m    342\u001b[0m   callback()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\env_first\\lib\\site-packages\\tensorflow\\python\\training\\saving\\functional_saver.py:323\u001b[0m, in \u001b[0;36mMultiDeviceSaver.restore.<locals>.restore_fn\u001b[1;34m()\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m device, saver \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_single_device_savers\u001b[38;5;241m.\u001b[39mitems()):\n\u001b[0;32m    322\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mdevice(device):\n\u001b[1;32m--> 323\u001b[0m     restore_ops\u001b[38;5;241m.\u001b[39mupdate(\u001b[43msaver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrestore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_prefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m restore_ops\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\env_first\\lib\\site-packages\\tensorflow\\python\\training\\saving\\functional_saver.py:115\u001b[0m, in \u001b[0;36m_SingleDeviceSaver.restore\u001b[1;34m(self, file_prefix, options)\u001b[0m\n\u001b[0;32m    112\u001b[0m restore_ops \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m saveable, restored_tensors \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_saveable_objects,\n\u001b[0;32m    114\u001b[0m                                       structured_restored_tensors):\n\u001b[1;32m--> 115\u001b[0m   restore_ops[saveable\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m \u001b[43msaveable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrestore\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[43m      \u001b[49m\u001b[43mrestored_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrestored_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m restore_ops\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\env_first\\lib\\site-packages\\tensorflow\\python\\training\\saving\\saveable_object_util.py:131\u001b[0m, in \u001b[0;36mResourceVariableSaveable.restore\u001b[1;34m(self, restored_tensors, restored_shapes)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_var_device):\n\u001b[0;32m    130\u001b[0m   restored_tensor \u001b[38;5;241m=\u001b[39m array_ops\u001b[38;5;241m.\u001b[39midentity(restored_tensor)\n\u001b[1;32m--> 131\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mresource_variable_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape_safe_assign_variable_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_op\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_var_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrestored_tensor\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\env_first\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:308\u001b[0m, in \u001b[0;36mshape_safe_assign_variable_handle\u001b[1;34m(handle, shape, value, name)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _handle_graph(handle):\n\u001b[0;32m    307\u001b[0m   value_tensor \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(value)\n\u001b[1;32m--> 308\u001b[0m \u001b[43mshape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_is_compatible_with\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m gen_resource_variable_ops\u001b[38;5;241m.\u001b[39massign_variable_op(\n\u001b[0;32m    310\u001b[0m     handle, value_tensor, name\u001b[38;5;241m=\u001b[39mname)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\env_first\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py:1161\u001b[0m, in \u001b[0;36mTensorShape.assert_is_compatible_with\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m   1149\u001b[0m \u001b[38;5;124;03m\"\"\"Raises exception if `self` and `other` do not represent the same shape.\u001b[39;00m\n\u001b[0;32m   1150\u001b[0m \n\u001b[0;32m   1151\u001b[0m \u001b[38;5;124;03mThis method can be used to assert that there exists a shape that both\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1158\u001b[0m \u001b[38;5;124;03m  ValueError: If `self` and `other` do not represent the same shape.\u001b[39;00m\n\u001b[0;32m   1159\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_compatible_with(other):\n\u001b[1;32m-> 1161\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShapes \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m are incompatible\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m, other))\n",
      "\u001b[1;31mValueError\u001b[0m: Shapes (16000, 250) and (8000, 250) are incompatible"
     ]
    }
   ],
   "source": [
    "CheckAccuracyAndLoss('SentimentalNet', test_set, wgt_dir, act_frac_size = 14, act_int_size = 1, wgt_frac_size = 15, wgt_int_size = 0, \n",
    "                    input_shape = (500), output_shape = 1, batch_size = test_batch_size);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Buffer Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buffer sections:  [0, 2000, 4000, 6000, 8000, 10000, 12000, 14000, 16000]\n",
      "Simulation Started, time: 12:42:40 cycles:  0 offset:  0\n",
      "procesed images: 0  time: 12:42:43 cycles:  13106 offset:  0\n",
      "procesed images: 1  time: 12:42:44 cycles:  26212 offset:  0\n",
      "procesed images: 2  time: 12:42:46 cycles:  39318 offset:  0\n",
      "procesed images: 3  time: 12:42:47 cycles:  52424 offset:  0\n",
      "procesed images: 4  time: 12:42:49 cycles:  65530 offset:  0\n",
      "procesed images: 5  time: 12:42:50 cycles:  78636 offset:  0\n",
      "procesed images: 6  time: 12:42:52 cycles:  91742 offset:  0\n",
      "procesed images: 7  time: 12:42:52 cycles:  104848 offset:  0\n",
      "procesed images: 8  time: 12:42:54 cycles:  117954 offset:  0\n",
      "procesed images: 9  time: 12:42:55 cycles:  131060 offset:  0\n",
      "procesed images: 10  time: 12:42:57 cycles:  144166 offset:  0\n",
      "procesed images: 11  time: 12:42:58 cycles:  157272 offset:  0\n",
      "procesed images: 12  time: 12:43:00 cycles:  170378 offset:  0\n",
      "procesed images: 13  time: 12:43:01 cycles:  183484 offset:  0\n",
      "procesed images: 14  time: 12:43:03 cycles:  196590 offset:  0\n",
      "procesed images: 15  time: 12:43:03 cycles:  209696 offset:  0\n",
      "procesed images: 16  time: 12:43:05 cycles:  222802 offset:  0\n",
      "procesed images: 17  time: 12:43:06 cycles:  235908 offset:  0\n",
      "procesed images: 18  time: 12:43:08 cycles:  249014 offset:  0\n",
      "procesed images: 19  time: 12:43:09 cycles:  262120 offset:  0\n",
      "procesed images: 20  time: 12:43:11 cycles:  275226 offset:  0\n",
      "procesed images: 21  time: 12:43:11 cycles:  288332 offset:  0\n",
      "procesed images: 22  time: 12:43:13 cycles:  301438 offset:  0\n",
      "procesed images: 23  time: 12:43:14 cycles:  314544 offset:  0\n",
      "procesed images: 24  time: 12:43:16 cycles:  327650 offset:  0\n",
      "procesed images: 25  time: 12:43:17 cycles:  340756 offset:  0\n",
      "procesed images: 26  time: 12:43:19 cycles:  353862 offset:  0\n",
      "procesed images: 27  time: 12:43:19 cycles:  366968 offset:  0\n",
      "procesed images: 28  time: 12:43:21 cycles:  380074 offset:  0\n",
      "procesed images: 29  time: 12:43:22 cycles:  393180 offset:  0\n",
      "procesed images: 30  time: 12:43:24 cycles:  406286 offset:  0\n",
      "procesed images: 31  time: 12:43:25 cycles:  419392 offset:  0\n",
      "procesed images: 32  time: 12:43:27 cycles:  432498 offset:  0\n",
      "procesed images: 33  time: 12:43:28 cycles:  445604 offset:  0\n",
      "procesed images: 34  time: 12:43:30 cycles:  458710 offset:  0\n",
      "procesed images: 35  time: 12:43:30 cycles:  471816 offset:  0\n",
      "procesed images: 36  time: 12:43:32 cycles:  484922 offset:  0\n",
      "procesed images: 37  time: 12:43:33 cycles:  498028 offset:  0\n",
      "procesed images: 38  time: 12:43:35 cycles:  511134 offset:  0\n",
      "procesed images: 39  time: 12:43:36 cycles:  524240 offset:  0\n",
      "procesed images: 40  time: 12:43:38 cycles:  537346 offset:  0\n",
      "procesed images: 41  time: 12:43:39 cycles:  550452 offset:  0\n",
      "procesed images: 42  time: 12:43:41 cycles:  563558 offset:  0\n",
      "procesed images: 43  time: 12:43:41 cycles:  576664 offset:  0\n",
      "procesed images: 44  time: 12:43:43 cycles:  589770 offset:  0\n",
      "procesed images: 45  time: 12:43:44 cycles:  602876 offset:  0\n",
      "procesed images: 46  time: 12:43:46 cycles:  615982 offset:  0\n",
      "procesed images: 47  time: 12:43:47 cycles:  629088 offset:  0\n",
      "procesed images: 48  time: 12:43:49 cycles:  642194 offset:  0\n",
      "procesed images: 49  time: 12:43:49 cycles:  655300 offset:  0\n",
      "procesed images: 50  time: 12:43:51 cycles:  668406 offset:  0\n",
      "procesed images: 51  time: 12:43:52 cycles:  681512 offset:  0\n",
      "procesed images: 52  time: 12:43:54 cycles:  694618 offset:  0\n",
      "procesed images: 53  time: 12:43:55 cycles:  707724 offset:  0\n",
      "procesed images: 54  time: 12:43:57 cycles:  720830 offset:  0\n",
      "procesed images: 55  time: 12:43:57 cycles:  733936 offset:  0\n",
      "procesed images: 56  time: 12:43:59 cycles:  747042 offset:  0\n",
      "procesed images: 57  time: 12:44:00 cycles:  760148 offset:  0\n",
      "procesed images: 58  time: 12:44:02 cycles:  773254 offset:  0\n",
      "procesed images: 59  time: 12:44:03 cycles:  786360 offset:  0\n",
      "procesed images: 60  time: 12:44:05 cycles:  799466 offset:  0\n",
      "procesed images: 61  time: 12:44:06 cycles:  812572 offset:  0\n",
      "procesed images: 62  time: 12:44:08 cycles:  825678 offset:  0\n",
      "procesed images: 63  time: 12:44:08 cycles:  838784 offset:  0\n",
      "procesed images: 64  time: 12:44:10 cycles:  851890 offset:  0\n",
      "procesed images: 65  time: 12:44:11 cycles:  864996 offset:  0\n",
      "procesed images: 66  time: 12:44:13 cycles:  878102 offset:  0\n",
      "procesed images: 67  time: 12:44:14 cycles:  891208 offset:  0\n",
      "procesed images: 68  time: 12:44:16 cycles:  904314 offset:  0\n",
      "procesed images: 69  time: 12:44:16 cycles:  917420 offset:  0\n",
      "procesed images: 70  time: 12:44:18 cycles:  930526 offset:  0\n",
      "procesed images: 71  time: 12:44:19 cycles:  943632 offset:  0\n",
      "procesed images: 72  time: 12:44:21 cycles:  956738 offset:  0\n",
      "procesed images: 73  time: 12:44:22 cycles:  969844 offset:  0\n",
      "procesed images: 74  time: 12:44:24 cycles:  982950 offset:  0\n",
      "procesed images: 75  time: 12:44:24 cycles:  996056 offset:  0\n",
      "procesed images: 76  time: 12:44:26 cycles:  1009162 offset:  0\n",
      "procesed images: 77  time: 12:44:27 cycles:  1022268 offset:  0\n",
      "procesed images: 78  time: 12:44:29 cycles:  1035374 offset:  0\n",
      "procesed images: 79  time: 12:44:30 cycles:  1048480 offset:  0\n",
      "procesed images: 80  time: 12:44:32 cycles:  1061586 offset:  0\n",
      "procesed images: 81  time: 12:44:32 cycles:  1074692 offset:  0\n",
      "procesed images: 82  time: 12:44:34 cycles:  1087798 offset:  0\n",
      "procesed images: 83  time: 12:44:35 cycles:  1100904 offset:  0\n",
      "procesed images: 84  time: 12:44:37 cycles:  1114010 offset:  0\n",
      "procesed images: 85  time: 12:44:38 cycles:  1127116 offset:  0\n",
      "procesed images: 86  time: 12:44:40 cycles:  1140222 offset:  0\n",
      "procesed images: 87  time: 12:44:41 cycles:  1153328 offset:  0\n",
      "procesed images: 88  time: 12:44:43 cycles:  1166434 offset:  0\n",
      "procesed images: 89  time: 12:44:43 cycles:  1179540 offset:  0\n",
      "procesed images: 90  time: 12:44:45 cycles:  1192646 offset:  0\n",
      "procesed images: 91  time: 12:44:46 cycles:  1205752 offset:  0\n",
      "procesed images: 92  time: 12:44:48 cycles:  1218858 offset:  0\n",
      "procesed images: 93  time: 12:44:49 cycles:  1231964 offset:  0\n",
      "procesed images: 94  time: 12:44:51 cycles:  1245070 offset:  0\n",
      "procesed images: 95  time: 12:44:52 cycles:  1258176 offset:  0\n",
      "procesed images: 96  time: 12:44:53 cycles:  1271282 offset:  0\n",
      "procesed images: 97  time: 12:44:54 cycles:  1284388 offset:  0\n",
      "procesed images: 98  time: 12:44:56 cycles:  1297494 offset:  0\n",
      "procesed images: 99  time: 12:44:57 cycles:  1310600 offset:  0\n",
      "procesed images: 100  time: 12:44:59 cycles:  1323706 offset:  0\n",
      "procesed images: 101  time: 12:45:00 cycles:  1336812 offset:  0\n",
      "procesed images: 102  time: 12:45:02 cycles:  1349918 offset:  0\n",
      "procesed images: 103  time: 12:45:02 cycles:  1363024 offset:  0\n",
      "procesed images: 104  time: 12:45:04 cycles:  1376130 offset:  0\n",
      "procesed images: 105  time: 12:45:05 cycles:  1389236 offset:  0\n",
      "procesed images: 106  time: 12:45:07 cycles:  1402342 offset:  0\n",
      "procesed images: 107  time: 12:45:08 cycles:  1415448 offset:  0\n",
      "procesed images: 108  time: 12:45:10 cycles:  1428554 offset:  0\n",
      "procesed images: 109  time: 12:45:10 cycles:  1441660 offset:  0\n",
      "procesed images: 110  time: 12:45:12 cycles:  1454766 offset:  0\n",
      "procesed images: 111  time: 12:45:13 cycles:  1467872 offset:  0\n",
      "procesed images: 112  time: 12:45:15 cycles:  1480978 offset:  0\n",
      "procesed images: 113  time: 12:45:16 cycles:  1494084 offset:  0\n",
      "procesed images: 114  time: 12:45:18 cycles:  1507190 offset:  0\n",
      "procesed images: 115  time: 12:45:19 cycles:  1520296 offset:  0\n",
      "procesed images: 116  time: 12:45:21 cycles:  1533402 offset:  0\n",
      "procesed images: 117  time: 12:45:21 cycles:  1546508 offset:  0\n",
      "procesed images: 118  time: 12:45:23 cycles:  1559614 offset:  0\n",
      "procesed images: 119  time: 12:45:24 cycles:  1572720 offset:  0\n",
      "procesed images: 120  time: 12:45:26 cycles:  1585826 offset:  0\n",
      "procesed images: 121  time: 12:45:27 cycles:  1598932 offset:  0\n",
      "procesed images: 122  time: 12:45:29 cycles:  1612038 offset:  0\n",
      "procesed images: 123  time: 12:45:29 cycles:  1625144 offset:  0\n",
      "procesed images: 124  time: 12:45:31 cycles:  1638250 offset:  0\n",
      "procesed images: 125  time: 12:45:32 cycles:  1651356 offset:  0\n",
      "procesed images: 126  time: 12:45:34 cycles:  1664462 offset:  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "procesed images: 127  time: 12:45:35 cycles:  1677568 offset:  0\n",
      "procesed images: 128  time: 12:45:37 cycles:  1690674 offset:  0\n",
      "procesed images: 129  time: 12:45:38 cycles:  1703780 offset:  0\n",
      "procesed images: 130  time: 12:45:40 cycles:  1716886 offset:  0\n",
      "procesed images: 131  time: 12:45:40 cycles:  1729992 offset:  0\n",
      "procesed images: 132  time: 12:45:43 cycles:  1743098 offset:  0\n",
      "procesed images: 133  time: 12:45:43 cycles:  1756204 offset:  0\n",
      "procesed images: 134  time: 12:45:45 cycles:  1769310 offset:  0\n",
      "procesed images: 135  time: 12:45:46 cycles:  1782416 offset:  0\n",
      "procesed images: 136  time: 12:45:48 cycles:  1795522 offset:  0\n",
      "procesed images: 137  time: 12:45:49 cycles:  1808628 offset:  0\n",
      "procesed images: 138  time: 12:45:51 cycles:  1821734 offset:  0\n",
      "procesed images: 139  time: 12:45:51 cycles:  1834840 offset:  0\n",
      "procesed images: 140  time: 12:45:53 cycles:  1847946 offset:  0\n",
      "procesed images: 141  time: 12:45:54 cycles:  1861052 offset:  0\n",
      "procesed images: 142  time: 12:45:56 cycles:  1874158 offset:  0\n",
      "procesed images: 143  time: 12:45:57 cycles:  1887264 offset:  0\n",
      "procesed images: 144  time: 12:45:59 cycles:  1900370 offset:  0\n",
      "procesed images: 145  time: 12:46:00 cycles:  1913476 offset:  0\n",
      "procesed images: 146  time: 12:46:02 cycles:  1926582 offset:  0\n",
      "procesed images: 147  time: 12:46:02 cycles:  1939688 offset:  0\n",
      "procesed images: 148  time: 12:46:04 cycles:  1952794 offset:  0\n",
      "procesed images: 149  time: 12:46:05 cycles:  1965900 offset:  0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'Number of Addresses': 16000,\n",
       "  'Data': array([0, 0, 0, ..., 1, 1, 0], dtype=int8),\n",
       "  'HighCyclesCount': array([330836,  26792,  32308, ..., 816060, 629082, 955544], dtype=uint32),\n",
       "  'OffCyclesCount': array([0, 0, 0, ..., 0, 0, 0], dtype=uint32),\n",
       "  'LowCyclesCount': array([1635064, 1939108, 1933592, ..., 1149840, 1336818, 1010356],\n",
       "        dtype=uint32),\n",
       "  'Flips': array([146,  68,  82, ...,  85,  67,  88], dtype=uint32),\n",
       "  'offset': 0},\n",
       " 1965900)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainBatchSize = testBatchSize = 1\n",
    "_,_,testSet = GetIMBDDataset(train_batch_size=1, test_batch_size=1)\n",
    "\n",
    "QSentimentalNet  = GetNeuralNetworkModel('SentimentalNet',(500),1,quantization = True, aging=False,wordSize = 16, fracSize = 15)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "QSentimentalNet.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "QSentimentalNet.load_weights(wgtDir).expect_partial()\n",
    "WeightQuantization(model = QSentimentalNet, fracBits = 15, intBits = 0)\n",
    "\n",
    "\n",
    "LI = [1,4,8,12,16]\n",
    "AI = [3,7,10,15,19]\n",
    "buffer_simulation(QSentimentalNet,testSet, integer_bits = 0, fractional_bits = 15, samples = 150, start_from = 0,\n",
    "                 bit_invertion = False, bit_shifting = False, CNN_gating = False, write_mode ='default',\n",
    "                 results_dir = 'Data/Stats/SentimentalNet/IMDB Reviews Dataset/Baseline/',buffer_size = 2*16000,\n",
    "                 layer_indexes = LI , activation_indixes = AI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; CNN-Gated, case 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "c:\\users\\nicol\\anaconda3\\envs\\tf_env\\lib\\site-packages\\tensorflow\\python\\keras\\datasets\\imdb.py:155: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "c:\\users\\nicol\\anaconda3\\envs\\tf_env\\lib\\site-packages\\tensorflow\\python\\keras\\datasets\\imdb.py:156: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buffer sections:  [0, 131072, 262144, 393216, 524288, 655360, 786432, 917504, 1048576]\n",
      "Simulation Started, time: 12:53:19 cycles:  0 offset:  0\n",
      "procesed images: 0  time: 12:53:24 cycles:  13106 offset:  393216\n",
      "procesed images: 1  time: 12:53:26 cycles:  26212 offset:  655360\n",
      "procesed images: 2  time: 12:53:29 cycles:  39318 offset:  1048576\n",
      "procesed images: 3  time: 12:53:31 cycles:  52424 offset:  262144\n",
      "procesed images: 4  time: 12:53:34 cycles:  65530 offset:  655360\n",
      "procesed images: 5  time: 12:53:35 cycles:  78636 offset:  917504\n",
      "procesed images: 6  time: 12:53:38 cycles:  91742 offset:  262144\n",
      "procesed images: 7  time: 12:53:40 cycles:  104848 offset:  524288\n",
      "procesed images: 8  time: 12:53:43 cycles:  117954 offset:  917504\n",
      "procesed images: 9  time: 12:53:45 cycles:  131060 offset:  131072\n",
      "procesed images: 10  time: 12:53:48 cycles:  144166 offset:  524288\n",
      "procesed images: 11  time: 12:53:49 cycles:  157272 offset:  786432\n",
      "procesed images: 12  time: 12:53:53 cycles:  170378 offset:  131072\n",
      "procesed images: 13  time: 12:53:54 cycles:  183484 offset:  393216\n",
      "procesed images: 14  time: 12:53:57 cycles:  196590 offset:  786432\n",
      "procesed images: 15  time: 12:53:59 cycles:  209696 offset:  1048576\n",
      "procesed images: 16  time: 12:54:02 cycles:  222802 offset:  393216\n",
      "procesed images: 17  time: 12:54:04 cycles:  235908 offset:  655360\n",
      "procesed images: 18  time: 12:54:07 cycles:  249014 offset:  1048576\n",
      "procesed images: 19  time: 12:54:09 cycles:  262120 offset:  262144\n",
      "procesed images: 20  time: 12:54:12 cycles:  275226 offset:  655360\n",
      "procesed images: 21  time: 12:54:13 cycles:  288332 offset:  917504\n",
      "procesed images: 22  time: 12:54:16 cycles:  301438 offset:  262144\n",
      "procesed images: 23  time: 12:54:18 cycles:  314544 offset:  524288\n",
      "procesed images: 24  time: 12:54:21 cycles:  327650 offset:  917504\n",
      "procesed images: 25  time: 12:54:23 cycles:  340756 offset:  131072\n",
      "procesed images: 26  time: 12:54:26 cycles:  353862 offset:  524288\n",
      "procesed images: 27  time: 12:54:28 cycles:  366968 offset:  786432\n",
      "procesed images: 28  time: 12:54:31 cycles:  380074 offset:  131072\n",
      "procesed images: 29  time: 12:54:33 cycles:  393180 offset:  393216\n",
      "procesed images: 30  time: 12:54:36 cycles:  406286 offset:  786432\n",
      "procesed images: 31  time: 12:54:38 cycles:  419392 offset:  1048576\n",
      "procesed images: 32  time: 12:54:41 cycles:  432498 offset:  393216\n",
      "procesed images: 33  time: 12:54:42 cycles:  445604 offset:  655360\n",
      "procesed images: 34  time: 12:54:45 cycles:  458710 offset:  1048576\n",
      "procesed images: 35  time: 12:54:47 cycles:  471816 offset:  262144\n",
      "procesed images: 36  time: 12:54:50 cycles:  484922 offset:  655360\n",
      "procesed images: 37  time: 12:54:52 cycles:  498028 offset:  917504\n",
      "procesed images: 38  time: 12:54:55 cycles:  511134 offset:  262144\n",
      "procesed images: 39  time: 12:54:57 cycles:  524240 offset:  524288\n",
      "procesed images: 40  time: 12:55:00 cycles:  537346 offset:  917504\n",
      "procesed images: 41  time: 12:55:01 cycles:  550452 offset:  131072\n",
      "procesed images: 42  time: 12:55:05 cycles:  563558 offset:  524288\n",
      "procesed images: 43  time: 12:55:06 cycles:  576664 offset:  786432\n",
      "procesed images: 44  time: 12:55:09 cycles:  589770 offset:  131072\n",
      "procesed images: 45  time: 12:55:11 cycles:  602876 offset:  393216\n",
      "procesed images: 46  time: 12:55:14 cycles:  615982 offset:  786432\n",
      "procesed images: 47  time: 12:55:16 cycles:  629088 offset:  1048576\n",
      "procesed images: 48  time: 12:55:19 cycles:  642194 offset:  393216\n",
      "procesed images: 49  time: 12:55:21 cycles:  655300 offset:  655360\n",
      "procesed images: 50  time: 12:55:24 cycles:  668406 offset:  1048576\n",
      "procesed images: 51  time: 12:55:26 cycles:  681512 offset:  262144\n",
      "procesed images: 52  time: 12:55:29 cycles:  694618 offset:  655360\n",
      "procesed images: 53  time: 12:55:31 cycles:  707724 offset:  917504\n",
      "procesed images: 54  time: 12:55:34 cycles:  720830 offset:  262144\n",
      "procesed images: 55  time: 12:55:35 cycles:  733936 offset:  524288\n",
      "procesed images: 56  time: 12:55:39 cycles:  747042 offset:  917504\n",
      "procesed images: 57  time: 12:55:41 cycles:  760148 offset:  131072\n",
      "procesed images: 58  time: 12:55:44 cycles:  773254 offset:  524288\n",
      "procesed images: 59  time: 12:55:45 cycles:  786360 offset:  786432\n",
      "procesed images: 60  time: 12:55:48 cycles:  799466 offset:  131072\n",
      "procesed images: 61  time: 12:55:50 cycles:  812572 offset:  393216\n",
      "procesed images: 62  time: 12:55:53 cycles:  825678 offset:  786432\n",
      "procesed images: 63  time: 12:55:55 cycles:  838784 offset:  1048576\n",
      "procesed images: 64  time: 12:55:59 cycles:  851890 offset:  393216\n",
      "procesed images: 65  time: 12:56:00 cycles:  864996 offset:  655360\n",
      "procesed images: 66  time: 12:56:03 cycles:  878102 offset:  1048576\n",
      "procesed images: 67  time: 12:56:05 cycles:  891208 offset:  262144\n",
      "procesed images: 68  time: 12:56:08 cycles:  904314 offset:  655360\n",
      "procesed images: 69  time: 12:56:10 cycles:  917420 offset:  917504\n",
      "procesed images: 70  time: 12:56:14 cycles:  930526 offset:  262144\n",
      "procesed images: 71  time: 12:56:15 cycles:  943632 offset:  524288\n",
      "procesed images: 72  time: 12:56:19 cycles:  956738 offset:  917504\n",
      "procesed images: 73  time: 12:56:20 cycles:  969844 offset:  131072\n",
      "procesed images: 74  time: 12:56:23 cycles:  982950 offset:  524288\n",
      "procesed images: 75  time: 12:56:26 cycles:  996056 offset:  786432\n",
      "procesed images: 76  time: 12:56:29 cycles:  1009162 offset:  131072\n",
      "procesed images: 77  time: 12:56:31 cycles:  1022268 offset:  393216\n",
      "procesed images: 78  time: 12:56:34 cycles:  1035374 offset:  786432\n",
      "procesed images: 79  time: 12:56:36 cycles:  1048480 offset:  1048576\n",
      "procesed images: 80  time: 12:56:39 cycles:  1061586 offset:  393216\n",
      "procesed images: 81  time: 12:56:41 cycles:  1074692 offset:  655360\n",
      "procesed images: 82  time: 12:56:44 cycles:  1087798 offset:  1048576\n",
      "procesed images: 83  time: 12:56:46 cycles:  1100904 offset:  262144\n",
      "procesed images: 84  time: 12:56:49 cycles:  1114010 offset:  655360\n",
      "procesed images: 85  time: 12:56:51 cycles:  1127116 offset:  917504\n",
      "procesed images: 86  time: 12:56:54 cycles:  1140222 offset:  262144\n",
      "procesed images: 87  time: 12:56:56 cycles:  1153328 offset:  524288\n",
      "procesed images: 88  time: 12:56:59 cycles:  1166434 offset:  917504\n",
      "procesed images: 89  time: 12:57:00 cycles:  1179540 offset:  131072\n",
      "procesed images: 90  time: 12:57:04 cycles:  1192646 offset:  524288\n",
      "procesed images: 91  time: 12:57:05 cycles:  1205752 offset:  786432\n",
      "procesed images: 92  time: 12:57:08 cycles:  1218858 offset:  131072\n",
      "procesed images: 93  time: 12:57:10 cycles:  1231964 offset:  393216\n",
      "procesed images: 94  time: 12:57:13 cycles:  1245070 offset:  786432\n",
      "procesed images: 95  time: 12:57:15 cycles:  1258176 offset:  1048576\n",
      "procesed images: 96  time: 12:57:18 cycles:  1271282 offset:  393216\n",
      "procesed images: 97  time: 12:57:20 cycles:  1284388 offset:  655360\n",
      "procesed images: 98  time: 12:57:23 cycles:  1297494 offset:  1048576\n",
      "procesed images: 99  time: 12:57:24 cycles:  1310600 offset:  262144\n",
      "procesed images: 100  time: 12:57:28 cycles:  1323706 offset:  655360\n",
      "procesed images: 101  time: 12:57:30 cycles:  1336812 offset:  917504\n",
      "procesed images: 102  time: 12:57:33 cycles:  1349918 offset:  262144\n",
      "procesed images: 103  time: 12:57:35 cycles:  1363024 offset:  524288\n",
      "procesed images: 104  time: 12:57:38 cycles:  1376130 offset:  917504\n",
      "procesed images: 105  time: 12:57:40 cycles:  1389236 offset:  131072\n",
      "procesed images: 106  time: 12:57:43 cycles:  1402342 offset:  524288\n",
      "procesed images: 107  time: 12:57:45 cycles:  1415448 offset:  786432\n",
      "procesed images: 108  time: 12:57:48 cycles:  1428554 offset:  131072\n",
      "procesed images: 109  time: 12:57:50 cycles:  1441660 offset:  393216\n",
      "procesed images: 110  time: 12:57:53 cycles:  1454766 offset:  786432\n",
      "procesed images: 111  time: 12:57:55 cycles:  1467872 offset:  1048576\n",
      "procesed images: 112  time: 12:57:58 cycles:  1480978 offset:  393216\n",
      "procesed images: 113  time: 12:58:00 cycles:  1494084 offset:  655360\n",
      "procesed images: 114  time: 12:58:03 cycles:  1507190 offset:  1048576\n",
      "procesed images: 115  time: 12:58:04 cycles:  1520296 offset:  262144\n",
      "procesed images: 116  time: 12:58:08 cycles:  1533402 offset:  655360\n",
      "procesed images: 117  time: 12:58:09 cycles:  1546508 offset:  917504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "procesed images: 118  time: 12:58:12 cycles:  1559614 offset:  262144\n",
      "procesed images: 119  time: 12:58:14 cycles:  1572720 offset:  524288\n",
      "procesed images: 120  time: 12:58:17 cycles:  1585826 offset:  917504\n",
      "procesed images: 121  time: 12:58:19 cycles:  1598932 offset:  131072\n",
      "procesed images: 122  time: 12:58:22 cycles:  1612038 offset:  524288\n",
      "procesed images: 123  time: 12:58:24 cycles:  1625144 offset:  786432\n",
      "procesed images: 124  time: 12:58:27 cycles:  1638250 offset:  131072\n",
      "procesed images: 125  time: 12:58:29 cycles:  1651356 offset:  393216\n",
      "procesed images: 126  time: 12:58:32 cycles:  1664462 offset:  786432\n",
      "procesed images: 127  time: 12:58:34 cycles:  1677568 offset:  1048576\n",
      "procesed images: 128  time: 12:58:37 cycles:  1690674 offset:  393216\n",
      "procesed images: 129  time: 12:58:39 cycles:  1703780 offset:  655360\n",
      "procesed images: 130  time: 12:58:42 cycles:  1716886 offset:  1048576\n",
      "procesed images: 131  time: 12:58:44 cycles:  1729992 offset:  262144\n",
      "procesed images: 132  time: 12:58:47 cycles:  1743098 offset:  655360\n",
      "procesed images: 133  time: 12:58:49 cycles:  1756204 offset:  917504\n",
      "procesed images: 134  time: 12:58:52 cycles:  1769310 offset:  262144\n",
      "procesed images: 135  time: 12:58:54 cycles:  1782416 offset:  524288\n",
      "procesed images: 136  time: 12:58:57 cycles:  1795522 offset:  917504\n",
      "procesed images: 137  time: 12:58:58 cycles:  1808628 offset:  131072\n",
      "procesed images: 138  time: 12:59:02 cycles:  1821734 offset:  524288\n",
      "procesed images: 139  time: 12:59:03 cycles:  1834840 offset:  786432\n",
      "procesed images: 140  time: 12:59:06 cycles:  1847946 offset:  131072\n",
      "procesed images: 141  time: 12:59:08 cycles:  1861052 offset:  393216\n",
      "procesed images: 142  time: 12:59:11 cycles:  1874158 offset:  786432\n",
      "procesed images: 143  time: 12:59:13 cycles:  1887264 offset:  1048576\n",
      "procesed images: 144  time: 12:59:16 cycles:  1900370 offset:  393216\n",
      "procesed images: 145  time: 12:59:18 cycles:  1913476 offset:  655360\n",
      "procesed images: 146  time: 12:59:21 cycles:  1926582 offset:  1048576\n",
      "procesed images: 147  time: 12:59:22 cycles:  1939688 offset:  262144\n",
      "procesed images: 148  time: 12:59:25 cycles:  1952794 offset:  655360\n",
      "procesed images: 149  time: 12:59:27 cycles:  1965900 offset:  917504\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'Number of Addresses': 1048576,\n",
       "  'Data': array([2, 2, 2, ..., 2, 2, 2], dtype=int8),\n",
       "  'HighCyclesCount': array([ 82694,  34413,  33215, ...,  97722, 111114, 144354], dtype=uint32),\n",
       "  'OffCyclesCount': array([1727008, 1727008, 1727008, ..., 1735500, 1735500, 1735500],\n",
       "        dtype=uint32),\n",
       "  'LowCyclesCount': array([156198, 204479, 205677, ..., 132678, 119286,  86046], dtype=uint32),\n",
       "  'Flips': array([25, 22, 17, ...,  0,  0,  0], dtype=uint32),\n",
       "  'offset': 917504},\n",
       " 1965900)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainBatchSize = testBatchSize = 1\n",
    "_,_,testSet = GetIMBDDataset(train_batch_size=1, test_batch_size=1)\n",
    "\n",
    "QSentimentalNet  = GetNeuralNetworkModel('SentimentalNet',(500),1,quantization = True, aging=False,wordSize = 16, fracSize = 15)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "QSentimentalNet.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "QSentimentalNet.load_weights(wgtDir).expect_partial()\n",
    "WeightQuantization(model = QSentimentalNet, fracBits = 15, intBits = 0)\n",
    "\n",
    "\n",
    "LI = [1,4,8,12,16]\n",
    "AI = [3,7,10,15,19]\n",
    "buffer_simulation(QSentimentalNet,testSet, integer_bits = 0, fractional_bits = 15, samples = 150, start_from = 0,\n",
    "                 bit_invertion = False, bit_shifting = False, CNN_gating = True, write_mode ='default',\n",
    "                 results_dir = 'Data/Stats/SentimentalNet/IMDB Reviews Dataset/CNN-Gated/Full Buffer',buffer_size = 2*1024*1024,\n",
    "                 layer_indexes = LI , activation_indixes = AI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; CNN-Gated, case 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buffer sections:  [0, 2000, 4000, 6000, 8000, 10000, 12000, 14000, 16000]\n",
      "Simulation Started, time: 13:00:41 cycles:  0 offset:  0\n",
      "procesed images: 0  time: 13:00:44 cycles:  13106 offset:  10000\n",
      "procesed images: 1  time: 13:00:45 cycles:  26212 offset:  12000\n",
      "procesed images: 2  time: 13:00:47 cycles:  39318 offset:  6000\n",
      "procesed images: 3  time: 13:00:48 cycles:  52424 offset:  8000\n",
      "procesed images: 4  time: 13:00:50 cycles:  65530 offset:  2000\n",
      "procesed images: 5  time: 13:00:50 cycles:  78636 offset:  4000\n",
      "procesed images: 6  time: 13:00:52 cycles:  91742 offset:  14000\n",
      "procesed images: 7  time: 13:00:53 cycles:  104848 offset:  16000\n",
      "procesed images: 8  time: 13:00:55 cycles:  117954 offset:  10000\n",
      "procesed images: 9  time: 13:00:56 cycles:  131060 offset:  12000\n",
      "procesed images: 10  time: 13:00:58 cycles:  144166 offset:  6000\n",
      "procesed images: 11  time: 13:00:58 cycles:  157272 offset:  8000\n",
      "procesed images: 12  time: 13:01:00 cycles:  170378 offset:  2000\n",
      "procesed images: 13  time: 13:01:01 cycles:  183484 offset:  4000\n",
      "procesed images: 14  time: 13:01:03 cycles:  196590 offset:  14000\n",
      "procesed images: 15  time: 13:01:04 cycles:  209696 offset:  16000\n",
      "procesed images: 16  time: 13:01:06 cycles:  222802 offset:  10000\n",
      "procesed images: 17  time: 13:01:07 cycles:  235908 offset:  12000\n",
      "procesed images: 18  time: 13:01:09 cycles:  249014 offset:  6000\n",
      "procesed images: 19  time: 13:01:09 cycles:  262120 offset:  8000\n",
      "procesed images: 20  time: 13:01:11 cycles:  275226 offset:  2000\n",
      "procesed images: 21  time: 13:01:12 cycles:  288332 offset:  4000\n",
      "procesed images: 22  time: 13:01:14 cycles:  301438 offset:  14000\n",
      "procesed images: 23  time: 13:01:15 cycles:  314544 offset:  16000\n",
      "procesed images: 24  time: 13:01:17 cycles:  327650 offset:  10000\n",
      "procesed images: 25  time: 13:01:18 cycles:  340756 offset:  12000\n",
      "procesed images: 26  time: 13:01:20 cycles:  353862 offset:  6000\n",
      "procesed images: 27  time: 13:01:20 cycles:  366968 offset:  8000\n",
      "procesed images: 28  time: 13:01:22 cycles:  380074 offset:  2000\n",
      "procesed images: 29  time: 13:01:23 cycles:  393180 offset:  4000\n",
      "procesed images: 30  time: 13:01:25 cycles:  406286 offset:  14000\n",
      "procesed images: 31  time: 13:01:26 cycles:  419392 offset:  16000\n",
      "procesed images: 32  time: 13:01:28 cycles:  432498 offset:  10000\n",
      "procesed images: 33  time: 13:01:28 cycles:  445604 offset:  12000\n",
      "procesed images: 34  time: 13:01:30 cycles:  458710 offset:  6000\n",
      "procesed images: 35  time: 13:01:31 cycles:  471816 offset:  8000\n",
      "procesed images: 36  time: 13:01:33 cycles:  484922 offset:  2000\n",
      "procesed images: 37  time: 13:01:34 cycles:  498028 offset:  4000\n",
      "procesed images: 38  time: 13:01:36 cycles:  511134 offset:  14000\n",
      "procesed images: 39  time: 13:01:37 cycles:  524240 offset:  16000\n",
      "procesed images: 40  time: 13:01:39 cycles:  537346 offset:  10000\n",
      "procesed images: 41  time: 13:01:39 cycles:  550452 offset:  12000\n",
      "procesed images: 42  time: 13:01:41 cycles:  563558 offset:  6000\n",
      "procesed images: 43  time: 13:01:42 cycles:  576664 offset:  8000\n",
      "procesed images: 44  time: 13:01:44 cycles:  589770 offset:  2000\n",
      "procesed images: 45  time: 13:01:45 cycles:  602876 offset:  4000\n",
      "procesed images: 46  time: 13:01:47 cycles:  615982 offset:  14000\n",
      "procesed images: 47  time: 13:01:47 cycles:  629088 offset:  16000\n",
      "procesed images: 48  time: 13:01:49 cycles:  642194 offset:  10000\n",
      "procesed images: 49  time: 13:01:50 cycles:  655300 offset:  12000\n",
      "procesed images: 50  time: 13:01:52 cycles:  668406 offset:  6000\n",
      "procesed images: 51  time: 13:01:53 cycles:  681512 offset:  8000\n",
      "procesed images: 52  time: 13:01:55 cycles:  694618 offset:  2000\n",
      "procesed images: 53  time: 13:01:55 cycles:  707724 offset:  4000\n",
      "procesed images: 54  time: 13:01:57 cycles:  720830 offset:  14000\n",
      "procesed images: 55  time: 13:01:58 cycles:  733936 offset:  16000\n",
      "procesed images: 56  time: 13:02:00 cycles:  747042 offset:  10000\n",
      "procesed images: 57  time: 13:02:01 cycles:  760148 offset:  12000\n",
      "procesed images: 58  time: 13:02:03 cycles:  773254 offset:  6000\n",
      "procesed images: 59  time: 13:02:04 cycles:  786360 offset:  8000\n",
      "procesed images: 60  time: 13:02:06 cycles:  799466 offset:  2000\n",
      "procesed images: 61  time: 13:02:06 cycles:  812572 offset:  4000\n",
      "procesed images: 62  time: 13:02:08 cycles:  825678 offset:  14000\n",
      "procesed images: 63  time: 13:02:09 cycles:  838784 offset:  16000\n",
      "procesed images: 64  time: 13:02:11 cycles:  851890 offset:  10000\n",
      "procesed images: 65  time: 13:02:12 cycles:  864996 offset:  12000\n",
      "procesed images: 66  time: 13:02:14 cycles:  878102 offset:  6000\n",
      "procesed images: 67  time: 13:02:14 cycles:  891208 offset:  8000\n",
      "procesed images: 68  time: 13:02:16 cycles:  904314 offset:  2000\n",
      "procesed images: 69  time: 13:02:17 cycles:  917420 offset:  4000\n",
      "procesed images: 70  time: 13:02:19 cycles:  930526 offset:  14000\n",
      "procesed images: 71  time: 13:02:20 cycles:  943632 offset:  16000\n",
      "procesed images: 72  time: 13:02:22 cycles:  956738 offset:  10000\n",
      "procesed images: 73  time: 13:02:23 cycles:  969844 offset:  12000\n",
      "procesed images: 74  time: 13:02:25 cycles:  982950 offset:  6000\n",
      "procesed images: 75  time: 13:02:25 cycles:  996056 offset:  8000\n",
      "procesed images: 76  time: 13:02:27 cycles:  1009162 offset:  2000\n",
      "procesed images: 77  time: 13:02:28 cycles:  1022268 offset:  4000\n",
      "procesed images: 78  time: 13:02:30 cycles:  1035374 offset:  14000\n",
      "procesed images: 79  time: 13:02:31 cycles:  1048480 offset:  16000\n",
      "procesed images: 80  time: 13:02:33 cycles:  1061586 offset:  10000\n",
      "procesed images: 81  time: 13:02:33 cycles:  1074692 offset:  12000\n",
      "procesed images: 82  time: 13:02:35 cycles:  1087798 offset:  6000\n",
      "procesed images: 83  time: 13:02:36 cycles:  1100904 offset:  8000\n",
      "procesed images: 84  time: 13:02:38 cycles:  1114010 offset:  2000\n",
      "procesed images: 85  time: 13:02:39 cycles:  1127116 offset:  4000\n",
      "procesed images: 86  time: 13:02:41 cycles:  1140222 offset:  14000\n",
      "procesed images: 87  time: 13:02:41 cycles:  1153328 offset:  16000\n",
      "procesed images: 88  time: 13:02:43 cycles:  1166434 offset:  10000\n",
      "procesed images: 89  time: 13:02:44 cycles:  1179540 offset:  12000\n",
      "procesed images: 90  time: 13:02:46 cycles:  1192646 offset:  6000\n",
      "procesed images: 91  time: 13:02:47 cycles:  1205752 offset:  8000\n",
      "procesed images: 92  time: 13:02:49 cycles:  1218858 offset:  2000\n",
      "procesed images: 93  time: 13:02:49 cycles:  1231964 offset:  4000\n",
      "procesed images: 94  time: 13:02:51 cycles:  1245070 offset:  14000\n",
      "procesed images: 95  time: 13:02:52 cycles:  1258176 offset:  16000\n",
      "procesed images: 96  time: 13:02:54 cycles:  1271282 offset:  10000\n",
      "procesed images: 97  time: 13:02:55 cycles:  1284388 offset:  12000\n",
      "procesed images: 98  time: 13:02:57 cycles:  1297494 offset:  6000\n",
      "procesed images: 99  time: 13:02:58 cycles:  1310600 offset:  8000\n",
      "procesed images: 100  time: 13:03:00 cycles:  1323706 offset:  2000\n",
      "procesed images: 101  time: 13:03:00 cycles:  1336812 offset:  4000\n",
      "procesed images: 102  time: 13:03:02 cycles:  1349918 offset:  14000\n",
      "procesed images: 103  time: 13:03:03 cycles:  1363024 offset:  16000\n",
      "procesed images: 104  time: 13:03:05 cycles:  1376130 offset:  10000\n",
      "procesed images: 105  time: 13:03:06 cycles:  1389236 offset:  12000\n",
      "procesed images: 106  time: 13:03:08 cycles:  1402342 offset:  6000\n",
      "procesed images: 107  time: 13:03:08 cycles:  1415448 offset:  8000\n",
      "procesed images: 108  time: 13:03:10 cycles:  1428554 offset:  2000\n",
      "procesed images: 109  time: 13:03:11 cycles:  1441660 offset:  4000\n",
      "procesed images: 110  time: 13:03:13 cycles:  1454766 offset:  14000\n",
      "procesed images: 111  time: 13:03:14 cycles:  1467872 offset:  16000\n",
      "procesed images: 112  time: 13:03:16 cycles:  1480978 offset:  10000\n",
      "procesed images: 113  time: 13:03:16 cycles:  1494084 offset:  12000\n",
      "procesed images: 114  time: 13:03:18 cycles:  1507190 offset:  6000\n",
      "procesed images: 115  time: 13:03:19 cycles:  1520296 offset:  8000\n",
      "procesed images: 116  time: 13:03:21 cycles:  1533402 offset:  2000\n",
      "procesed images: 117  time: 13:03:22 cycles:  1546508 offset:  4000\n",
      "procesed images: 118  time: 13:03:24 cycles:  1559614 offset:  14000\n",
      "procesed images: 119  time: 13:03:25 cycles:  1572720 offset:  16000\n",
      "procesed images: 120  time: 13:03:27 cycles:  1585826 offset:  10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "procesed images: 121  time: 13:03:27 cycles:  1598932 offset:  12000\n",
      "procesed images: 122  time: 13:03:29 cycles:  1612038 offset:  6000\n",
      "procesed images: 123  time: 13:03:30 cycles:  1625144 offset:  8000\n",
      "procesed images: 124  time: 13:03:32 cycles:  1638250 offset:  2000\n",
      "procesed images: 125  time: 13:03:33 cycles:  1651356 offset:  4000\n",
      "procesed images: 126  time: 13:03:35 cycles:  1664462 offset:  14000\n",
      "procesed images: 127  time: 13:03:35 cycles:  1677568 offset:  16000\n",
      "procesed images: 128  time: 13:03:37 cycles:  1690674 offset:  10000\n",
      "procesed images: 129  time: 13:03:38 cycles:  1703780 offset:  12000\n",
      "procesed images: 130  time: 13:03:40 cycles:  1716886 offset:  6000\n",
      "procesed images: 131  time: 13:03:41 cycles:  1729992 offset:  8000\n",
      "procesed images: 132  time: 13:03:43 cycles:  1743098 offset:  2000\n",
      "procesed images: 133  time: 13:03:44 cycles:  1756204 offset:  4000\n",
      "procesed images: 134  time: 13:03:46 cycles:  1769310 offset:  14000\n",
      "procesed images: 135  time: 13:03:46 cycles:  1782416 offset:  16000\n",
      "procesed images: 136  time: 13:03:48 cycles:  1795522 offset:  10000\n",
      "procesed images: 137  time: 13:03:49 cycles:  1808628 offset:  12000\n",
      "procesed images: 138  time: 13:03:51 cycles:  1821734 offset:  6000\n",
      "procesed images: 139  time: 13:03:52 cycles:  1834840 offset:  8000\n",
      "procesed images: 140  time: 13:03:54 cycles:  1847946 offset:  2000\n",
      "procesed images: 141  time: 13:03:54 cycles:  1861052 offset:  4000\n",
      "procesed images: 142  time: 13:03:56 cycles:  1874158 offset:  14000\n",
      "procesed images: 143  time: 13:03:57 cycles:  1887264 offset:  16000\n",
      "procesed images: 144  time: 13:03:59 cycles:  1900370 offset:  10000\n",
      "procesed images: 145  time: 13:04:00 cycles:  1913476 offset:  12000\n",
      "procesed images: 146  time: 13:04:02 cycles:  1926582 offset:  6000\n",
      "procesed images: 147  time: 13:04:03 cycles:  1939688 offset:  8000\n",
      "procesed images: 148  time: 13:04:05 cycles:  1952794 offset:  2000\n",
      "procesed images: 149  time: 13:04:05 cycles:  1965900 offset:  4000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'Number of Addresses': 16000,\n",
       "  'Data': array([2, 2, 2, ..., 2, 2, 2], dtype=int8),\n",
       "  'HighCyclesCount': array([304464,  42996,  47890, ..., 244240, 197748, 394506], dtype=uint32),\n",
       "  'OffCyclesCount': array([989076, 989076, 989076, ..., 845292, 845292, 845292], dtype=uint32),\n",
       "  'LowCyclesCount': array([672360, 933828, 928934, ..., 876368, 922860, 726102], dtype=uint32),\n",
       "  'Flips': array([126,  89,  94, ...,  98, 105, 125], dtype=uint32),\n",
       "  'offset': 4000},\n",
       " 1965900)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainBatchSize = testBatchSize = 1\n",
    "_,_,testSet = GetIMBDDataset(train_batch_size=1, test_batch_size=1)\n",
    "\n",
    "QSentimentalNet  = GetNeuralNetworkModel('SentimentalNet',(500),1,quantization = True, aging=False,wordSize = 16, fracSize = 15)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "QSentimentalNet.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "QSentimentalNet.load_weights(wgtDir).expect_partial()\n",
    "WeightQuantization(model = QSentimentalNet, fracBits = 15, intBits = 0)\n",
    "\n",
    "\n",
    "LI = [1,4,8,12,16]\n",
    "AI = [3,7,10,15,19]\n",
    "buffer_simulation(QSentimentalNet,testSet, integer_bits = 0, fractional_bits = 15, samples = 150, start_from = 0,\n",
    "                 bit_invertion = False, bit_shifting = False, CNN_gating = True, write_mode ='default',\n",
    "                 results_dir = 'Data/Stats/SentimentalNet/IMDB Reviews Dataset/CNN-Gated/', buffer_size = 2*16000,\n",
    "                 layer_indexes = LI , activation_indixes = AI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Error Injection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSet,validSet,testSet = GetIMBDDataset(32, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; a) Measuring effect of errors by buffer,section,number of faults and bit localization of the fault"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from copy import deepcopy\n",
    "from Quantization_and_Errors import Check_Accuracy_and_Loss\n",
    "from Simulation import save_obj, load_obj\n",
    "from datetime import datetime\n",
    "\n",
    "Start_from_zero = False\n",
    "\n",
    "if Start_from_zero:\n",
    "    Accs     = {0.0001:[],0.0005:[],0.001:[],0.005:[],0.01:[]}\n",
    "    Loss     = {0.0001:[],0.0005:[],0.001:[],0.005:[],0.01:[]}\n",
    "else:\n",
    "    Accs     = load_obj('Data/Errors/SentimentalNet/IMDB Reviews Dataset/Accs')\n",
    "    Loss     = load_obj('Data/Errors/SentimentalNet/IMDB Reviews Dataset/Loss')\n",
    "    \n",
    "cell_modifications = list(itertools.product(['1', 'x','0'], repeat=16))\n",
    "possible_errors_under_8000 = [\"\".join(i) for i in cell_modifications]\n",
    "cell_modifications = list(itertools.product(['1', 'x','0'], repeat=5))\n",
    "possible_errors_above_8000 = ['x'+\"\".join(i)+'xxxxxxxxxx' for i in cell_modifications]\n",
    "\n",
    "\n",
    "Num_of_samples = 200\n",
    "for Enumber in Accs:\n",
    "    for index in range(0,Num_of_samples):\n",
    "        if index < Num_of_samples//2:\n",
    "            mask = [True,False,True,False]\n",
    "        else:\n",
    "            mask = [False,True,False,True]\n",
    "        number_of_errors = np.ceil(Enumber*8000).astype(int)\n",
    "        locs1      = np.random.choice(range(0,8000),number_of_errors,False)\n",
    "        Errortype1 = np.random.choice(possible_errors_under_8000, number_of_errors, True)\n",
    "        locs2      = np.random.choice(range(8000,16000),number_of_errors,False)\n",
    "        Errortype2 = np.random.choice(possible_errors_above_8000, number_of_errors, True)\n",
    "        locs       = np.concatenate([locs1,locs2])\n",
    "        Errortype  = np.concatenate([Errortype1,Errortype2])\n",
    "        acc,loss   = CheckAccuracyAndLoss('PilotNet', test_dataset, wgtDir, outputShape=1, inputShape = (500),\n",
    "                                            aFracSize = 14, aIntSize = 1, wFracSize = 15, wIntSize = 0,\n",
    "                                            batchSize=32, verbose = 0, aging = mask,\n",
    "                                            faultyAddresses = locs, maskedFaults = Errortype)\n",
    "        Accs[Enumber].append(acc)\n",
    "        Loss[Enumber].append(loss)\n",
    "        if index % 10 == 0:\n",
    "            print(index)\n",
    "    print(str(Enumber)+' completada: ', datetime.now().strftime(\"%H:%M:%S\"))\n",
    "    #save_obj(Accs,'Data/Errors/SentimentalNet/IMDB Reviews Dataset/Accs')\n",
    "    #save_obj(Loss,'Data/Errors/SentimentalNet/IMDB Reviews Dataset/Loss')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_first",
   "language": "python",
   "name": "env_first"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
