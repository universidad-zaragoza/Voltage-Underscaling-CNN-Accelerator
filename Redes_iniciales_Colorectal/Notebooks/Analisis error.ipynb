{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf5f4f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle as pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from Stats import WeightQuantization,IntroduceFaultsInWeights,GenerateFaultsList\n",
    "from Nets import GetNeuralNetworkModel\n",
    "from Training import GetDatasets\n",
    "from Simulation import get_all_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03e9edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Estudio d elos pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc53b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ok probar las tecnica que tenemos con errores en los pesos\n",
    "Tamaño de los pesos por capas y por redes\n",
    "llevar los Nan a 0 a ver que ocurre\n",
    "crear mapa de fallo de pesos\n",
    "analizar si comienzo dede la posición 0 en adelante hasta el final d ela capa o continuo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02e7c49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle as pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from Stats import WeightQuantization,IntroduceFaultsInWeights,GenerateFaultsList\n",
    "#from Stats_original import WeightQuantization, IntroduceFaultsInWeights, GenerateFaultsList, CheckAccuracyAndLoss\n",
    "from Nets import GetNeuralNetworkModel\n",
    "from Training import GetDatasets\n",
    "from Simulation import get_all_outputs\n",
    "from Simulation import buffer_simulation, save_obj, load_obj\n",
    "from funciones import compilNet, same_elements\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pandas import ExcelWriter\n",
    "from openpyxl import Workbook\n",
    "from openpyxl import load_workbook\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67351031",
   "metadata": {},
   "source": [
    "### Definiciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "264836c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numero de bits para activaciones (a) y pesos (w)\n",
    "word_size  = 16\n",
    "afrac_size = 11  \n",
    "aint_size  = 4\n",
    "wfrac_size = 11\n",
    "wint_size  = 4\n",
    "# Tamaño del buffer de pesos == al tamaño de la capa con mayor numero de pesos (885120 pesos de 16 bits cada uno)\n",
    "wbuffer_size = 885120*word_size\n",
    "# Tamaño del buffer de activaciones == al tamaño de la capa con mayor numero de activaciones (290400 pesos de 16 bits cada uno)\n",
    "abuffer_size = 290400*word_size\n",
    "# Directorio de los pesos\n",
    "cwd = os.getcwd()\n",
    "wgt_dir = os.path.join(cwd, 'Data')\n",
    "wgt_dir = os.path.join(wgt_dir, 'Trained Weights')\n",
    "wgt_dir = os.path.join(wgt_dir, 'AlexNet')\n",
    "wgt_dir = os.path.join(wgt_dir, 'Colorectal Dataset')\n",
    "wgt_dir = os.path.join(wgt_dir,'Weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54787132",
   "metadata": {},
   "source": [
    "### Generacion de una muestra de fallos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b257a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mostrando las 5 primeras direcciones con fallos\n",
      "direcciones: [483, 1393, 1629, 2887, 2888]\n",
      "mascara de fallos: ['1xxxxxxxxxxxxxxx', 'xxxxxxxxxxx1xxxx', '0xxxxxxxxxxxxxxx', 'xxxx0xxxxxxxxxxx', 'xxxxxxxxxxxxxxx0']\n"
     ]
    }
   ],
   "source": [
    "# Decidir si cargar errores de un archivo locs y error_mask o generarlos aleatoriamente\n",
    "Cargar_errores = False\n",
    "\n",
    "if Cargar_errores:\n",
    "    locs  = load_obj('locs')\n",
    "    masks = load_obj('error_mask')\n",
    "else:\n",
    "    numero_bits_con_fallo = 1000\n",
    "    bits_con_fallo = np.random.randint(0,2,numero_bits_con_fallo)\n",
    "    #crear una mascara (m) del buffer de pesos donde x: bit sin fallo, 0: bit con fallo en 0, 1: bit con fallo en 1.\n",
    "    #si quieres introducir fallos en activaciones en lugar de los pesos, simplemente cambia wbuffer_size por abuffer_size.\n",
    "    mbuffer = np.array(['x']*(wbuffer_size-numero_bits_con_fallo))\n",
    "    mbuffer = np.concatenate([mbuffer,bits_con_fallo])\n",
    "    #distribuir los errores aleatoriamente en la mascara del buffer\n",
    "    np.random.shuffle(mbuffer)\n",
    "    #organizar la mascara del buffer por direcciones\n",
    "    mbuffer = np.reshape(mbuffer,(-1,word_size))\n",
    "    mbuffer = [\"\".join(i) for i in mbuffer]\n",
    "    #filtrar dejando solo las direcciones con error\n",
    "    locs  = [x for x,y in enumerate(mbuffer) if y.count('x') < 16]\n",
    "    masks = [y for x,y in enumerate(mbuffer) if y.count('x') < 16] \n",
    "print('mostrando las 5 primeras direcciones con fallos')\n",
    "print('direcciones:',locs[0:5])\n",
    "print('mascara de fallos:',masks[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a53b31",
   "metadata": {},
   "source": [
    "### Cargar el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "717789e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\usuario\\anaconda3\\envs\\env_first\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:3703: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable.debug_mode()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainBatchSize = testBatchSize = 16\n",
    "_,_,test_dataset = GetDatasets('colorectal_histology',(80,5,15),(227,227), 8, trainBatchSize, testBatchSize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2949da",
   "metadata": {},
   "source": [
    "### Crear la red sin fallos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7292d7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 17s 363ms/step - loss: 0.3125 - accuracy: 0.8907\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3124775290489197, 0.890666663646698]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation_aging = False\n",
    "\n",
    "#Acá la creamos, notese que como no se introduciran fallos en activaciones no es necesario pasar locs ni masks\n",
    "Net1 = GetNeuralNetworkModel('AlexNet', (227,227,3), 8, aging_active=activation_aging,\n",
    "                             word_size=word_size, frac_size=afrac_size, batch_size = testBatchSize)\n",
    "Net1.load_weights(wgt_dir).expect_partial()\n",
    "#Cuantizacion de los pesos\n",
    "WeightQuantization(model = Net1, frac_bits = wfrac_size, int_bits = wint_size)\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "Net1.compile(optimizer=optimizer, loss=loss, metrics='accuracy')\n",
    "pesos1 = Net1.get_weights()\n",
    "Net1.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33ba6ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#index=3\n",
    "#weights = Net1.layers[index].get_weights()\n",
    "#capa=Net1.layers[index].__class__.__name__\n",
    "#print(capa)\n",
    "#print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c884a73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 227, 227, 3)]     0         \n",
      "_________________________________________________________________\n",
      "lambda (Lambda)              (None, 227, 227, 3)       0         \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 227, 227, 3)       0         \n",
      "_________________________________________________________________\n",
      "Conv1 (Conv2D)               (None, 55, 55, 96)        34944     \n",
      "_________________________________________________________________\n",
      "lambda_2 (Lambda)            (None, 55, 55, 96)        0         \n",
      "_________________________________________________________________\n",
      "re_lu (ReLU)                 (None, 55, 55, 96)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 55, 55, 96)        384       \n",
      "_________________________________________________________________\n",
      "lambda_3 (Lambda)            (None, 55, 55, 96)        0         \n",
      "_________________________________________________________________\n",
      "lambda_4 (Lambda)            (None, 55, 55, 96)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 27, 27, 96)        0         \n",
      "_________________________________________________________________\n",
      "lambda_5 (Lambda)            (None, 27, 27, 96)        0         \n",
      "_________________________________________________________________\n",
      "Conv2 (Conv2D)               (None, 27, 27, 256)       614656    \n",
      "_________________________________________________________________\n",
      "lambda_6 (Lambda)            (None, 27, 27, 256)       0         \n",
      "_________________________________________________________________\n",
      "re_lu_1 (ReLU)               (None, 27, 27, 256)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 27, 27, 256)       1024      \n",
      "_________________________________________________________________\n",
      "lambda_7 (Lambda)            (None, 27, 27, 256)       0         \n",
      "_________________________________________________________________\n",
      "lambda_8 (Lambda)            (None, 27, 27, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 256)       0         \n",
      "_________________________________________________________________\n",
      "lambda_9 (Lambda)            (None, 13, 13, 256)       0         \n",
      "_________________________________________________________________\n",
      "Conv3 (Conv2D)               (None, 13, 13, 384)       885120    \n",
      "_________________________________________________________________\n",
      "lambda_10 (Lambda)           (None, 13, 13, 384)       0         \n",
      "_________________________________________________________________\n",
      "re_lu_2 (ReLU)               (None, 13, 13, 384)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 13, 13, 384)       1536      \n",
      "_________________________________________________________________\n",
      "lambda_11 (Lambda)           (None, 13, 13, 384)       0         \n",
      "_________________________________________________________________\n",
      "lambda_12 (Lambda)           (None, 13, 13, 384)       0         \n",
      "_________________________________________________________________\n",
      "Conv4 (Conv2D)               (None, 13, 13, 384)       147840    \n",
      "_________________________________________________________________\n",
      "lambda_13 (Lambda)           (None, 13, 13, 384)       0         \n",
      "_________________________________________________________________\n",
      "re_lu_3 (ReLU)               (None, 13, 13, 384)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 13, 13, 384)       1536      \n",
      "_________________________________________________________________\n",
      "lambda_14 (Lambda)           (None, 13, 13, 384)       0         \n",
      "_________________________________________________________________\n",
      "lambda_15 (Lambda)           (None, 13, 13, 384)       0         \n",
      "_________________________________________________________________\n",
      "Conv5 (Conv2D)               (None, 13, 13, 256)       98560     \n",
      "_________________________________________________________________\n",
      "lambda_16 (Lambda)           (None, 13, 13, 256)       0         \n",
      "_________________________________________________________________\n",
      "re_lu_4 (ReLU)               (None, 13, 13, 256)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 13, 13, 256)       1024      \n",
      "_________________________________________________________________\n",
      "lambda_17 (Lambda)           (None, 13, 13, 256)       0         \n",
      "_________________________________________________________________\n",
      "lambda_18 (Lambda)           (None, 13, 13, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "lambda_19 (Lambda)           (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4096)              37752832  \n",
      "_________________________________________________________________\n",
      "lambda_20 (Lambda)           (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "re_lu_5 (ReLU)               (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "lambda_21 (Lambda)           (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "lambda_22 (Lambda)           (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "re_lu_6 (ReLU)               (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "lambda_23 (Lambda)           (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 32776     \n",
      "_________________________________________________________________\n",
      "lambda_24 (Lambda)           (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "tf.compat.v1.nn.softmax (TFO (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "lambda_25 (Lambda)           (None, 8)                 0         \n",
      "=================================================================\n",
      "Total params: 56,353,544\n",
      "Trainable params: 56,350,792\n",
      "Non-trainable params: 2,752\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Net1.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "befb8f1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'InputLayer'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Net1.layers[0].__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26958891",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(0,len(outputs2)):\n",
    "    print('Capa',index,Net2.layers[index].__class__.__name__)\n",
    "    #a=(np.count_nonzero(outputs2[index]))\n",
    "    #b=(outputs2[index].size)\n",
    "    a=outputs1[index]== outputs2[index]\n",
    "    size_output=a.size\n",
    "    print('Cantidad de elementos de la capa:',size_output)\n",
    "    output_true=np.sum(a)\n",
    "    #print('Cantidad de elementos iguales entre modelo con fallos y sin fallos:', output_true)\n",
    "    amount_dif=size_output-output_true\n",
    "    print('Cantidad de elementos diferentes entre modelo con fallos y sin fallos', amount_dif)\n",
    "    ratio=(output_true*100)/size_output\n",
    "    ratio_dif=(amount_dif*100)/size_output\n",
    "    #print('Accurancy:', ratio,'%')\n",
    "    print('Ratio:', ratio_dif,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c00236e",
   "metadata": {},
   "source": [
    "## variante redimensionando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6109ff15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    }
   ],
   "source": [
    "activation_aging = False\n",
    "\n",
    "#cambiare la direccion del primer error y la posicion del fallo,\n",
    "#seteandolo en una direccion baja (que afecte a la capa de normalizacion) y así poder observar la caida\n",
    "#locs[0] = 0\n",
    "#masks[0] = 'xx1xxxxxxxxxxxxx'\n",
    "error_mask_address = load_obj('Data/Fault Characterization/variante_mask_vc_707/error_mask_x/error_mask_x/vc_707/error_mask_054')\n",
    "locs_address = load_obj('Data/Fault Characterization/variante_mask_vc_707/error_mask_x/error_mask_x/vc_707/locs_054')\n",
    "\n",
    "\n",
    "#Acá la creamos, notese que como no se introduciran fallos en activaciones no es necesario pasar locs ni masks\n",
    "Net2 = GetNeuralNetworkModel('AlexNet', (227,227,3), 8, aging_active=activation_aging,\n",
    "                             word_size=word_size, frac_size=afrac_size, batch_size = testBatchSize)\n",
    "Net2.load_weights(wgt_dir).expect_partial()\n",
    "#Cuantizacion de los pesos\n",
    "WeightQuantization(model = Net2, frac_bits = wfrac_size, int_bits = wint_size)\n",
    "#Introducir fallas en cada capa de pesos\n",
    "weights = Net2.get_weights()\n",
    "print(len(weights))\n",
    "#weights_0 = Net2.layers[0].get_weights()\n",
    "##print(weights_0)\n",
    "#for index,layer in enumerate(Net2.layers):\n",
    "#    print(Net2.layers[index].__class__.__name__)\n",
    "#    \n",
    "#    if(len(Net2.layers[index].weights) > 0):\n",
    "#        if layer.__class__.__name__ in ['Conv2D','Conv1D','DepthwiseConv2D']:\n",
    "#        #if Net2.layers[index].__class__.__name__=='Conv2D':\n",
    "#            weights, biases = Net2.layers[index].get_weights()\n",
    "#            weights_conv = np.reshape(weights, (-1))\n",
    "#            print('shape weights_conv',(weights_conv.shape))\n",
    "#            join_weights_conv_bias=np.concatenate((weights_conv, biases))\n",
    "#            print('join perfecto', join_weights_conv_bias.shape)\n",
    "#            \n",
    "#        if  Net2.layers[index].__class__.__name__=='BatchNormalization':\n",
    "#            gamma,beta,moving_mean,moving_std = Net2.layers[index].get_weights()\n",
    "#            gamma = np.reshape(gamma, (-1))\n",
    "#            beta = np.reshape(beta, (-1))\n",
    "#            moving_mean = np.reshape(moving_mean, (-1))\n",
    "#            moving_std = np.reshape(moving_std, (-1))\n",
    "#            weights_1=np.concatenate((gamma,beta,moving_mean,moving_std))\n",
    "#            print('shape de weights_1:', weights_1.shape)  \n",
    "#         \n",
    "#        if  Net2.layers[index].__class__.__name__=='Dense':  \n",
    "#            weights_dense, biases_dense = Net2.layers[index].get_weights()\n",
    "#            weights_dense = np.reshape(weights_dense, (-1))\n",
    "#            biases_dense = np.reshape(biases_dense, (-1))\n",
    "#            weights_2=np.concatenate((weights_dense, biases_dense))\n",
    "#            print('weights_2', weights_2.shape)\n",
    "### El tema está que a la hora d einyectar  los fallos como está diseñado se hace al modleo y de esta forma sería por capas\n",
    "###entonces en este caso cuando se procese una capa deberiamos de mandar a inyectar los errores y luego agregarlo al modelo \n",
    "### ahi no entiendo como manejarlo\n",
    "#positionList,faultList,NumberOfFaults = GenerateFaultsList(shape=itm.shape,locs=faulty_addresses,error_mask=masked_faults)\n",
    "#\n",
    "#    if NumberOfFaults > 0:\n",
    "#        weights[index] = IntroduceFaultsInWeights(itm,positionList,faultList,wint_size,wint_size)\n",
    "#qNet.set_weights(weights)\n",
    "##loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "##optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "##Net2.compile(optimizer=optimizer, loss=loss, metrics='accuracy')\n",
    "##loss,acc  = Net2.evaluate(test_dataset)\n",
    "#                    \n",
    "#            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2241fb2",
   "metadata": {},
   "source": [
    "## Algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ddd8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Algoritmo(crear una función que tome el modelo)\n",
    "le pido los pesos al modelo\n",
    "recorro las el modelo por capas\n",
    "si la capa tiene pesos y es convolución:\n",
    "    calculo la candidad de elementos que tiene la variable weights.size\n",
    "    lo guardo en un variable\n",
    "    la vectorizo a 1 dimensión y a concateno con el bias \n",
    "    le paso ese itm a la función IntroduceFaultsInWeights\n",
    "    luego la separos y las coloco son el shape original \n",
    "    y a model.layers[index] le establezco los pesos ya con fallos\n",
    "si la capa tiene pesos y es Normalizatión:\n",
    "    tiene los parametros gamma,beta,moving_mean,moving_std\n",
    "    hago lo mismo por cada elemento\n",
    "    (seria mejor hacer dos funciones nuevas para que hagan lo de vectorizar y luego separar de nuevo)\n",
    "    model.layers[index] le establezco los pesos ya con fallos\n",
    "Si es un capa Dense:\n",
    "    tiene weights y bias\n",
    "    hago los mismo \n",
    "    model.layers[index] le establezco los pesos ya con fallos\n",
    "la función devuelve el modelo para qu econtinue el algoritmo   \n",
    "\n",
    "para comprobar que funciona imprimo las variables positionList,faultLis\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6190528",
   "metadata": {},
   "source": [
    "## Función"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d4bbde64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2D\n",
      "tamaño d ela capa (None, 55, 55, 96)\n",
      "shape wights_conv_antes (11, 11, 3, 96)\n",
      "shape bias_antes_conv (96,)\n",
      "inyectando errores en los pesos\n",
      "numero de fallos insertados 134\n",
      "tamaño de del array weights 34848\n",
      "cantidad de pocisiones afectada capa 134\n",
      "tamaño de la lista d eposiciones 134\n",
      "tamaño de los weigts 34848\n",
      "cantidad de elementos iguales 34715\n",
      "cantidad de elementos diferentes 133\n",
      "shape bias_conv (96,)\n",
      "Bias cantidad de elementos iguales 96\n",
      "Bias cantidad de elementos diferentes 0\n",
      "set weights\n",
      "BatchNormalization\n",
      "Conv2D\n",
      "tamaño d ela capa (None, 27, 27, 256)\n",
      "shape wights_conv_antes (5, 5, 96, 256)\n",
      "shape bias_antes_conv (256,)\n",
      "inyectando errores en los pesos\n",
      "numero de fallos insertados 6627\n",
      "tamaño de del array weights 614400\n",
      "cantidad de pocisiones afectada capa 6627\n",
      "tamaño de la lista d eposiciones 6627\n",
      "tamaño de los weigts 614400\n",
      "cantidad de elementos iguales 607794\n",
      "cantidad de elementos diferentes 6606\n",
      "shape bias_conv (256,)\n",
      "Bias cantidad de elementos iguales 256\n",
      "Bias cantidad de elementos diferentes 0\n",
      "set weights\n",
      "BatchNormalization\n",
      "Conv2D\n",
      "tamaño d ela capa (None, 13, 13, 384)\n",
      "shape wights_conv_antes (3, 3, 256, 384)\n",
      "shape bias_antes_conv (384,)\n",
      "inyectando errores en los pesos\n",
      "numero de fallos insertados 8159\n",
      "tamaño de del array weights 884736\n",
      "cantidad de pocisiones afectada capa 8159\n",
      "tamaño de la lista d eposiciones 8159\n",
      "tamaño de los weigts 884736\n",
      "cantidad de elementos iguales 876612\n",
      "cantidad de elementos diferentes 8124\n",
      "shape bias_conv (384,)\n",
      "Bias cantidad de elementos iguales 384\n",
      "Bias cantidad de elementos diferentes 0\n",
      "set weights\n",
      "BatchNormalization\n",
      "Conv2D\n",
      "tamaño d ela capa (None, 13, 13, 384)\n",
      "shape wights_conv_antes (1, 1, 384, 384)\n",
      "shape bias_antes_conv (384,)\n",
      "inyectando errores en los pesos\n",
      "numero de fallos insertados 1048\n",
      "tamaño de del array weights 147456\n",
      "cantidad de pocisiones afectada capa 1048\n",
      "tamaño de la lista d eposiciones 1048\n",
      "tamaño de los weigts 147456\n",
      "cantidad de elementos iguales 146411\n",
      "cantidad de elementos diferentes 1045\n",
      "shape bias_conv (384,)\n",
      "Bias cantidad de elementos iguales 384\n",
      "Bias cantidad de elementos diferentes 0\n",
      "set weights\n",
      "BatchNormalization\n",
      "Conv2D\n",
      "tamaño d ela capa (None, 13, 13, 256)\n",
      "shape wights_conv_antes (1, 1, 384, 256)\n",
      "shape bias_antes_conv (256,)\n",
      "inyectando errores en los pesos\n",
      "numero de fallos insertados 662\n",
      "tamaño de del array weights 98304\n",
      "cantidad de pocisiones afectada capa 654\n",
      "tamaño de la lista d eposiciones 662\n",
      "tamaño de los weigts 98304\n",
      "cantidad de elementos iguales 97652\n",
      "cantidad de elementos diferentes 652\n",
      "shape bias_conv (256,)\n",
      "Bias cantidad de elementos iguales 248\n",
      "Bias cantidad de elementos diferentes 8\n",
      "set weights\n",
      "BatchNormalization\n",
      "Dense\n",
      "Dense\n",
      "Dense\n",
      "47/47 [==============================] - 17s 368ms/step - loss: 3.1662 - accuracy: 0.1200\n",
      "<tensorflow.python.keras.engine.functional.Functional object at 0x00000280BB05C430>\n"
     ]
    }
   ],
   "source": [
    "activation_aging = False\n",
    "\n",
    "error_mask = load_obj('Data/Fault Characterization/variante_mask_vc_707/error_mask_x/error_mask_x/vc_707/error_mask_054')\n",
    "locs = load_obj('Data/Fault Characterization/variante_mask_vc_707/error_mask_x/error_mask_x/vc_707/locs_054')\n",
    "wfrac_size = 11\n",
    "wint_size  = 4\n",
    "#locs = [0]\n",
    "#error_mask = ['xx1xxxxxxxxxxxxx']\n",
    "\n",
    "#Acá la creamos, notese que como no se introduciran fallos en activaciones no es necesario pasar locs ni masks\n",
    "Net2 = GetNeuralNetworkModel('AlexNet', (227,227,3), 8, aging_active=activation_aging,\n",
    "                             word_size=word_size, frac_size=afrac_size, batch_size = testBatchSize)\n",
    "Net2.load_weights(wgt_dir).expect_partial()\n",
    "#Cuantizacion de los pesos\n",
    "WeightQuantization(model = Net2, frac_bits = wfrac_size, int_bits = wint_size)\n",
    "for index,layer in enumerate(Net2.layers):\n",
    "   \n",
    "    \n",
    "    weights_tmp=[]\n",
    "    #print(Net2.layers[index].__class__.__name__)\n",
    "        \n",
    "    if(len(Net2.layers[index].weights) > 0):\n",
    "        print(Net2.layers[index].__class__.__name__)\n",
    "        if layer.__class__.__name__ in ['Conv2D','Conv1D','DepthwiseConv2D']:\n",
    "            print('tamaño d ela capa' ,layer.output_shape)\n",
    "            \n",
    "            weights, biases = Net2.layers[index].get_weights()\n",
    "                \n",
    "            w_shape= weights.shape\n",
    "            b_shape= biases.shape\n",
    "            print('shape wights_conv_antes',w_shape)\n",
    "            print('shape bias_antes_conv',b_shape)\n",
    "            wsize=weights.size\n",
    "            weights_conv = np.reshape(weights, (-1))\n",
    "            weig_conv_bias=np.concatenate((weights_conv, biases))\n",
    "            itm= weig_conv_bias\n",
    "            positionList,faultList,NumberOfFaults = GenerateFaultsList(shape=itm.shape,locs=locs,error_mask=error_mask)\n",
    "            itm_fail= IntroduceFaultsInWeights(itm,positionList,faultList,wfrac_size,wint_size) \n",
    "            \n",
    "            print('numero de fallos insertados', NumberOfFaults)\n",
    "            #print('lista de posiciones', positionList )\n",
    "            #print('shape de layer en 0',layer.output_shape[0])\n",
    "            layer_size=layer.output_shape[1]*layer.output_shape[2]*layer.output_shape[3]\n",
    "            print('tamaño de del array weights',wsize)\n",
    "            print('cantidad de pocisiones afectada capa',np.count_nonzero(positionList <= wsize))\n",
    "            print('tamaño de la lista d eposiciones', len(positionList))\n",
    "            #d = itm_fail[:wsize]\n",
    "            w=np.reshape(itm_fail[:wsize], (w_shape))\n",
    "            print('tamaño de los weigts',wsize)\n",
    "            comp_weights = np.equal(w, weights)\n",
    "            print('cantidad de elementos iguales', np.count_nonzero(comp_weights))\n",
    "            print('cantidad de elementos diferentes', np.sum(comp_weights == 0))\n",
    "            b=np.reshape(itm_fail[wsize:], (b_shape))\n",
    "            print('shape bias_conv',b.shape)\n",
    "            comp_bias = np.equal(b, biases)\n",
    "            print('Bias cantidad de elementos iguales', np.count_nonzero(comp_bias))\n",
    "            print('Bias cantidad de elementos diferentes', np.sum(comp_bias == 0))\n",
    "            #print('compr_bias', comp_bias)\n",
    "            weights_tmp.append(w)\n",
    "            weights_tmp.append(b)\n",
    "            Net2.layers[index].set_weights(weights_tmp)\n",
    "            print('set weights')\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "Net2.compile(optimizer=optimizer, loss=loss, metrics='accuracy')\n",
    "loss,acc  = Net2.evaluate(test_dataset)                \n",
    "print(Net2)\n",
    "#Net_faulty = WeightsFaults(Net2,locs,error_mask,wfrac_size,wint_size)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea59aa14",
   "metadata": {},
   "source": [
    "# Testing code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2f9cd5",
   "metadata": {},
   "source": [
    "### Comparacion entre modelos con y sin fallas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e5f4acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [x for x,y in test_dataset]\n",
    "#salidas del modelo sin fallas para la primer imagen del dataset de prueba\n",
    "outputs1 = get_all_outputs(Net1,X[0])\n",
    "#salidas del modelo con fallas para la primer imagen del dataset de prueba\n",
    "outputs2 = get_all_outputs(Net2,X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3256343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capa 0 InputLayer\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  0.0\n",
      "Capa 1 Lambda\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  0.0\n",
      "Capa 2 Lambda\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  0.0\n",
      "Capa 3 Conv2D\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  1023.98645\n",
      "Capa 4 Lambda\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  17.46289\n",
      "Capa 5 ReLU\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  15.999512\n",
      "Capa 6 BatchNormalization\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  506.44302\n",
      "Capa 7 Lambda\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  18.387207\n",
      "Capa 8 Lambda\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  18.387207\n",
      "Capa 9 MaxPooling2D\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  18.250977\n",
      "Capa 10 Lambda\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  18.250977\n",
      "Capa 11 Conv2D\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  36854.1\n",
      "Capa 12 Lambda\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  23.53125\n",
      "Capa 13 ReLU\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  15.999512\n",
      "Capa 14 BatchNormalization\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  320.53326\n",
      "Capa 15 Lambda\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  17.95459\n",
      "Capa 16 Lambda\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  17.95459\n",
      "Capa 17 MaxPooling2D\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  17.736816\n",
      "Capa 18 Lambda\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  17.736816\n",
      "Capa 19 Conv2D\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  73963.85\n",
      "Capa 20 Lambda\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  31.999023\n",
      "Capa 21 ReLU\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  15.999512\n",
      "Capa 22 BatchNormalization\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  83.028435\n",
      "Capa 23 Lambda\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  16.617188\n",
      "Capa 24 Lambda\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  16.617188\n",
      "Capa 25 Conv2D\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  32786.863\n",
      "Capa 26 Lambda\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  22.806152\n",
      "Capa 27 ReLU\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  15.999512\n",
      "Capa 28 BatchNormalization\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  33.559692\n",
      "Capa 29 Lambda\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  16.910156\n",
      "Capa 30 Lambda\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  16.910156\n",
      "Capa 31 Conv2D\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  15432.038\n",
      "Capa 32 Lambda\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  23.135742\n",
      "Capa 33 ReLU\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  15.999512\n",
      "Capa 34 BatchNormalization\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  28.282602\n",
      "Capa 35 Lambda\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  16.90039\n",
      "Capa 36 Lambda\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  16.90039\n",
      "Capa 37 MaxPooling2D\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  16.90039\n",
      "Capa 38 Lambda\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  16.90039\n",
      "Capa 39 Flatten\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  16.90039\n",
      "Capa 40 Dense\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  326.5027\n",
      "Capa 41 Lambda\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  31.999023\n",
      "Capa 42 ReLU\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  15.999512\n",
      "Capa 43 Dropout\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  15.999512\n",
      "Capa 44 Lambda\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  15.999512\n",
      "Capa 45 Dense\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  10.924135\n",
      "Capa 46 Lambda\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  10.924316\n",
      "Capa 47 ReLU\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  7.885742\n",
      "Capa 48 Dropout\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  7.885742\n",
      "Capa 49 Lambda\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  7.885742\n",
      "Capa 50 Dense\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  19.947113\n",
      "Capa 51 Lambda\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  15.950684\n",
      "Capa 52 TFOpLambda\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  0.976925\n",
      "Capa 53 Lambda\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  0.9765625\n"
     ]
    }
   ],
   "source": [
    "#Comparando las salidas\n",
    "for index in range(0,len(outputs1)):\n",
    "    print('Capa',index,Net1.layers[index].__class__.__name__)\n",
    "    print('maxima diferencia absoluta entre modelo con fallas y sin fallas: ',np.max(np.abs(outputs1[index]-outputs2[index])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8dc51ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def WeightsFaults(Net2,locs,error_mask,wfrac_size,wint_size):\n",
    "    \n",
    "    \n",
    "    \n",
    "    for index,layer in enumerate(Net2.layers):\n",
    "        weights_tmp=[]\n",
    "        print(Net2.layers[index].__class__.__name__)\n",
    "        \n",
    "        if(len(Net2.layers[index].weights) > 0):\n",
    "            print(Net2.layers[index].__class__.__name__)\n",
    "            if layer.__class__.__name__ in ['Conv2D','Conv1D','DepthwiseConv2D']:\n",
    "                weights, biases = Net2.layers[index].get_weights()\n",
    "                w_shape= weights.shape\n",
    "                b_shape= biases\n",
    "                wsize=weights.size\n",
    "                weights_conv = np.reshape(weights, (-1))\n",
    "                weig_conv_bias=np.concatenate((weights_conv, biases))\n",
    "                itm= weig_conv_bias\n",
    "                positionList,faultList,NumberOfFaults = GenerateFaultsList(shape=itm.shape,locs=locs,error_mask=error_mask)\n",
    "                itm_fail= IntroduceFaultsInWeights(itm,positionList,faultList,wfrac_size,wint_size) \n",
    "                #d = itm_fail[:wsize]\n",
    "                w=np.reshape(itm_fail[:wsize], (shape))\n",
    "                b=np.reshape(itm_fail[wsize:], (shape))\n",
    "                weights_tmp.append(w)\n",
    "                weights_tmp.append(b)\n",
    "                Net2.layers[index].set_weights(weights_tmp)\n",
    "                \n",
    "                \n",
    "        #if  Net2.layers[index].__class__.__name__=='BatchNormalization':\n",
    "        #        gamma,beta,moving_mean,moving_std = Net2.layers[index].get_weights()\n",
    "        #        weights_0.append(gamma)\n",
    "        #        weights_0.append(beta)\n",
    "        #        weights_0.append(moving_mean)\n",
    "        #        weights_0.append(moving_std)\n",
    "        #        print(gamma.shape)\n",
    "        #        print(beta.shape)\n",
    "        #        print(moving_mean.shape)\n",
    "        #        print(moving_std.shape)\n",
    "        #        #gamma = np.reshape(gamma, (-1))\n",
    "        #        #beta = np.reshape(beta, (-1))\n",
    "        #        #moving_mean = np.reshape(moving_mean, (-1))\n",
    "        #        #moving_std = np.reshape(moving_std, (-1))\n",
    "        #        #weights_1=np.concatenate((gamma,beta,moving_mean,moving_std))\n",
    "        #        #print('shape de weights_1:', weights_1.shape)  \n",
    "        #     \n",
    "        #    if  Net2.layers[index].__class__.__name__=='Dense':  \n",
    "        #        weights_dense, biases_dense = Net2.layers[index].get_weights()\n",
    "        #        weights_0.append(weights_dense)\n",
    "        #        weights_0.append(biases_dense)\n",
    "        #        print(weights_dense.shape)\n",
    "        #        print(weights_dense.shape)\n",
    "        #    \n",
    "        #        #weights_dense = np.reshape(weights_dense, (-1))\n",
    "        #        #biases_dense = np.reshape(biases_dense, (-1))\n",
    "        #        #weights_2=np.concatenate((weights_dense, biases_dense))\n",
    "        #        #print('weights_2', weights_2.shape)\n",
    "        return Net2    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ba4df5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n",
      "InputLayer\n",
      "Lambda\n",
      "Lambda\n",
      "Conv2D\n",
      "(11, 11, 3, 96)\n",
      "34848\n",
      "(96,)\n",
      "Lambda\n",
      "ReLU\n",
      "BatchNormalization\n",
      "(96,)\n",
      "(96,)\n",
      "(96,)\n",
      "(96,)\n",
      "Lambda\n",
      "Lambda\n",
      "MaxPooling2D\n",
      "Lambda\n",
      "Conv2D\n",
      "(5, 5, 96, 256)\n",
      "614400\n",
      "(256,)\n",
      "Lambda\n",
      "ReLU\n",
      "BatchNormalization\n",
      "(256,)\n",
      "(256,)\n",
      "(256,)\n",
      "(256,)\n",
      "Lambda\n",
      "Lambda\n",
      "MaxPooling2D\n",
      "Lambda\n",
      "Conv2D\n",
      "(3, 3, 256, 384)\n",
      "884736\n",
      "(384,)\n",
      "Lambda\n",
      "ReLU\n",
      "BatchNormalization\n",
      "(384,)\n",
      "(384,)\n",
      "(384,)\n",
      "(384,)\n",
      "Lambda\n",
      "Lambda\n",
      "Conv2D\n",
      "(1, 1, 384, 384)\n",
      "147456\n",
      "(384,)\n",
      "Lambda\n",
      "ReLU\n",
      "BatchNormalization\n",
      "(384,)\n",
      "(384,)\n",
      "(384,)\n",
      "(384,)\n",
      "Lambda\n",
      "Lambda\n",
      "Conv2D\n",
      "(1, 1, 384, 256)\n",
      "98304\n",
      "(256,)\n",
      "Lambda\n",
      "ReLU\n",
      "BatchNormalization\n",
      "(256,)\n",
      "(256,)\n",
      "(256,)\n",
      "(256,)\n",
      "Lambda\n",
      "Lambda\n",
      "MaxPooling2D\n",
      "Lambda\n",
      "Flatten\n",
      "Dense\n",
      "(9216, 4096)\n",
      "(9216, 4096)\n",
      "Lambda\n",
      "ReLU\n",
      "Dropout\n",
      "Lambda\n",
      "Dense\n",
      "(4096, 4096)\n",
      "(4096, 4096)\n",
      "Lambda\n",
      "ReLU\n",
      "Dropout\n",
      "Lambda\n",
      "Dense\n",
      "(4096, 8)\n",
      "(4096, 8)\n",
      "Lambda\n",
      "TFOpLambda\n",
      "Lambda\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "activation_aging = False\n",
    "\n",
    "#cambiare la direccion del primer error y la posicion del fallo,\n",
    "#seteandolo en una direccion baja (que afecte a la capa de normalizacion) y así poder observar la caida\n",
    "#locs[0] = 0\n",
    "#masks[0] = 'xx1xxxxxxxxxxxxx'\n",
    "error_mask_address = load_obj('Data/Fault Characterization/variante_mask_vc_707/error_mask_x/error_mask_x/vc_707/error_mask_054')\n",
    "locs_address = load_obj('Data/Fault Characterization/variante_mask_vc_707/error_mask_x/error_mask_x/vc_707/locs_054')\n",
    "\n",
    "\n",
    "#Acá la creamos, notese que como no se introduciran fallos en activaciones no es necesario pasar locs ni masks\n",
    "Net2 = GetNeuralNetworkModel('AlexNet', (227,227,3), 8, aging_active=activation_aging,\n",
    "                             word_size=word_size, frac_size=afrac_size, batch_size = testBatchSize)\n",
    "Net2.load_weights(wgt_dir).expect_partial()\n",
    "#Cuantizacion de los pesos\n",
    "WeightQuantization(model = Net2, frac_bits = wfrac_size, int_bits = wint_size)\n",
    "#Introducir fallas en cada capa de pesos\n",
    "weights = Net2.get_weights()\n",
    "print(len(weights))\n",
    "weights_0 = []\n",
    "#print(weights_0)\n",
    "for index,layer in enumerate(Net2.layers):\n",
    "    print(Net2.layers[index].__class__.__name__)\n",
    "    \n",
    "    if(len(Net2.layers[index].weights) > 0):\n",
    "        if layer.__class__.__name__ in ['Conv2D','Conv1D','DepthwiseConv2D']:\n",
    "        #if Net2.layers[index].__class__.__name__=='Conv2D':\n",
    "            weights, biases = Net2.layers[index].get_weights()\n",
    "            weights_0.append(weights)\n",
    "            weights_0.append(biases)\n",
    "            print(weights.shape)\n",
    "            print(weights.size)\n",
    "            print(biases.shape)\n",
    "            weights_conv = np.reshape(weights, (-1))\n",
    "            #print('shape weights_conv',(weights_conv.shape))\n",
    "            join_weights_conv_bias=np.concatenate((weights_conv, biases))\n",
    "            #print('join perfecto', join_weights_conv_bias.shape)\n",
    "            \n",
    "        if  Net2.layers[index].__class__.__name__=='BatchNormalization':\n",
    "            gamma,beta,moving_mean,moving_std = Net2.layers[index].get_weights()\n",
    "            weights_0.append(gamma)\n",
    "            weights_0.append(beta)\n",
    "            weights_0.append(moving_mean)\n",
    "            weights_0.append(moving_std)\n",
    "            print(gamma.shape)\n",
    "            print(beta.shape)\n",
    "            print(moving_mean.shape)\n",
    "            print(moving_std.shape)\n",
    "            #gamma = np.reshape(gamma, (-1))\n",
    "            #beta = np.reshape(beta, (-1))\n",
    "            #moving_mean = np.reshape(moving_mean, (-1))\n",
    "            #moving_std = np.reshape(moving_std, (-1))\n",
    "            #weights_1=np.concatenate((gamma,beta,moving_mean,moving_std))\n",
    "            #print('shape de weights_1:', weights_1.shape)  \n",
    "         \n",
    "        if  Net2.layers[index].__class__.__name__=='Dense':  \n",
    "            weights_dense, biases_dense = Net2.layers[index].get_weights()\n",
    "            weights_0.append(weights_dense)\n",
    "            weights_0.append(biases_dense)\n",
    "            print(weights_dense.shape)\n",
    "            print(weights_dense.shape)\n",
    "            #weights_dense = np.reshape(weights_dense, (-1))\n",
    "            #biases_dense = np.reshape(biases_dense, (-1))\n",
    "            #weights_2=np.concatenate((weights_dense, biases_dense))\n",
    "            #print('weights_2', weights_2.shape)\n",
    "## El tema está que a la hora d einyectar  los fallos como está diseñado se hace al modleo y de esta forma sería por capas\n",
    "##entonces en este caso cuando se procese una capa deberiamos de mandar a inyectar los errores y luego agregarlo al modelo \n",
    "## ahi no entiendo como manejarlo\n",
    "print(len(weights_0))\n",
    "#positionList,faultList,NumberOfFaults = GenerateFaultsList(shape=itm.shape,locs=faulty_addresses,error_mask=masked_faults)\n",
    "#\n",
    "#    if NumberOfFaults > 0:\n",
    "#        weights[index] = IntroduceFaultsInWeights(itm,positionList,faultList,wint_size,wint_size)\n",
    "#qNet.set_weights(weights)\n",
    "#loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "#optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "#Net2.compile(optimizer=optimizer, loss=loss, metrics='accuracy')\n",
    "#loss,acc  = Net2.evaluate(test_dataset)\n",
    "                    \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b82f21",
   "metadata": {},
   "source": [
    "### Crear la red con fallos en pesos y analizar los NAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "32c08af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inyectando errores en los pesos\n",
      "inyectando errores en los pesos\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "indices[0] = [3, 464] does not index into param shape [4096,8], node name: GatherNd [Op:GatherNd]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Input \u001b[1;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 16>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m#print('faultList:',faultList)\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m NumberOfFaults \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     20\u001b[0m      \u001b[38;5;66;03m#print('NumberOfFaults:', NumberOfFaults)\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m         weights[index] \u001b[38;5;241m=\u001b[39m \u001b[43mIntroduceFaultsInWeights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitm\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpositionList\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfaultList\u001b[49m\u001b[43m,\u001b[49m\u001b[43mwint_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43mwint_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m qNet\u001b[38;5;241m.\u001b[39mset_weights(weights)\n\u001b[0;32m     23\u001b[0m loss \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mCategoricalCrossentropy()\n",
      "File \u001b[1;32m~\\Desktop\\CNN_Gating\\Stats.py:70\u001b[0m, in \u001b[0;36mIntroduceFaultsInWeights\u001b[1;34m(tensor, positionList, faultList, intSize, fractSize)\u001b[0m\n\u001b[0;32m     68\u001b[0m \ttensor  \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast(tensor\u001b[38;5;241m/\u001b[39mfactor,dtype \u001b[38;5;241m=\u001b[39m Ogdtype)\n\u001b[0;32m     69\u001b[0m \t\u001b[38;5;28;01mreturn\u001b[39;00m tensor\n\u001b[1;32m---> 70\u001b[0m affectedValues \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgather_nd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpositionList\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m newValues \u001b[38;5;241m=\u001b[39m ApplyFault(affectedValues,faultList)\n\u001b[0;32m     72\u001b[0m tensor \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mtensor_scatter_nd_update(tensor, positionList, newValues)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\env_first\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:206\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;124;03m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 206\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m    208\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m    209\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m    210\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(wrapper, args, kwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\env_first\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:5360\u001b[0m, in \u001b[0;36mgather_nd_v2\u001b[1;34m(params, indices, batch_dims, name)\u001b[0m\n\u001b[0;32m   5357\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgather_nd\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[0;32m   5358\u001b[0m \u001b[38;5;129m@dispatch\u001b[39m\u001b[38;5;241m.\u001b[39madd_dispatch_support\n\u001b[0;32m   5359\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgather_nd_v2\u001b[39m(params, indices, batch_dims\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m-> 5360\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgather_nd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_dims\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\env_first\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:206\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;124;03m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 206\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m    208\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m    209\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m    210\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(wrapper, args, kwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\env_first\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:5352\u001b[0m, in \u001b[0;36mgather_nd\u001b[1;34m(params, indices, name, batch_dims)\u001b[0m\n\u001b[0;32m   5350\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m params\u001b[38;5;241m.\u001b[39mgather_nd(indices, name\u001b[38;5;241m=\u001b[39mname)\n\u001b[0;32m   5351\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[1;32m-> 5352\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgen_array_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgather_nd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5353\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   5354\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m batch_gather_nd(params, indices, batch_dims\u001b[38;5;241m=\u001b[39mbatch_dims, name\u001b[38;5;241m=\u001b[39mname)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\env_first\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py:3714\u001b[0m, in \u001b[0;36mgather_nd\u001b[1;34m(params, indices, name)\u001b[0m\n\u001b[0;32m   3712\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m   3713\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 3714\u001b[0m   \u001b[43m_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from_not_ok_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3715\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_FallbackException:\n\u001b[0;32m   3716\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\env_first\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:6897\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   6895\u001b[0m message \u001b[38;5;241m=\u001b[39m e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6896\u001b[0m \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m-> 6897\u001b[0m \u001b[43msix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_status_to_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: indices[0] = [3, 464] does not index into param shape [4096,8], node name: GatherNd [Op:GatherNd]"
     ]
    }
   ],
   "source": [
    "#v=54\n",
    "#masked_faults = load_obj('Data/Fault Characterization/variante_mask_vc_707/error_mask_x/error_mask_x/vc_707/error_mask_0' + str(v))\n",
    "#faulty_addresses = load_obj('Data/Fault Characterization/variante_mask_vc_707/error_mask_x/error_mask_x/vc_707/locs_0' + str(v))\n",
    "#        #print('vol',vol)\n",
    "#        #vol = vol - 1\n",
    "#\n",
    "# \n",
    "faulty_addresses = [16000]\n",
    "error_mask = ['xx1xxxxxxxxxxxxx']\n",
    "Net2 = GetNeuralNetworkModel('AlexNet', (227,227,3), 8, faulty_addresses=locs, masked_faults=error_mask,\n",
    "                                 aging_active=False, word_size=word_size, frac_size=afrac_size, \n",
    "                                 batch_size = testBatchSize)\n",
    "Net2.load_weights(wgt_dir).expect_partial()\n",
    "WeightQuantization(model = Net2, frac_bits = wfrac_size, int_bits = wint_size)\n",
    "weights = Net2.get_weights()\n",
    "\n",
    "for index,itm in enumerate(weights):\n",
    "    positionList,faultList,NumberOfFaults = GenerateFaultsList(shape=itm.shape,locs=faulty_addresses,error_mask=masked_faults)\n",
    "    #print('faultList:',faultList)\n",
    "    if NumberOfFaults > 0:\n",
    "     #print('NumberOfFaults:', NumberOfFaults)\n",
    "        weights[index] = IntroduceFaultsInWeights(itm,positionList,faultList,wint_size,wint_size)\n",
    "qNet.set_weights(weights)\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "Net2.compile(optimizer=optimizer, loss=loss, metrics='accuracy')\n",
    "loss,acc  = Net2.evaluate(test_dataset)\n",
    "        \n",
    "        \n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d143d5",
   "metadata": {},
   "source": [
    "### Crear la red con fallos en pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8a0016b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "(11, 11, 3, 96)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 16>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(index)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(itm\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 19\u001b[0m weights, biases\u001b[38;5;241m=\u001b[39mitm\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "activation_aging = False\n",
    "\n",
    "v=54\n",
    "error_mask = load_obj('Data/Fault Characterization/variante_mask_vc_707/error_mask_x/error_mask_x/vc_707/error_mask_0' + str(v))\n",
    "locs = load_obj('Data/Fault Characterization/variante_mask_vc_707/error_mask_x/error_mask_x/vc_707/locs_0' + str(v))\n",
    "        #print('vol',vol)\n",
    "        #vol = vol - 1\n",
    "\n",
    "    \n",
    "Net2 = GetNeuralNetworkModel('AlexNet', (227,227,3), 8, faulty_addresses=locs, masked_faults=error_mask,\n",
    "                                 aging_active=False, word_size=word_size, frac_size=afrac_size, \n",
    "                                 batch_size = testBatchSize)\n",
    "Net2.load_weights(wgt_dir).expect_partial()\n",
    "WeightQuantization(model = Net2, frac_bits = wfrac_size, int_bits = wint_size)\n",
    "weights = Net2.get_weights()\n",
    "for index,itm in enumerate(weights):\n",
    "    print(index)\n",
    "    print(itm.shape)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7cd02daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "index=2\n",
    " #print('Capa',j,NetVecinos.layers[j].__class__.__name__)\n",
    "weights = Net2.layers[index].get_weights()\n",
    "capa=Net2.layers[index].__class__.__name__\n",
    "print(capa)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2146562c",
   "metadata": {},
   "source": [
    "¿Porque se da este error? analizandolo bien corresponde a un error en el modelado, cuando modele la introduccion de errores en los pesos lo hice de manera similar a las activaciones, modificando los valores capa a capa, sin embargo al obtener los pesos mediante .get_weights() la lista que se obtiene no esta ordenada capa a capa como en el caso de cuando obtenia las activaciones. \n",
    "\n",
    "(por suerte esto no afecto al trabajo que hice, ya que, estadisticamente este error solo cobra importancia cuando hay errores en bits significativos de direcciones muy bajas (400 o menor en el caso de AlexNet) que son una parte muy reducida con respecto al total de direcciones (885120 en AlexNet))\n",
    "\n",
    "Para arreglar este error y que puedas continuar tu trabajo lo que tendrias que hacer es re-arreglar la lista obtenida por get_weights (presente en la variable weights del bloque de codigo anterior), si por ejemplola red tuviera las capas Conv->Batch Norm -> Dense entonces la weights estaria organizada así:\n",
    "- weights[0] -> Filtros de la capa Convolucion (Dimension = 4)\n",
    "- weights[1] -> Bias de la capa Convolucion    (Dimension = 1)\n",
    "- weights[2] -> Parametros gamma de la capa Batch Normalization (Dimension = 1)\n",
    "- weights[3] -> Parametros beta de la capa Batch Normalization  (Dimension = 1)\n",
    "- weights[4] -> Parametros moving_mean de la capa Batch Normalization (Dimension = 1)\n",
    "- weights[5] -> Parametros moving_std de la capa Batch Normalization  (Dimension = 1)\n",
    "- weights[6] -> Pesos de la capa Dense (Dimension = 2)\n",
    "- weights[7] -> Bias de la capa Dense (Dimension = 1)\n",
    "\n",
    "Para que el modelo este correcto deberias modificar la variable weights y que quede así:\n",
    "- weights[0] -> Filtros y Bias de la capa Convolucion (puedes vectorizarlos para que queden con Dimension = 1)\n",
    "- weights[1] -> Parametros gamma,beta,moving_mean,moving_std de la capa Batch Normalization (puedes vectorizarlos para que queden con Dimension = 1 o unirlos en diferentes ejes y que queden con Dimension = 4)\n",
    "- weights[2] -> Pesos y Bias de la capa Dense (puedes vectorizarlos queden con Dimension = 1)\n",
    "\n",
    "Como veras en el ciclo for index,item del bloque anterior, con los pesos como estan ahora con una falla en 0, el error seria introducido multiples veces en cada capa, por ejemplo en el primer indice de filtros y en el primer indice de los bias; cuando deberia ser introducido solamente en uno de estos dos, dependiendo de cual asumimos que va primero en memoria, los filtros o los bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e165e58e",
   "metadata": {},
   "source": [
    "### Comparacion entre modelos con y sin fallas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "db26fa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [x for x,y in test_dataset]\n",
    "#salidas del modelo sin fallas para la primer imagen del dataset de prueba\n",
    "outputs1 = get_all_outputs(Net1,X[0])\n",
    "#salidas del modelo con fallas para la primer imagen del dataset de prueba\n",
    "outputs2 = get_all_outputs(Net2,X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3f8bfac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capa 0 InputLayer\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  0.0\n",
      "Capa 1 Lambda\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  0.0\n",
      "Capa 2 Lambda\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  0.0\n",
      "Capa 3 Conv2D\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  3.7382812\n",
      "Capa 4 Lambda\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  3.7382812\n",
      "Capa 5 ReLU\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  0.0\n",
      "Capa 6 BatchNormalization\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  13.997774\n",
      "Capa 7 Lambda\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  13.997559\n",
      "Capa 8 Lambda\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  13.997559\n",
      "Capa 9 MaxPooling2D\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  13.997559\n",
      "Capa 10 Lambda\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  13.997559\n",
      "Capa 11 Conv2D\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  51.483\n",
      "Capa 12 Lambda\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  18.334473\n",
      "Capa 13 ReLU\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  7.857422\n",
      "Capa 14 BatchNormalization\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  21.294764\n",
      "Capa 15 Lambda\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  18.749023\n",
      "Capa 16 Lambda\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  18.749023\n",
      "Capa 17 MaxPooling2D\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  18.749023\n",
      "Capa 18 Lambda\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  18.749023\n",
      "Capa 19 Conv2D\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  65.95151\n",
      "Capa 20 Lambda\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  31.999023\n",
      "Capa 21 ReLU\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  15.999512\n",
      "Capa 22 BatchNormalization\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  31.097506\n",
      "Capa 23 Lambda\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  16.405762\n",
      "Capa 24 Lambda\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  16.405762\n",
      "Capa 25 Conv2D\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  56.270992\n",
      "Capa 26 Lambda\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  24.201172\n",
      "Capa 27 ReLU\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  15.999512\n",
      "Capa 28 BatchNormalization\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  19.843483\n",
      "Capa 29 Lambda\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  19.32959\n",
      "Capa 30 Lambda\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  19.32959\n",
      "Capa 31 Conv2D\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  75.75519\n",
      "Capa 32 Lambda\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  19.716797\n",
      "Capa 33 ReLU\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  15.999512\n",
      "Capa 34 BatchNormalization\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  22.064787\n",
      "Capa 35 Lambda\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  17.097168\n",
      "Capa 36 Lambda\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  17.097168\n",
      "Capa 37 MaxPooling2D\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  17.097168\n",
      "Capa 38 Lambda\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  17.097168\n",
      "Capa 39 Flatten\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  17.097168\n",
      "Capa 40 Dense\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  144.9683\n",
      "Capa 41 Lambda\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  31.999023\n",
      "Capa 42 ReLU\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  15.999512\n",
      "Capa 43 Dropout\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  15.999512\n",
      "Capa 44 Lambda\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  15.999512\n",
      "Capa 45 Dense\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  121.88271\n",
      "Capa 46 Lambda\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  17.13916\n",
      "Capa 47 ReLU\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  8.160156\n",
      "Capa 48 Dropout\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  8.160156\n",
      "Capa 49 Lambda\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  8.160156\n",
      "Capa 50 Dense\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  16.808973\n",
      "Capa 51 Lambda\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  16.809082\n",
      "Capa 52 TFOpLambda\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  0.99923515\n",
      "Capa 53 Lambda\n",
      "maxima diferencia absoluta entre modelo con fallas y sin fallas:  0.9995117\n"
     ]
    }
   ],
   "source": [
    "#Comparando las salidas\n",
    "for index in range(0,len(outputs1)):\n",
    "    print('Capa',index,Net1.layers[index].__class__.__name__)\n",
    "    print('maxima diferencia absoluta entre modelo con fallas y sin fallas: ',np.max(np.abs(outputs1[index]-outputs2[index])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_first",
   "language": "python",
   "name": "env_first"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
