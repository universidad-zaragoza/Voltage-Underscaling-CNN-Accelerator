{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf5f4f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle as pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from Stats import WeightQuantization,IntroduceFaultsInWeights,GenerateFaultsList\n",
    "from Nets import GetNeuralNetworkModel\n",
    "from Training import GetDatasets\n",
    "from Simulation import get_all_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03e9edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Estudio d elos pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02e7c49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from Simulation import buffer_simulation, save_obj, load_obj\n",
    "from funciones import compilNet, same_elements\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pandas import ExcelWriter\n",
    "from openpyxl import Workbook\n",
    "from openpyxl import load_workbook\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67351031",
   "metadata": {},
   "source": [
    "### Definiciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "264836c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numero de bits para activaciones (a) y pesos (w)\n",
    "word_size  = 16\n",
    "afrac_size = 11  \n",
    "aint_size  = 4\n",
    "wfrac_size = 11\n",
    "wint_size  = 4\n",
    "# Tamaño del buffer de pesos == al tamaño de la capa con mayor numero de pesos (885120 pesos de 16 bits cada uno)\n",
    "#wbuffer_size = 885120*word_size\n",
    "# Tamaño del buffer de activaciones == al tamaño de la capa con mayor numero de activaciones (290400 pesos de 16 bits cada uno)\n",
    "#abuffer_size = 290400*word_size\n",
    "# Directorio de los pesos\n",
    "cwd = os.getcwd()\n",
    "wgt_dir = os.path.join(cwd, 'Data')\n",
    "wgt_dir = os.path.join(wgt_dir, 'Trained Weights')\n",
    "wgt_dir = os.path.join(wgt_dir, 'AlexNet')\n",
    "wgt_dir = os.path.join(wgt_dir, 'Colorectal Dataset')\n",
    "wgt_dir = os.path.join(wgt_dir,'Weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54787132",
   "metadata": {},
   "source": [
    "### Generacion de una muestra de fallos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a53b31",
   "metadata": {},
   "source": [
    "### Cargar el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "717789e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\usuario\\anaconda3\\envs\\env_first\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:3703: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable.debug_mode()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainBatchSize = testBatchSize = 16\n",
    "_,_,test_dataset = GetDatasets('colorectal_histology',(80,5,15),(227,227), 8, trainBatchSize, testBatchSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "238db46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "Conv2D\n",
      "    - Tamaño del itm vectorizado 34848\n",
      "    - Tamaño del itm vectorizado 34944\n",
      "  Numero total de pesos: 34944\n",
      "  Total de numero de fallos: 134\n",
      "1dentro 134\n",
      "inyectando errores en los pesos\n",
      "weight.size 34848\n",
      "weight.size 96\n",
      "47/47 [==============================] - 14s 306ms/step - loss: 13.1099 - accuracy: 0.1227\n",
      "weight.size 34848\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (11,11,3,96) into shape (11,3,96)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 24>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight.size\u001b[39m\u001b[38;5;124m'\u001b[39m,weight\u001b[38;5;241m.\u001b[39msize)\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(vectorized_pesos))\n\u001b[1;32m---> 76\u001b[0m     weight[index]\u001b[38;5;241m=\u001b[39m vectorized_pesos[\u001b[38;5;241m0\u001b[39m:weight\u001b[38;5;241m.\u001b[39msize]\u001b[38;5;241m.\u001b[39mreshape(weight\u001b[38;5;241m.\u001b[39mshape,order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28mprint\u001b[39m(index)\n\u001b[0;32m     78\u001b[0m layer\u001b[38;5;241m.\u001b[39mset_weights(weights)\n",
      "\u001b[1;31mValueError\u001b[0m: could not broadcast input array from shape (11,11,3,96) into shape (11,3,96)"
     ]
    }
   ],
   "source": [
    "error_mask = load_obj('Data/Fault Characterization/variante_mask_vc_707/error_mask_x/error_mask_x/vc_707/error_mask_054')\n",
    "locs = load_obj('Data/Fault Characterization/variante_mask_vc_707/error_mask_x/error_mask_x/vc_707/locs_054')\n",
    "\n",
    "\n",
    "activation_aging = [False]*11\n",
    "\n",
    "afrac_size = 11  \n",
    "aint_size  = 4\n",
    "wfrac_size = 11\n",
    "wint_size  = 4\n",
    "\n",
    "Net2 = GetNeuralNetworkModel('AlexNet', (227,227,3), 8, aging_active=activation_aging,\n",
    "                             word_size=word_size, frac_size=afrac_size, batch_size = testBatchSize)\n",
    "Net2.load_weights(wgt_dir).expect_partial()\n",
    "#Cuantizacion de los pesos\n",
    "WeightQuantization(model = Net2, frac_bits = wfrac_size, int_bits = wint_size)\n",
    "num_weights=[]\n",
    "num_fallos=[]\n",
    "num_direc_afect= []\n",
    "name_layer=[]\n",
    "layer_size=[]\n",
    "acc_layer=[]\n",
    "\n",
    "for index,layer in enumerate(Net2.layers):\n",
    "    print(index)\n",
    "    weights = layer.get_weights()\n",
    "    \n",
    "    \n",
    "    if weights:\n",
    "        layer_name=Net2.layers[index].__class__.__name__\n",
    "        print(layer_name)\n",
    "        \n",
    "        name_layer.append(layer_name)\n",
    "        \n",
    "        \n",
    "        vectorized_pesos = []\n",
    "            \n",
    "        for itm in weights:\n",
    "            #print('    - Numero de pesos x itm:',itm.size)\n",
    "            vectorized_pesos.extend(itm.flatten(order='F'))\n",
    "            print('    - Tamaño del itm vectorizado',len(vectorized_pesos))\n",
    "        vectorized_pesos = np.array(vectorized_pesos)\n",
    "        num_weights.append(vectorized_pesos.size)\n",
    "        print('  Numero total de pesos:',vectorized_pesos.size)\n",
    "            \n",
    "        indexList,faultList,NumberOfFaults = GenerateFaultsList(shape=vectorized_pesos.shape,locs=locs,error_mask=error_mask)\n",
    "        num_fallos.append(NumberOfFaults)\n",
    "        print('  Total de numero de fallos:',NumberOfFaults)\n",
    "        #print('  indexList:',len(indexList))\n",
    "        #print('  Listfault fallos:',len(faultList))\n",
    "      \n",
    "        if NumberOfFaults >  0 :\n",
    "            print('1dentro',NumberOfFaults)\n",
    "            weight_fail= IntroduceFaultsInWeights(vectorized_pesos,indexList,faultList,wint_size,wfrac_size) \n",
    "            direc_afect= np.count_nonzero(vectorized_pesos!=weight_fail)\n",
    "            num_direc_afect.append(direc_afect)\n",
    "        if NumberOfFaults==0:\n",
    "            print('dentro',NumberOfFaults)\n",
    "            num_direc_afect.append(0)\n",
    "            print('  Total de direcciones afectadas:',np.count_nonzero(vectorized_pesos!=weight_fail))\n",
    "            print('num_direc_afect',num_direc_afect)\n",
    "        for index, weight in enumerate(weights):\n",
    "            print('weight.size',weight.size)\n",
    "            \n",
    "            weights[index] = weight_fail[0:weight.size].numpy().reshape(weight.shape,order='F')\n",
    "        layer.set_weights(weights)\n",
    "        loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "        Net2.compile(optimizer=optimizer, loss=loss, metrics='accuracy')\n",
    "        loss,acc  = Net2.evaluate(test_dataset) \n",
    "        acc_layer.append(acc)\n",
    "        layer.get_weights()\n",
    "        for index, weight in enumerate(weights):\n",
    "            print('weight.size',weight.size)\n",
    "            print(type(vectorized_pesos))\n",
    "            weight[index]= vectorized_pesos[0:weight.size].reshape(weight.shape,order='F')\n",
    "            print(index)\n",
    "        layer.set_weights(weights)\n",
    "            \n",
    "                                          \n",
    "DFrame_num_weights= pd.DataFrame(num_weights)\n",
    "DFrame_num_fallos= pd.DataFrame(num_fallos)\n",
    "DFrame_num_direc_afect= pd.DataFrame(num_direc_afect)\n",
    "DFrame_layer= pd.DataFrame(name_layer)\n",
    "DFrame_acc_layer= pd.DataFrame(acc_layer)\n",
    "\n",
    "                                          \n",
    "\n",
    "statistics = pd.concat([DFrame_layer,DFrame_num_weights,DFrame_num_fallos,DFrame_num_direc_afect,DFrame_acc_layer], axis=1, join='outer')\n",
    "statistics.columns =['DFrame_layer', 'num_weights','num_fallos', 'num_direc_afect','Acc']\n",
    "print(statistics)\n",
    "statistics.to_excel('statist_weigts.xlsx', sheet_name='fichero_707', index=False)                                          \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1204d924",
   "metadata": {},
   "source": [
    "## Algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45638664",
   "metadata": {},
   "outputs": [],
   "source": [
    "Algoritmo(crear una función que tome el modelo)\n",
    "le pido los pesos al modelo\n",
    "recorro las el modelo por capas\n",
    "si la capa tiene pesos y es convolución:\n",
    "    calculo la candidad de elementos que tiene la variable weights.size\n",
    "    lo guardo en un variable\n",
    "    la vectorizo a 1 dimensión y a concateno con el bias \n",
    "    le paso ese itm a la función IntroduceFaultsInWeights\n",
    "    luego la separos y las coloco son el shape original \n",
    "    y a model.layers[index] le establezco los pesos ya con fallos\n",
    "si la capa tiene pesos y es Normalizatión:\n",
    "    tiene los parametros gamma,beta,moving_mean,moving_std\n",
    "    hago lo mismo por cada elemento\n",
    "    (seria mejor hacer dos funciones nuevas para que hagan lo de vectorizar y luego separar de nuevo)\n",
    "    model.layers[index] le establezco los pesos ya con fallos\n",
    "Si es un capa Dense:\n",
    "    tiene weights y bias\n",
    "    hago los mismo \n",
    "    model.layers[index] le establezco los pesos ya con fallos\n",
    "la función devuelve el modelo para qu econtinue el algoritmo   \n",
    "\n",
    "para comprobar que funciona imprimo las variables positionList,faultLis\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfe9b96",
   "metadata": {},
   "source": [
    "## Función"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54753a6c",
   "metadata": {},
   "source": [
    "### Uso la misma forma que usas tu para llevarlo a 1 dimensión pero por separado igual\n",
    "### hay break en el código porque fui comparando los valores de tus variables con las mias y veo que no hay difrencias antes de inyectar fallos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5fcc9c",
   "metadata": {},
   "source": [
    "## Método 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0db5ffc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_mask = load_obj('Data/Fault Characterization/variante_mask_vc_707/buffer_weights/error_mask_054')\n",
    "locs = load_obj('Data/Fault Characterization/variante_mask_vc_707/buffer_weights/locs_054')\n",
    "#print('losc pesos',locs_pesos[0:10])\n",
    "#loss: 3.0441 - accuracy: 0.1173\n",
    "\n",
    "activation_aging = False\n",
    "\n",
    "\n",
    "wfrac_size = 11\n",
    "wint_size  = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff756175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introducciendo errores en Conv1\n",
      "itm dentro\n",
      "    Numero de pesos: 34848\n",
      "tamaño del vector vectorizado 34848\n",
      "<class 'list'>\n",
      "itm dentro\n",
      "    Numero de pesos: 96\n",
      "tamaño del vector vectorizado 34944\n",
      "<class 'list'>\n",
      "shape (34944,)\n",
      "  Numero total de pesos: 34944\n",
      "  Total de direcciones con fallos: 466\n",
      "  indexList: 466\n",
      "  Listfault fallos: 466\n",
      "inyectando errores en los pesos\n",
      "  Total de direcciones afectadas: 249\n",
      "total de elementos iguales 34695\n",
      "index 0\n",
      "index 1\n",
      "Introducciendo errores en batch_normalization_45\n",
      "itm dentro\n",
      "    Numero de pesos: 96\n",
      "tamaño del vector vectorizado 96\n",
      "<class 'list'>\n",
      "itm dentro\n",
      "    Numero de pesos: 96\n",
      "tamaño del vector vectorizado 192\n",
      "<class 'list'>\n",
      "itm dentro\n",
      "    Numero de pesos: 96\n",
      "tamaño del vector vectorizado 288\n",
      "<class 'list'>\n",
      "itm dentro\n",
      "    Numero de pesos: 96\n",
      "tamaño del vector vectorizado 384\n",
      "<class 'list'>\n",
      "shape (384,)\n",
      "  Numero total de pesos: 384\n",
      "  Total de direcciones con fallos: 2\n",
      "  indexList: 2\n",
      "  Listfault fallos: 2\n",
      "inyectando errores en los pesos\n",
      "  Total de direcciones afectadas: 0\n",
      "total de elementos iguales 384\n",
      "index 0\n",
      "index 1\n",
      "index 2\n",
      "index 3\n",
      "Introducciendo errores en Conv2\n",
      "itm dentro\n",
      "    Numero de pesos: 614400\n",
      "tamaño del vector vectorizado 614400\n",
      "<class 'list'>\n",
      "itm dentro\n",
      "    Numero de pesos: 256\n",
      "tamaño del vector vectorizado 614656\n",
      "<class 'list'>\n",
      "shape (614656,)\n",
      "  Numero total de pesos: 614656\n",
      "  Total de direcciones con fallos: 7277\n",
      "  indexList: 7277\n",
      "  Listfault fallos: 7277\n",
      "inyectando errores en los pesos\n",
      "  Total de direcciones afectadas: 3705\n",
      "total de elementos iguales 610951\n",
      "index 0\n",
      "index 1\n",
      "Introducciendo errores en batch_normalization_46\n",
      "itm dentro\n",
      "    Numero de pesos: 256\n",
      "tamaño del vector vectorizado 256\n",
      "<class 'list'>\n",
      "itm dentro\n",
      "    Numero de pesos: 256\n",
      "tamaño del vector vectorizado 512\n",
      "<class 'list'>\n",
      "itm dentro\n",
      "    Numero de pesos: 256\n",
      "tamaño del vector vectorizado 768\n",
      "<class 'list'>\n",
      "itm dentro\n",
      "    Numero de pesos: 256\n",
      "tamaño del vector vectorizado 1024\n",
      "<class 'list'>\n",
      "shape (1024,)\n",
      "  Numero total de pesos: 1024\n",
      "  Total de direcciones con fallos: 32\n",
      "  indexList: 32\n",
      "  Listfault fallos: 32\n",
      "inyectando errores en los pesos\n",
      "  Total de direcciones afectadas: 19\n",
      "total de elementos iguales 1005\n",
      "index 0\n",
      "index 1\n",
      "index 2\n",
      "index 3\n",
      "Introducciendo errores en Conv3\n",
      "itm dentro\n",
      "    Numero de pesos: 884736\n",
      "tamaño del vector vectorizado 884736\n",
      "<class 'list'>\n",
      "itm dentro\n",
      "    Numero de pesos: 384\n",
      "tamaño del vector vectorizado 885120\n",
      "<class 'list'>\n",
      "shape (885120,)\n",
      "  Numero total de pesos: 885120\n",
      "  Total de direcciones con fallos: 11172\n",
      "  indexList: 11172\n",
      "  Listfault fallos: 11172\n",
      "inyectando errores en los pesos\n",
      "  Total de direcciones afectadas: 5767\n",
      "total de elementos iguales 879353\n",
      "index 0\n",
      "index 1\n",
      "Introducciendo errores en batch_normalization_47\n",
      "itm dentro\n",
      "    Numero de pesos: 384\n",
      "tamaño del vector vectorizado 384\n",
      "<class 'list'>\n",
      "itm dentro\n",
      "    Numero de pesos: 384\n",
      "tamaño del vector vectorizado 768\n",
      "<class 'list'>\n",
      "itm dentro\n",
      "    Numero de pesos: 384\n",
      "tamaño del vector vectorizado 1152\n",
      "<class 'list'>\n",
      "itm dentro\n",
      "    Numero de pesos: 384\n",
      "tamaño del vector vectorizado 1536\n",
      "<class 'list'>\n",
      "shape (1536,)\n",
      "  Numero total de pesos: 1536\n",
      "  Total de direcciones con fallos: 36\n",
      "  indexList: 36\n",
      "  Listfault fallos: 36\n",
      "inyectando errores en los pesos\n",
      "  Total de direcciones afectadas: 22\n",
      "total de elementos iguales 1514\n",
      "index 0\n",
      "index 1\n",
      "index 2\n",
      "index 3\n",
      "Introducciendo errores en Conv4\n",
      "itm dentro\n",
      "    Numero de pesos: 147456\n",
      "tamaño del vector vectorizado 147456\n",
      "<class 'list'>\n",
      "itm dentro\n",
      "    Numero de pesos: 384\n",
      "tamaño del vector vectorizado 147840\n",
      "<class 'list'>\n",
      "shape (147840,)\n",
      "  Numero total de pesos: 147840\n",
      "  Total de direcciones con fallos: 1773\n",
      "  indexList: 1773\n",
      "  Listfault fallos: 1773\n",
      "inyectando errores en los pesos\n",
      "  Total de direcciones afectadas: 902\n",
      "total de elementos iguales 146938\n",
      "index 0\n",
      "index 1\n",
      "Introducciendo errores en batch_normalization_48\n",
      "itm dentro\n",
      "    Numero de pesos: 384\n",
      "tamaño del vector vectorizado 384\n",
      "<class 'list'>\n",
      "itm dentro\n",
      "    Numero de pesos: 384\n",
      "tamaño del vector vectorizado 768\n",
      "<class 'list'>\n",
      "itm dentro\n",
      "    Numero de pesos: 384\n",
      "tamaño del vector vectorizado 1152\n",
      "<class 'list'>\n",
      "itm dentro\n",
      "    Numero de pesos: 384\n",
      "tamaño del vector vectorizado 1536\n",
      "<class 'list'>\n",
      "shape (1536,)\n",
      "  Numero total de pesos: 1536\n",
      "  Total de direcciones con fallos: 36\n",
      "  indexList: 36\n",
      "  Listfault fallos: 36\n",
      "inyectando errores en los pesos\n",
      "  Total de direcciones afectadas: 20\n",
      "total de elementos iguales 1516\n",
      "index 0\n",
      "index 1\n",
      "index 2\n",
      "index 3\n",
      "Introducciendo errores en Conv5\n",
      "itm dentro\n",
      "    Numero de pesos: 98304\n",
      "tamaño del vector vectorizado 98304\n",
      "<class 'list'>\n",
      "itm dentro\n",
      "    Numero de pesos: 256\n",
      "tamaño del vector vectorizado 98560\n",
      "<class 'list'>\n",
      "shape (98560,)\n",
      "  Numero total de pesos: 98560\n",
      "  Total de direcciones con fallos: 1178\n",
      "  indexList: 1178\n",
      "  Listfault fallos: 1178\n",
      "inyectando errores en los pesos\n",
      "  Total de direcciones afectadas: 628\n",
      "total de elementos iguales 97932\n",
      "index 0\n",
      "index 1\n",
      "Introducciendo errores en batch_normalization_49\n",
      "itm dentro\n",
      "    Numero de pesos: 256\n",
      "tamaño del vector vectorizado 256\n",
      "<class 'list'>\n",
      "itm dentro\n",
      "    Numero de pesos: 256\n",
      "tamaño del vector vectorizado 512\n",
      "<class 'list'>\n",
      "itm dentro\n",
      "    Numero de pesos: 256\n",
      "tamaño del vector vectorizado 768\n",
      "<class 'list'>\n",
      "itm dentro\n",
      "    Numero de pesos: 256\n",
      "tamaño del vector vectorizado 1024\n",
      "<class 'list'>\n",
      "shape (1024,)\n",
      "  Numero total de pesos: 1024\n",
      "  Total de direcciones con fallos: 32\n",
      "  indexList: 32\n",
      "  Listfault fallos: 32\n",
      "inyectando errores en los pesos\n",
      "  Total de direcciones afectadas: 21\n",
      "total de elementos iguales 1003\n",
      "index 0\n",
      "index 1\n",
      "index 2\n",
      "index 3\n",
      "Introducciendo errores en dense_27\n",
      "itm dentro\n",
      "    Numero de pesos: 37748736\n",
      "tamaño del vector vectorizado 37748736\n",
      "<class 'list'>\n",
      "itm dentro\n",
      "    Numero de pesos: 4096\n",
      "tamaño del vector vectorizado 37752832\n",
      "<class 'list'>\n",
      "shape (37752832,)\n",
      "  Numero total de pesos: 37752832\n",
      "  Total de direcciones con fallos: 12483\n",
      "  indexList: 12483\n",
      "  Listfault fallos: 12483\n",
      "inyectando errores en los pesos\n",
      "  Total de direcciones afectadas: 6447\n",
      "total de elementos iguales 37746385\n",
      "index 0\n",
      "index 1\n",
      "Introducciendo errores en dense_28\n",
      "itm dentro\n",
      "    Numero de pesos: 16777216\n",
      "tamaño del vector vectorizado 16777216\n",
      "<class 'list'>\n",
      "itm dentro\n",
      "    Numero de pesos: 4096\n",
      "tamaño del vector vectorizado 16781312\n",
      "<class 'list'>\n",
      "shape (16781312,)\n",
      "  Numero total de pesos: 16781312\n",
      "  Total de direcciones con fallos: 12483\n",
      "  indexList: 12483\n",
      "  Listfault fallos: 12483\n",
      "inyectando errores en los pesos\n",
      "  Total de direcciones afectadas: 6396\n",
      "total de elementos iguales 16774916\n",
      "index 0\n",
      "index 1\n",
      "Introducciendo errores en dense_29\n",
      "itm dentro\n",
      "    Numero de pesos: 32768\n",
      "tamaño del vector vectorizado 32768\n",
      "<class 'list'>\n",
      "itm dentro\n",
      "    Numero de pesos: 8\n",
      "tamaño del vector vectorizado 32776\n",
      "<class 'list'>\n",
      "shape (32776,)\n",
      "  Numero total de pesos: 32776\n",
      "  Total de direcciones con fallos: 456\n",
      "  indexList: 456\n",
      "  Listfault fallos: 456\n",
      "inyectando errores en los pesos\n",
      "  Total de direcciones afectadas: 251\n",
      "total de elementos iguales 32525\n",
      "index 0\n",
      "index 1\n",
      "47/47 [==============================] - 15s 319ms/step - loss: 7.2686 - accuracy: 0.1200\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#locs[0] = 0\n",
    "#error_mask[0] = 'xx1xxxxxxxxxxxxx'\n",
    "#\n",
    "\n",
    "#Acá la creamos, notese que como no se introduciran fallos en activaciones no es necesario pasar locs ni masks\n",
    "Net3 = GetNeuralNetworkModel('AlexNet', (227,227,3), 8, aging_active=activation_aging,\n",
    "                             word_size=word_size, frac_size=afrac_size, batch_size = testBatchSize)\n",
    "Net3.load_weights(wgt_dir).expect_partial()\n",
    "#Cuantizacion de los pesos\n",
    "WeightQuantization(model = Net3, frac_bits = wfrac_size, int_bits = wint_size)\n",
    "for layer in Net3.layers:\n",
    "    # weights contendrá una lista de arreglos, cada elemento de la lista contiene los pesos de un mismo tipo\n",
    "    # por ejemplo en una capa convolucional la lista es [w,b] donde w son los pesos y b los bias (tambien son pesos)\n",
    "    weights = layer.get_weights()\n",
    "    if weights:\n",
    "        print('Introducciendo errores en',layer.name)\n",
    "        # La idea a seguir ahora es:\n",
    "        # 1) Unir todos los pesos de la capa en una lista 1-D que simboliza su orden en memoria\n",
    "        # 2) aplicar los fallos en esta lista 1-D\n",
    "        # 3) retomar las dimensiones originales de los pesos e insertarlos en la capa.\n",
    "        vectorized_weights = []\n",
    "        # etapa 1) \n",
    "        for itm in weights:\n",
    "            print('itm dentro')\n",
    "            \n",
    "            print('    Numero de pesos:',itm.size)\n",
    "            vectorized_weights.extend(itm.flatten(order='F'))\n",
    "            print('tamaño del vector vectorizado',len(vectorized_weights))\n",
    "            print(type(vectorized_weights))\n",
    "        vectorized_weights = np.array(vectorized_weights)\n",
    "        #break\n",
    "        print('shape', vectorized_weights.shape)\n",
    "        print('  Numero total de pesos:',vectorized_weights.size)\n",
    "        # etapa 2) \n",
    "        indexList,Listfault,NumberOfFaults = GenerateFaultsList(shape=vectorized_weights.shape,locs=locs,error_mask=error_mask)\n",
    "        print('  Total de direcciones con fallos:',NumberOfFaults)\n",
    "        print('  indexList:',len(indexList))\n",
    "        print('  Listfault fallos:',len(Listfault))\n",
    "        if NumberOfFaults > 0:\n",
    "            faulty_weights = IntroduceFaultsInWeights(vectorized_weights,indexList,Listfault,wint_size,wfrac_size)\n",
    "            #break\n",
    "        # etapa 3)\n",
    "        print('  Total de direcciones afectadas:',np.count_nonzero(vectorized_weights!=faulty_weights))\n",
    "        diferentes=np.not_equal(vectorized_weights, faulty_weights)\n",
    "        print('total de elementos iguales',  np.count_nonzero(np.equal(vectorized_weights, faulty_weights)))\n",
    "        for index, itm in enumerate(weights):\n",
    "            print('index',index)\n",
    "            weights[index] = faulty_weights[0:itm.size].numpy().reshape(itm.shape,order='F')\n",
    "            #faulty_weights = faulty_weights[itm.size:]\n",
    "            layer.set_weights(weights)\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "Net3.compile(optimizer=optimizer, loss=loss, metrics='accuracy')\n",
    "loss,acc  = Net3.evaluate(test_dataset)          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748600f4",
   "metadata": {},
   "source": [
    "# Método 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea83f294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2D\n",
      "tamaño d ela capa (None, 55, 55, 96)\n",
      "<class 'numpy.ndarray'>\n",
      "shape wights_conv_antes (11, 11, 3, 96)\n",
      "shape bias_antes_conv (96,)\n",
      "(34848,)\n",
      "34944\n",
      "itm.shape (34944,)\n",
      "  Total de direcciones con fallos: 466\n",
      "  indexList: 466\n",
      "  Listfault fallos: 466\n",
      "inyectando errores en los pesos\n",
      "numero de fallos insertados 466\n",
      "tamaño de del array que contiene los weights 34848\n",
      "cantidad de pocisiones afectada capa 466\n",
      "tamaño de la lista d eposiciones 466\n",
      "cantidad de elementos diferente realmente 464\n",
      "cantidad de elementos iguales 34480\n",
      "shape bias_conv (96,)\n",
      "Bias cantidad de elementos iguales\n",
      "Bias cantidad de elementos diferentes 96\n",
      "set weights\n",
      "BatchNormalization\n",
      "Conv2D\n",
      "tamaño d ela capa (None, 27, 27, 256)\n",
      "<class 'numpy.ndarray'>\n",
      "shape wights_conv_antes (5, 5, 96, 256)\n",
      "shape bias_antes_conv (256,)\n",
      "(614400,)\n",
      "614656\n",
      "itm.shape (614656,)\n",
      "  Total de direcciones con fallos: 7277\n",
      "  indexList: 7277\n",
      "  Listfault fallos: 7277\n",
      "inyectando errores en los pesos\n",
      "numero de fallos insertados 7277\n",
      "tamaño de del array que contiene los weights 614400\n",
      "cantidad de pocisiones afectada capa 7275\n",
      "tamaño de la lista d eposiciones 7277\n",
      "cantidad de elementos diferente realmente 7238\n",
      "cantidad de elementos iguales 607418\n",
      "shape bias_conv (256,)\n",
      "Bias cantidad de elementos iguales\n",
      "Bias cantidad de elementos diferentes 254\n",
      "set weights\n",
      "BatchNormalization\n",
      "Conv2D\n",
      "tamaño d ela capa (None, 13, 13, 384)\n",
      "<class 'numpy.ndarray'>\n",
      "shape wights_conv_antes (3, 3, 256, 384)\n",
      "shape bias_antes_conv (384,)\n",
      "(884736,)\n",
      "885120\n",
      "itm.shape (885120,)\n",
      "  Total de direcciones con fallos: 11172\n",
      "  indexList: 11172\n",
      "  Listfault fallos: 11172\n",
      "inyectando errores en los pesos\n",
      "numero de fallos insertados 11172\n",
      "tamaño de del array que contiene los weights 884736\n",
      "cantidad de pocisiones afectada capa 11172\n",
      "tamaño de la lista d eposiciones 11172\n",
      "cantidad de elementos diferente realmente 11136\n",
      "cantidad de elementos iguales 873984\n",
      "shape bias_conv (384,)\n",
      "Bias cantidad de elementos iguales\n",
      "Bias cantidad de elementos diferentes 384\n",
      "set weights\n",
      "BatchNormalization\n",
      "Conv2D\n",
      "tamaño d ela capa (None, 13, 13, 384)\n",
      "<class 'numpy.ndarray'>\n",
      "shape wights_conv_antes (1, 1, 384, 384)\n",
      "shape bias_antes_conv (384,)\n",
      "(147456,)\n",
      "147840\n",
      "itm.shape (147840,)\n",
      "  Total de direcciones con fallos: 1773\n",
      "  indexList: 1773\n",
      "  Listfault fallos: 1773\n",
      "inyectando errores en los pesos\n",
      "numero de fallos insertados 1773\n",
      "tamaño de del array que contiene los weights 147456\n",
      "cantidad de pocisiones afectada capa 1773\n",
      "tamaño de la lista d eposiciones 1773\n",
      "cantidad de elementos diferente realmente 1762\n",
      "cantidad de elementos iguales 146078\n",
      "shape bias_conv (384,)\n",
      "Bias cantidad de elementos iguales\n",
      "Bias cantidad de elementos diferentes 384\n",
      "set weights\n",
      "BatchNormalization\n",
      "Conv2D\n",
      "tamaño d ela capa (None, 13, 13, 256)\n",
      "<class 'numpy.ndarray'>\n",
      "shape wights_conv_antes (1, 1, 384, 256)\n",
      "shape bias_antes_conv (256,)\n",
      "(98304,)\n",
      "98560\n",
      "itm.shape (98560,)\n",
      "  Total de direcciones con fallos: 1178\n",
      "  indexList: 1178\n",
      "  Listfault fallos: 1178\n",
      "inyectando errores en los pesos\n",
      "numero de fallos insertados 1178\n",
      "tamaño de del array que contiene los weights 98304\n",
      "cantidad de pocisiones afectada capa 1178\n",
      "tamaño de la lista d eposiciones 1178\n",
      "cantidad de elementos diferente realmente 1174\n",
      "cantidad de elementos iguales 97386\n",
      "shape bias_conv (256,)\n",
      "Bias cantidad de elementos iguales\n",
      "Bias cantidad de elementos diferentes 256\n",
      "set weights\n",
      "BatchNormalization\n",
      "Dense\n",
      "Dense\n",
      "Dense\n",
      "47/47 [==============================] - 15s 313ms/step - loss: 3.3910 - accuracy: 0.1173\n",
      "<tensorflow.python.keras.engine.functional.Functional object at 0x000002A747782370>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#Acá la creamos, notese que como no se introduciran fallos en activaciones no es necesario pasar locs ni masks\n",
    "Net2 = GetNeuralNetworkModel('AlexNet', (227,227,3), 8, aging_active=activation_aging,\n",
    "                             word_size=word_size, frac_size=afrac_size, batch_size = testBatchSize)\n",
    "Net2.load_weights(wgt_dir).expect_partial()\n",
    "#Cuantizacion de los pesos\n",
    "WeightQuantization(model = Net2, frac_bits = wfrac_size, int_bits = wint_size)\n",
    "for index,layer in enumerate(Net2.layers):\n",
    "   \n",
    "    \n",
    "    weights_tmp=[]\n",
    "    itm=[]\n",
    "    #print(Net2.layers[index].__class__.__name__)\n",
    "        \n",
    "    weights = layer.get_weights()\n",
    "    if weights:\n",
    "        print(Net2.layers[index].__class__.__name__)\n",
    "        if layer.__class__.__name__ in ['Conv2D','Conv1D','DepthwiseConv2D']:\n",
    "            print('tamaño d ela capa' ,layer.output_shape)\n",
    "            \n",
    "            weights, biases = Net2.layers[index].get_weights()\n",
    "            print(type(weights))\n",
    "                \n",
    "            w_shape= weights.shape\n",
    "            b_shape= biases.shape\n",
    "            print('shape wights_conv_antes',w_shape)\n",
    "            print('shape bias_antes_conv',b_shape)\n",
    "            wsize=weights.size\n",
    "            weights_conv = weights.flatten(order='F')\n",
    "            print(weights_conv.shape)\n",
    "            bias=biases.flatten(order='F')\n",
    "            itm= np.concatenate((weights_conv, biases))\n",
    "            print(len(itm))\n",
    "            #break\n",
    "            \n",
    "            print('itm.shape',itm.shape)\n",
    "            positionList,faultList,NumberOfFaults = GenerateFaultsList(shape=itm.shape,locs=locs,error_mask=error_mask)\n",
    "            print('  Total de direcciones con fallos:',NumberOfFaults)\n",
    "            print('  indexList:',len(positionList))\n",
    "            print('  Listfault fallos:',len(faultList))\n",
    "            itm_fail= IntroduceFaultsInWeights(itm,positionList,faultList,wfrac_size,wint_size) \n",
    "           \n",
    "            print('numero de fallos insertados', NumberOfFaults)\n",
    "            print('tamaño de del array que contiene los weights',wsize)\n",
    "            #Analizo la cantidad de posiciones que estarán afectadas contando los elementos menores \n",
    "            print('cantidad de pocisiones afectada capa',np.count_nonzero(positionList <= wsize))\n",
    "            print('tamaño de la lista d eposiciones', len(positionList))\n",
    "            #d = itm_fail[:wsize]\n",
    "            #Extraigo la cantidad de lementos que tenia el array de weights y le coloco la form aoriginal para hacer el set\n",
    "            w=np.reshape(itm_fail[:wsize], (w_shape))\n",
    "            comp_weights = np.not_equal(w, weights)\n",
    "            # para analizar la cantidad de elementos que han variado luego de introducir fallos\n",
    "            print('cantidad de elementos diferente realmente', np.count_nonzero(itm_fail!=itm))\n",
    "            print('cantidad de elementos iguales', np.count_nonzero(np.equal(itm_fail, itm)))\n",
    "             #Extraigo la cantidad de lementos que tenia el array del bias y le coloco la form original para hacer el set\n",
    "            b=np.reshape(itm_fail[wsize:], (b_shape))\n",
    "            print('shape bias_conv',b.shape)\n",
    "            comp_bias = np.not_equal(b, biases)\n",
    "            #Compruno los elementos que han variado ante sy despues de los fallos\n",
    "            print('Bias cantidad de elementos iguales', )\n",
    "            print('Bias cantidad de elementos diferentes', np.sum(comp_bias == 0))\n",
    "            #Lo inserto en un alista para formar la estructura que tenia luego del get_w\n",
    "            weights_tmp.append(w)\n",
    "            weights_tmp.append(b)\n",
    "            Net2.layers[index].set_weights(weights_tmp)\n",
    "            print('set weights')\n",
    "            #Este código debe ir identado con el for pero quise ir viendo el acc por cada capa xq alex lo sugirió y lo coloqué \n",
    "            #aquí, para el modelo completo ya sabes cambia la posición \n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "Net2.compile(optimizer=optimizer, loss=loss, metrics='accuracy')\n",
    "loss,acc  = Net2.evaluate(test_dataset)                \n",
    "print(Net2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1acd5a",
   "metadata": {},
   "source": [
    "# Metdodo optimizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3efd334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduciendo errores en Conv1\n",
      "    - Numero de pesos: 34848\n",
      "    - Tamaño del vector vectorizado 34848\n",
      "    - Numero de pesos: 96\n",
      "    - Tamaño del vector vectorizado 34944\n",
      "  Numero total de pesos: 34944\n",
      "  Total de direcciones con fallos: 466\n",
      "inyectando errores en los pesos\n",
      "  Total de direcciones afectadas: 249\n",
      "----------------------------------\n",
      "Introduciendo errores en batch_normalization_50\n",
      "    - Numero de pesos: 96\n",
      "    - Tamaño del vector vectorizado 96\n",
      "    - Numero de pesos: 96\n",
      "    - Tamaño del vector vectorizado 192\n",
      "    - Numero de pesos: 96\n",
      "    - Tamaño del vector vectorizado 288\n",
      "    - Numero de pesos: 96\n",
      "    - Tamaño del vector vectorizado 384\n",
      "  Numero total de pesos: 384\n",
      "  Total de direcciones con fallos: 2\n",
      "inyectando errores en los pesos\n",
      "  Total de direcciones afectadas: 0\n",
      "----------------------------------\n",
      "Introduciendo errores en Conv2\n",
      "    - Numero de pesos: 614400\n",
      "    - Tamaño del vector vectorizado 614400\n",
      "    - Numero de pesos: 256\n",
      "    - Tamaño del vector vectorizado 614656\n",
      "  Numero total de pesos: 614656\n",
      "  Total de direcciones con fallos: 7277\n",
      "inyectando errores en los pesos\n",
      "  Total de direcciones afectadas: 3705\n",
      "----------------------------------\n",
      "Introduciendo errores en batch_normalization_51\n",
      "    - Numero de pesos: 256\n",
      "    - Tamaño del vector vectorizado 256\n",
      "    - Numero de pesos: 256\n",
      "    - Tamaño del vector vectorizado 512\n",
      "    - Numero de pesos: 256\n",
      "    - Tamaño del vector vectorizado 768\n",
      "    - Numero de pesos: 256\n",
      "    - Tamaño del vector vectorizado 1024\n",
      "  Numero total de pesos: 1024\n",
      "  Total de direcciones con fallos: 32\n",
      "inyectando errores en los pesos\n",
      "  Total de direcciones afectadas: 19\n",
      "----------------------------------\n",
      "Introduciendo errores en Conv3\n",
      "    - Numero de pesos: 884736\n",
      "    - Tamaño del vector vectorizado 884736\n",
      "    - Numero de pesos: 384\n",
      "    - Tamaño del vector vectorizado 885120\n",
      "  Numero total de pesos: 885120\n",
      "  Total de direcciones con fallos: 11172\n",
      "inyectando errores en los pesos\n",
      "  Total de direcciones afectadas: 5767\n",
      "----------------------------------\n",
      "Introduciendo errores en batch_normalization_52\n",
      "    - Numero de pesos: 384\n",
      "    - Tamaño del vector vectorizado 384\n",
      "    - Numero de pesos: 384\n",
      "    - Tamaño del vector vectorizado 768\n",
      "    - Numero de pesos: 384\n",
      "    - Tamaño del vector vectorizado 1152\n",
      "    - Numero de pesos: 384\n",
      "    - Tamaño del vector vectorizado 1536\n",
      "  Numero total de pesos: 1536\n",
      "  Total de direcciones con fallos: 36\n",
      "inyectando errores en los pesos\n",
      "  Total de direcciones afectadas: 22\n",
      "----------------------------------\n",
      "Introduciendo errores en Conv4\n",
      "    - Numero de pesos: 147456\n",
      "    - Tamaño del vector vectorizado 147456\n",
      "    - Numero de pesos: 384\n",
      "    - Tamaño del vector vectorizado 147840\n",
      "  Numero total de pesos: 147840\n",
      "  Total de direcciones con fallos: 1773\n",
      "inyectando errores en los pesos\n",
      "  Total de direcciones afectadas: 902\n",
      "----------------------------------\n",
      "Introduciendo errores en batch_normalization_53\n",
      "    - Numero de pesos: 384\n",
      "    - Tamaño del vector vectorizado 384\n",
      "    - Numero de pesos: 384\n",
      "    - Tamaño del vector vectorizado 768\n",
      "    - Numero de pesos: 384\n",
      "    - Tamaño del vector vectorizado 1152\n",
      "    - Numero de pesos: 384\n",
      "    - Tamaño del vector vectorizado 1536\n",
      "  Numero total de pesos: 1536\n",
      "  Total de direcciones con fallos: 36\n",
      "inyectando errores en los pesos\n",
      "  Total de direcciones afectadas: 20\n",
      "----------------------------------\n",
      "Introduciendo errores en Conv5\n",
      "    - Numero de pesos: 98304\n",
      "    - Tamaño del vector vectorizado 98304\n",
      "    - Numero de pesos: 256\n",
      "    - Tamaño del vector vectorizado 98560\n",
      "  Numero total de pesos: 98560\n",
      "  Total de direcciones con fallos: 1178\n",
      "inyectando errores en los pesos\n",
      "  Total de direcciones afectadas: 628\n",
      "----------------------------------\n",
      "Introduciendo errores en batch_normalization_54\n",
      "    - Numero de pesos: 256\n",
      "    - Tamaño del vector vectorizado 256\n",
      "    - Numero de pesos: 256\n",
      "    - Tamaño del vector vectorizado 512\n",
      "    - Numero de pesos: 256\n",
      "    - Tamaño del vector vectorizado 768\n",
      "    - Numero de pesos: 256\n",
      "    - Tamaño del vector vectorizado 1024\n",
      "  Numero total de pesos: 1024\n",
      "  Total de direcciones con fallos: 32\n",
      "inyectando errores en los pesos\n",
      "  Total de direcciones afectadas: 21\n",
      "----------------------------------\n",
      "Introduciendo errores en dense_30\n",
      "    - Numero de pesos: 37748736\n",
      "    - Tamaño del vector vectorizado 37748736\n",
      "    - Numero de pesos: 4096\n",
      "    - Tamaño del vector vectorizado 37752832\n",
      "  Numero total de pesos: 37752832\n",
      "  Total de direcciones con fallos: 12483\n",
      "inyectando errores en los pesos\n",
      "  Total de direcciones afectadas: 6447\n",
      "----------------------------------\n",
      "Introduciendo errores en dense_31\n",
      "    - Numero de pesos: 16777216\n",
      "    - Tamaño del vector vectorizado 16777216\n",
      "    - Numero de pesos: 4096\n",
      "    - Tamaño del vector vectorizado 16781312\n",
      "  Numero total de pesos: 16781312\n",
      "  Total de direcciones con fallos: 12483\n",
      "inyectando errores en los pesos\n",
      "  Total de direcciones afectadas: 6396\n",
      "----------------------------------\n",
      "Introduciendo errores en dense_32\n",
      "    - Numero de pesos: 32768\n",
      "    - Tamaño del vector vectorizado 32768\n",
      "    - Numero de pesos: 8\n",
      "    - Tamaño del vector vectorizado 32776\n",
      "  Numero total de pesos: 32776\n",
      "  Total de direcciones con fallos: 456\n",
      "inyectando errores en los pesos\n",
      "  Total de direcciones afectadas: 251\n",
      "----------------------------------\n",
      "47/47 [==============================] - 14s 303ms/step - loss: 7.2686 - accuracy: 0.1200\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#Acá la creamos, notese que como no se introduciran fallos en activaciones no es necesario pasar locs ni masks\n",
    "Net = GetNeuralNetworkModel('AlexNet', (227,227,3), 8, aging_active=activation_aging,\n",
    "                             word_size=word_size, frac_size=afrac_size, batch_size = testBatchSize)\n",
    "Net.load_weights(wgt_dir).expect_partial()\n",
    "#Cuantizacion de los pesos\n",
    "WeightQuantization(model = Net, frac_bits = wfrac_size, int_bits = wint_size)\n",
    "for layer in Net.layers:\n",
    "    weights = layer.get_weights()\n",
    "    if weights:\n",
    "        print('Introduciendo errores en',layer.name)\n",
    "        vectorized_weights = []\n",
    "        for weight in weights:\n",
    "            print('    - Numero de pesos:',weight.size)\n",
    "            vectorized_weights.extend(weight.flatten(order='F'))\n",
    "            print('    - Tamaño del vector vectorizado',len(vectorized_weights))\n",
    "\n",
    "        vectorized_weights = np.array(vectorized_weights)\n",
    "        print('  Numero total de pesos:',vectorized_weights.size)\n",
    "        \n",
    "        indexList,faultList,NumberOfFaults = GenerateFaultsList(shape=vectorized_weights.shape,locs=locs,error_mask=error_mask)\n",
    "        print('  Total de direcciones con fallos:',NumberOfFaults)\n",
    "\n",
    "        if NumberOfFaults > 0:\n",
    "            faulty_weights = IntroduceFaultsInWeights(vectorized_weights,indexList,faultList,wint_size,wfrac_size)\n",
    "\n",
    "            for index, weight in enumerate(weights):\n",
    "                weights[index] = faulty_weights[0:weight.size].numpy().reshape(weight.shape,order='F')\n",
    "\n",
    "            layer.set_weights(weights)\n",
    "            \n",
    "            print('  Total de direcciones afectadas:',np.count_nonzero(vectorized_weights!=faulty_weights))\n",
    "\n",
    "        print('----------------------------------')\n",
    "\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "Net.compile(optimizer=optimizer, loss=loss, metrics='accuracy')\n",
    "loss,acc  = Net3.evaluate(test_dataset)          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5841a0e",
   "metadata": {},
   "source": [
    "# Analizar salidas d elas variables para los dos casos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d22b267",
   "metadata": {},
   "source": [
    "### antes de inyectar los fallos los vectores que pasamos son iguales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36de1f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cantidad de elementos diferente yami 0\n",
      "cantidad de elementos iguales yami 34944\n",
      "  Total de direcciones afectadas nicolas: 0\n"
     ]
    }
   ],
   "source": [
    "print('cantidad de elementos diferente yami', np.count_nonzero(vectorized_weights!=itm))\n",
    "print('cantidad de elementos iguales yami', np.count_nonzero(np.equal(vectorized_weights, itm)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ef5132",
   "metadata": {},
   "source": [
    "### luego ya no lo son"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "508d7001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elementos diferentes entre ambos vectores luego de inyect fallos 461\n",
      "cantidad de elementos iguales vectores luego de inyect fallos 34483\n",
      "  Total de direcciones  luego deinyec fallos nicolas: 249\n",
      "cantidad de elementos diferente yami 464\n"
     ]
    }
   ],
   "source": [
    "print('Elementos diferentes entre ambos vectores luego de inyect fallos', np.count_nonzero(faulty_weights!=itm_fail))\n",
    "print('cantidad de elementos iguales vectores luego de inyect fallos', np.count_nonzero(np.equal(faulty_weights, itm_fail)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('  Total de direcciones  luego deinyec fallos nicolas:',np.count_nonzero(vectorized_weights!=faulty_weights))\n",
    "print('cantidad de elementos diferente yami', np.count_nonzero(itm!=itm_fail))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db5cff16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2D\n",
      "tamaño d ela capa (None, 55, 55, 96)\n",
      "shape wights_conv_antes (11, 11, 3, 96)\n",
      "shape bias_antes_conv (96,)\n",
      "itm.shape (34944,)\n",
      "positionList 466\n",
      "faultList 466\n",
      "466 466\n",
      "inyectando errores en los pesos\n",
      "numero de fallos insertados 466\n",
      "tamaño de del array que contiene los weights 34848\n",
      "cantidad de pocisiones afectada capa 466\n",
      "tamaño de la lista d eposiciones 466\n",
      "cantidad de elementos iguales 34385\n",
      "cantidad de elementos diferentes 463\n",
      "shape bias_conv (96,)\n",
      "Bias cantidad de elementos iguales 96\n",
      "Bias cantidad de elementos diferentes 0\n",
      "set weights\n",
      "47/47 [==============================] - 14s 300ms/step - loss: 9.9115 - accuracy: 0.1173\n",
      "BatchNormalization\n",
      "Conv2D\n",
      "tamaño d ela capa (None, 27, 27, 256)\n",
      "shape wights_conv_antes (5, 5, 96, 256)\n",
      "shape bias_antes_conv (256,)\n",
      "itm.shape (614656,)\n",
      "positionList 7277\n",
      "faultList 7277\n",
      "7277 7277\n",
      "inyectando errores en los pesos\n",
      "numero de fallos insertados 7277\n",
      "tamaño de del array que contiene los weights 614400\n",
      "cantidad de pocisiones afectada capa 7275\n",
      "tamaño de la lista d eposiciones 7277\n",
      "cantidad de elementos iguales 607165\n",
      "cantidad de elementos diferentes 7235\n",
      "shape bias_conv (256,)\n",
      "Bias cantidad de elementos iguales 254\n",
      "Bias cantidad de elementos diferentes 2\n",
      "set weights\n",
      "47/47 [==============================] - 14s 304ms/step - loss: 9.6045 - accuracy: 0.1227\n",
      "BatchNormalization\n",
      "Conv2D\n",
      "tamaño d ela capa (None, 13, 13, 384)\n",
      "shape wights_conv_antes (3, 3, 256, 384)\n",
      "shape bias_antes_conv (384,)\n",
      "itm.shape (885120,)\n",
      "positionList 11172\n",
      "faultList 11172\n",
      "11172 11172\n",
      "inyectando errores en los pesos\n",
      "numero de fallos insertados 11172\n",
      "tamaño de del array que contiene los weights 884736\n",
      "cantidad de pocisiones afectada capa 11172\n",
      "tamaño de la lista d eposiciones 11172\n",
      "cantidad de elementos iguales 873623\n",
      "cantidad de elementos diferentes 11113\n",
      "shape bias_conv (384,)\n",
      "Bias cantidad de elementos iguales 384\n",
      "Bias cantidad de elementos diferentes 0\n",
      "set weights\n",
      "47/47 [==============================] - 15s 309ms/step - loss: 8.2025 - accuracy: 0.1173\n",
      "BatchNormalization\n",
      "Conv2D\n",
      "tamaño d ela capa (None, 13, 13, 384)\n",
      "shape wights_conv_antes (1, 1, 384, 384)\n",
      "shape bias_antes_conv (384,)\n",
      "itm.shape (147840,)\n",
      "positionList 1773\n",
      "faultList 1773\n",
      "1773 1773\n",
      "inyectando errores en los pesos\n",
      "numero de fallos insertados 1773\n",
      "tamaño de del array que contiene los weights 147456\n",
      "cantidad de pocisiones afectada capa 1773\n",
      "tamaño de la lista d eposiciones 1773\n",
      "cantidad de elementos iguales 145689\n",
      "cantidad de elementos diferentes 1767\n",
      "shape bias_conv (384,)\n",
      "Bias cantidad de elementos iguales 384\n",
      "Bias cantidad de elementos diferentes 0\n",
      "set weights\n",
      "47/47 [==============================] - 14s 301ms/step - loss: 7.4642 - accuracy: 0.1173\n",
      "BatchNormalization\n",
      "Conv2D\n",
      "tamaño d ela capa (None, 13, 13, 256)\n",
      "shape wights_conv_antes (1, 1, 384, 256)\n",
      "shape bias_antes_conv (256,)\n",
      "itm.shape (98560,)\n",
      "positionList 1178\n",
      "faultList 1178\n",
      "1178 1178\n",
      "inyectando errores en los pesos\n",
      "numero de fallos insertados 1178\n",
      "tamaño de del array que contiene los weights 98304\n",
      "cantidad de pocisiones afectada capa 1178\n",
      "tamaño de la lista d eposiciones 1178\n",
      "cantidad de elementos iguales 97131\n",
      "cantidad de elementos diferentes 1173\n",
      "shape bias_conv (256,)\n",
      "Bias cantidad de elementos iguales 256\n",
      "Bias cantidad de elementos diferentes 0\n",
      "set weights\n",
      "47/47 [==============================] - 14s 300ms/step - loss: 3.0441 - accuracy: 0.1173\n",
      "BatchNormalization\n",
      "Dense\n",
      "Dense\n",
      "Dense\n",
      "<tensorflow.python.keras.engine.functional.Functional object at 0x0000023E9AC0DF40>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Máscara base Buffer de pesos\n",
    "error_mask = load_obj('Data/Fault Characterization/variante_mask_vc_707/buffer_weights/error_mask_054')\n",
    "locs = load_obj('Data/Fault Characterization/variante_mask_vc_707/buffer_weights/locs_054')\n",
    "#print('losc pesos',locs_pesos[0:10])\n",
    "#loss: 3.0441 - accuracy: 0.1173\n",
    "\n",
    "activation_aging = False\n",
    "\n",
    "\n",
    "wfrac_size = 11\n",
    "wint_size  = 4\n",
    "\n",
    "\n",
    "\n",
    "#Acá la creamos, notese que como no se introduciran fallos en activaciones no es necesario pasar locs ni masks\n",
    "Net2 = GetNeuralNetworkModel('AlexNet', (227,227,3), 8, aging_active=activation_aging,\n",
    "                             word_size=word_size, frac_size=afrac_size, batch_size = testBatchSize)\n",
    "Net2.load_weights(wgt_dir).expect_partial()\n",
    "#Cuantizacion de los pesos\n",
    "WeightQuantization(model = Net2, frac_bits = wfrac_size, int_bits = wint_size)\n",
    "for index,layer in enumerate(Net2.layers):\n",
    "   \n",
    "    \n",
    "    weights_tmp=[]\n",
    "    #print(Net2.layers[index].__class__.__name__)\n",
    "        \n",
    "    if(len(Net2.layers[index].weights) > 0):\n",
    "        print(Net2.layers[index].__class__.__name__)\n",
    "        if layer.__class__.__name__ in ['Conv2D','Conv1D','DepthwiseConv2D']:\n",
    "            print('tamaño d ela capa' ,layer.output_shape)\n",
    "            weights, biases = Net2.layers[index].get_weights()\n",
    "            w_shape= weights.shape\n",
    "            b_shape= biases.shape\n",
    "            print('shape wights_conv_antes',w_shape)\n",
    "            print('shape bias_antes_conv',b_shape)\n",
    "            wsize=weights.size\n",
    "            weights_conv = np.reshape(weights, (-1))\n",
    "            weig_conv_bias=np.concatenate((weights_conv, biases))\n",
    "            itm= weig_conv_bias\n",
    "            print('itm.shape',itm.shape)\n",
    "            positionList,faultList,NumberOfFaults = GenerateFaultsList(shape=itm.shape,locs=locs,error_mask=error_mask)\n",
    "            print('positionList',len(positionList))\n",
    "            print('faultList',len(faultList))\n",
    "            print(NumberOfFaults,NumberOfFaults)\n",
    "            itm_fail= IntroduceFaultsInWeights(itm,positionList,faultList,wfrac_size,wint_size) \n",
    "            print('numero de fallos insertados', NumberOfFaults)\n",
    "            print('tamaño de del array que contiene los weights',wsize)\n",
    "            #Analizo la cantidad de posiciones que estarán afectadas contando los elementos menores \n",
    "            print('cantidad de pocisiones afectada capa',np.count_nonzero(positionList <= wsize))\n",
    "            print('tamaño de la lista d eposiciones', len(positionList))\n",
    "            \n",
    "            #Extraigo la cantidad de lementos que tenia el array de weights y le coloco la form aoriginal para hacer el set\n",
    "            w=np.reshape(itm_fail[:wsize], (w_shape))\n",
    "            comp_weights = np.equal(w, weights)\n",
    "            # para analizar la cantidad de elementos que han variado luego de introducir fallos\n",
    "            print('cantidad de elementos iguales', np.count_nonzero(comp_weights))\n",
    "            print('cantidad de elementos diferentes', np.sum(comp_weights == 0))\n",
    "             #Extraigo la cantidad de lementos que tenia el array del bias y le coloco la form original para hacer el set\n",
    "            b=np.reshape(itm_fail[wsize:], (b_shape))\n",
    "            print('shape bias_conv',b.shape)\n",
    "            comp_bias = np.equal(b, biases)\n",
    "            #Compruno los elementos que han variado ante sy despues de los fallos\n",
    "            print('Bias cantidad de elementos iguales', np.count_nonzero(comp_bias))\n",
    "            print('Bias cantidad de elementos diferentes', np.sum(comp_bias == 0))\n",
    "            #Lo inserto en un alista para formar la estructura que tenia luego del get_w\n",
    "            weights_tmp.append(w)\n",
    "            weights_tmp.append(b)\n",
    "            Net2.layers[index].set_weights(weights_tmp)\n",
    "            print('set weights')\n",
    "            #Este código debe ir identado con el for pero quise ir viendo el acc por cada capa xq alex lo sugirió y lo coloqué \n",
    "            #aquí, para el modelo completo ya sabes cambia la posición \n",
    "            loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "            optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "            Net2.compile(optimizer=optimizer, loss=loss, metrics='accuracy')\n",
    "            loss,acc  = Net2.evaluate(test_dataset)                \n",
    "print(Net2)\n",
    "#Net_faulty = WeightsFaults(Net2,locs,error_mask,wfrac_size,wint_size)\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_first",
   "language": "python",
   "name": "env_first"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
